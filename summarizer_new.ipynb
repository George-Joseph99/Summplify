{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c501685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk\n",
    "import io\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from itertools import islice\n",
    "from rouge import Rouge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9fd77f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download()  # uncomment these lines once they are not downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb19c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(article):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "    article_preprocessed = []\n",
    "    sentences = sent_tokenize(article)\n",
    "    for sentence in sentences:\n",
    "        sentence_preprocessed = []\n",
    "        sentence = re.sub(r\"[^a-zA-Z\\s]+\", \"\", sentence)\n",
    "        words = word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            if (word not in stopwords_english and word not in string.punctuation):\n",
    "                word_stemmed = stemmer.stem(word)  \n",
    "                sentence_preprocessed.append(word_stemmed)\n",
    "#         sentence_preprocessed = \" \".join(sentence_preprocessed)\n",
    "        article_preprocessed.append(sentence_preprocessed)\n",
    "            \n",
    "        \n",
    "        \n",
    "#     words = [word_tokenize(sent) for sent in sentences]\n",
    "#     words_without_stopwords = [[word for word in sent if word not in stopwords.words('english')] for sent in words]\n",
    "    \n",
    "    return article_preprocessed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d0a784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['quarterli', 'profit', 'us', 'media', 'giant', 'timewarn', 'jump', 'bn', 'three', 'month', 'decemb', 'yearearli'], ['the', 'firm', 'one', 'biggest', 'investor', 'googl', 'benefit', 'sale', 'highspe', 'internet', 'connect', 'higher', 'advert', 'sale'], ['timewarn', 'said', 'fourth', 'quarter', 'sale', 'rose', 'bn', 'bn'], ['it', 'profit', 'buoy', 'oneoff', 'gain', 'offset', 'profit', 'dip', 'warner', 'bro', 'less', 'user', 'aol'], ['time', 'warner', 'said', 'friday', 'own', 'searchengin', 'googl'], ['but', 'internet', 'busi', 'aol', 'mix', 'fortun'], ['it', 'lost', 'subscrib', 'fourth', 'quarter', 'profit', 'lower', 'preced', 'three', 'quarter'], ['howev', 'compani', 'said', 'aol', 'underli', 'profit', 'except', 'item', 'rose', 'back', 'stronger', 'internet', 'advertis', 'revenu'], ['it', 'hope', 'increas', 'subscrib', 'offer', 'onlin', 'servic', 'free', 'timewarn', 'internet', 'custom', 'tri', 'sign', 'aol', 'exist', 'custom', 'highspe', 'broadband'], ['timewarn', 'also', 'restat', 'result', 'follow', 'probe', 'us', 'secur', 'exchang', 'commiss', 'sec', 'close', 'conclud'], ['time', 'warner', 'fourth', 'quarter', 'profit', 'slightli', 'better', 'analyst', 'expect'], ['but', 'film', 'divis', 'saw', 'profit', 'slump', 'help', 'boxoffic', 'flop', 'alexand', 'catwoman', 'sharp', 'contrast', 'yearearli', 'third', 'final', 'film', 'lord', 'ring', 'trilog', 'boost', 'result'], ['for', 'fullyear', 'timewarn', 'post', 'profit', 'bn', 'perform', 'revenu', 'grew', 'bn'], ['our', 'financi', 'perform', 'strong', 'meet', 'exceed', 'fullyear', 'object', 'greatli', 'enhanc', 'flexibl', 'chairman', 'chief', 'execut', 'richard', 'parson', 'said'], ['for', 'timewarn', 'project', 'oper', 'earn', 'growth', 'around', 'also', 'expect', 'higher', 'revenu', 'wider', 'profit', 'margin'], ['timewarn', 'restat', 'account', 'part', 'effort', 'resolv', 'inquiri', 'aol', 'us', 'market', 'regul'], ['it', 'alreadi', 'offer', 'pay', 'settl', 'charg', 'deal', 'review', 'sec'], ['the', 'compani', 'said', 'unabl', 'estim', 'amount', 'need', 'set', 'asid', 'legal', 'reserv', 'previous', 'set'], ['it', 'intend', 'adjust', 'way', 'account', 'deal', 'german', 'music', 'publish', 'bertelsmann', 'purchas', 'stake', 'aol', 'europ', 'report', 'advertis', 'revenu'], ['it', 'book', 'sale', 'stake', 'aol', 'europ', 'loss', 'valu', 'stake']]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "article_file = io.open(\"articles/original (\" + str(1) +\").txt\")\n",
    "article_file.readline()\n",
    "article = article_file.read()\n",
    "article_file.close()\n",
    "\n",
    "# print(article)\n",
    "article_preprocessed = preprocessing(article)\n",
    "print(article_preprocessed)\n",
    "print(len(article_preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f41b89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_string(sentences):  # converts list of lists to list of strings\n",
    "    sentences_modified = []   # list of strings\n",
    "    for sentence in sentences:\n",
    "        sentence_modified = ''.join(sentence)\n",
    "        sentences_modified.append(sentence_modified)\n",
    "    return sentences_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fdb016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature 1\n",
    "\n",
    "def calculate_TF_IDF(content):\n",
    "    flat_words = [word for sent in content for word in sent]\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(flat_words)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    word_scores = {}\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        word_scores[feature] = tfidf_matrix[:, i].sum()\n",
    "    return word_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dd44d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_each_sentence_score(article_preprocessed, word_scores):\n",
    "    sentence_scores = []\n",
    "    for sent in article_preprocessed:\n",
    "        score = 0\n",
    "        for word in sent:\n",
    "            score += word_scores.get(word, 0)\n",
    "        sentence_scores.append(score)\n",
    "        \n",
    "    sentence_scores = sentence_scores / max(sentence_scores)\n",
    "    return sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed29965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(sentences, sentence_scores, num_sentences):\n",
    "    top_sentences_idx = sorted(range(len(sentence_scores)), key=lambda i: sentence_scores[i], reverse=True)[:num_sentences]\n",
    "    summary = [sentences[i] for i in top_sentences_idx]\n",
    "    return ' '.join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44f0eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tf_idf(file_name): # using tf-idf\n",
    "    \n",
    "    article_file = io.open(\"articles/original (\" + str(file_name) +\").txt\")\n",
    "    article_file.readline()\n",
    "    article = article_file.read()\n",
    "    article_file.close()\n",
    "    \n",
    "    sentences = sent_tokenize(article)\n",
    "    sentences[0] = sentences[0][1:]\n",
    "    \n",
    "    summarized_file = io.open(\"articles/summarized (\" + str(file_name) +\").txt\")\n",
    "    summarized_original = summarized_file.read()\n",
    "    summarized_file.close()\n",
    "\n",
    "    article_preprocessed = preprocessing(article)\n",
    "    word_scores = calculate_TF_IDF(article_preprocessed)\n",
    "    sentence_scores = calculate_each_sentence_score(article_preprocessed, word_scores)\n",
    "    summary = generate_summary(sentences, sentence_scores, 7)\n",
    "    \n",
    "    print(sentence_scores)\n",
    "    rouge = Rouge(metrics=['rouge-n', 'rouge-l'], max_n=2)\n",
    "    scores_tf_idf = rouge.get_scores(summary, summarized_original)\n",
    "    \n",
    "    rouge_1_tf_idf = scores_tf_idf['rouge-1']['f']\n",
    "    rouge_2_tf_idf = scores_tf_idf['rouge-2']['f']\n",
    "    rouge_l_tf_idf = scores_tf_idf['rouge-l']['f']\n",
    "    print('t-idf accuracy')\n",
    "    print('Rouge 1 score is: %f' % (rouge_1_tf_idf))\n",
    "    print('Rouge 2 score is: %f' % (rouge_2_tf_idf))\n",
    "    print('Rouge l score is: %f' % (rouge_l_tf_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e519623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75       0.28571429 0.64285714 0.96428571 0.85714286 0.60714286\n",
      " 0.75       0.82142857 0.64285714 1.         0.64285714 0.32142857\n",
      " 0.35714286 0.96428571]\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.709402\n",
      "Rouge 2 score is: 0.629310\n",
      "Rouge l score is: 0.529915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "test_tf_idf(301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17129375",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature 2\n",
    "\n",
    "def sentence_length(article_preprocessed):\n",
    "    article_preprocessed = convert_list_to_string(article_preprocessed)\n",
    "    max_length = 0\n",
    "    for sentence in article_preprocessed:\n",
    "        # print(sentence)\n",
    "        if len(sentence) > max_length:\n",
    "            max_length = len(sentence)\n",
    "            \n",
    "    sentence_length_feature = []\n",
    "    for sentence in article_preprocessed:\n",
    "        sentence_length_feature.append(len(sentence) / max_length)\n",
    "    return sentence_length_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a4b22f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.559322033898305,\n",
       " 0.6694915254237288,\n",
       " 0.3135593220338983,\n",
       " 0.4830508474576271,\n",
       " 0.3305084745762712,\n",
       " 0.2288135593220339,\n",
       " 0.4745762711864407,\n",
       " 0.6779661016949152,\n",
       " 0.847457627118644,\n",
       " 0.6016949152542372,\n",
       " 0.4745762711864407,\n",
       " 1.0,\n",
       " 0.423728813559322,\n",
       " 0.8728813559322034,\n",
       " 0.652542372881356,\n",
       " 0.5084745762711864,\n",
       " 0.3389830508474576,\n",
       " 0.5338983050847458,\n",
       " 0.8220338983050848,\n",
       " 0.3050847457627119]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_file = io.open(\"articles/original (\" + str(1) +\").txt\")\n",
    "article_file.readline()\n",
    "art = article_file.read()\n",
    "article_file.close()\n",
    "\n",
    "art = preprocessing(art)\n",
    "print(len(art))\n",
    "art = convert_list_to_string(art)\n",
    "sentence_length(article_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fc1f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_X_labels(article_preprocessed):\n",
    "    # feature 1 (tf_idf)\n",
    "    word_scores = calculate_TF_IDF(article_preprocessed)\n",
    "    tf_idf_score_feature = calculate_each_sentence_score(article_preprocessed, word_scores)\n",
    "    \n",
    "    # feature 2 (sentence_length)\n",
    "    sentence_length_feature = sentence_length(article_preprocessed)\n",
    "    \n",
    "    matrix = np.column_stack((tf_idf_score_feature, sentence_length_feature))\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c41f3ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.79069767 0.55932203]\n",
      " [0.62790698 0.66949153]\n",
      " [0.81395349 0.31355932]\n",
      " [0.97674419 0.48305085]\n",
      " [0.34883721 0.33050847]\n",
      " [0.37209302 0.22881356]\n",
      " [0.76744186 0.47457627]\n",
      " [0.95348837 0.6779661 ]\n",
      " [1.         0.84745763]\n",
      " [0.58139535 0.60169492]\n",
      " [0.60465116 0.47457627]\n",
      " [0.81395349 1.        ]\n",
      " [0.88372093 0.42372881]\n",
      " [0.53488372 0.87288136]\n",
      " [0.81395349 0.65254237]\n",
      " [0.62790698 0.50847458]\n",
      " [0.39534884 0.33898305]\n",
      " [0.48837209 0.53389831]\n",
      " [0.86046512 0.8220339 ]\n",
      " [0.65116279 0.30508475]]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "article_file = io.open(\"articles/original (\" + str(1) +\").txt\")\n",
    "article_file.readline()\n",
    "article = article_file.read()\n",
    "article_file.close()\n",
    "\n",
    "art = preprocessing(article)\n",
    "art = convert_list_to_string(art)\n",
    "print(generate_X_labels(article_preprocessed))\n",
    "print(len(generate_X_labels(article_preprocessed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1be1abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Y_labels(original, summarized):\n",
    "    Y_list = []\n",
    "    original_sentences = sent_tokenize(original)\n",
    "    original_sentences[0] = original_sentences[0][1:] # to remove the \\n\n",
    "    summarized_sentences = sent_tokenize(summarized)\n",
    "    \n",
    "    for original_sentence in original_sentences:\n",
    "        added = 0\n",
    "        for summarized_sentence in summarized_sentences:\n",
    "            if original_sentence in summarized_sentence:\n",
    "                Y_list.append(1)\n",
    "                added = 1\n",
    "                break\n",
    "        if added == 0:\n",
    "            Y_list.append(0)\n",
    "    \n",
    "    return Y_list, original_sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad975c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0]\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "article_file = io.open(\"articles/original (\" + str(301) +\").txt\")\n",
    "article_file.readline()\n",
    "article = article_file.read()\n",
    "article_file.close()\n",
    "\n",
    "summarized_file = io.open(\"articles/summarized (\" + str(301) +\").txt\")\n",
    "summarized = summarized_file.read()\n",
    "summarized_file.close()\n",
    "\n",
    "Y,_ = generate_Y_labels(article, summarized)\n",
    "print(Y)\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5378413a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8576\n",
      "8576\n"
     ]
    }
   ],
   "source": [
    "X_matrix = []\n",
    "X = []\n",
    "Y = []\n",
    "sentences = []\n",
    "\n",
    "for i in range (1, 101):   # loading business articles\n",
    "    article_file = io.open(\"train_original/business/article (\" + str(i) +\").txt\")\n",
    "    article_file.readline()\n",
    "    article = article_file.read()\n",
    "    article_file.close()\n",
    "\n",
    "    summarized_file = io.open(\"train_summary/business/summary (\" + str(i) +\").txt\")\n",
    "    summarized = summarized_file.read()\n",
    "    summarized_file.close()\n",
    "    \n",
    "    article_preprocessed = preprocessing(article)\n",
    "#     article_preprocessed = convert_list_to_string(article_preprocessed)\n",
    "    X_i = generate_X_labels(article_preprocessed)\n",
    "    Y_i, original_list_no_first_space = generate_Y_labels(article, summarized)\n",
    "    \n",
    "    if(len(X_i) != len(Y_i)):\n",
    "        print('Error! features and labels are not equal in length')\n",
    "        \n",
    "    Y.extend(Y_i)\n",
    "    X_matrix.extend(X_i)\n",
    "    sentences.extend(original_list_no_first_space)\n",
    "    \n",
    "for i in range (1, 101):   # loading entertainment articles\n",
    "    article_file = io.open(\"train_original/entertainment/article (\" + str(i) +\").txt\")\n",
    "    article_file.readline()\n",
    "    article = article_file.read()\n",
    "    article_file.close()\n",
    "\n",
    "    summarized_file = io.open(\"train_summary/entertainment/summary (\" + str(i) +\").txt\")\n",
    "    summarized = summarized_file.read()\n",
    "    summarized_file.close()\n",
    "    \n",
    "    article_preprocessed = preprocessing(article)\n",
    "#     article_preprocessed = convert_list_to_string(article_preprocessed)\n",
    "    X_i = generate_X_labels(article_preprocessed)\n",
    "    Y_i, original_list_no_first_space = generate_Y_labels(article, summarized)\n",
    "    \n",
    "    if(len(X_i) != len(Y_i)):\n",
    "        print('Error! features and labels are not equal in length')\n",
    "        \n",
    "    Y.extend(Y_i)\n",
    "    X_matrix.extend(X_i)\n",
    "    sentences.extend(original_list_no_first_space)\n",
    "    \n",
    "for i in range (1, 101):   # loading politics articles\n",
    "    article_file = io.open(\"train_original/politics/article (\" + str(i) +\").txt\")\n",
    "    article_file.readline()\n",
    "    article = article_file.read()\n",
    "    article_file.close()\n",
    "\n",
    "    summarized_file = io.open(\"train_summary/politics/summary (\" + str(i) +\").txt\")\n",
    "    summarized = summarized_file.read()\n",
    "    summarized_file.close()\n",
    "    \n",
    "    article_preprocessed = preprocessing(article)\n",
    "#     article_preprocessed = convert_list_to_string(article_preprocessed)\n",
    "    X_i = generate_X_labels(article_preprocessed)\n",
    "    Y_i, original_list_no_first_space = generate_Y_labels(article, summarized)\n",
    "    \n",
    "    if(len(X_i) != len(Y_i)):\n",
    "        print('Error! features and labels are not equal in length')\n",
    "        \n",
    "    Y.extend(Y_i)\n",
    "    X_matrix.extend(X_i)\n",
    "    sentences.extend(original_list_no_first_space)\n",
    "    \n",
    "for i in range (1, 101):   # loading sport articles\n",
    "    article_file = io.open(\"train_original/sport/article (\" + str(i) +\").txt\")\n",
    "    article_file.readline()\n",
    "    article = article_file.read()\n",
    "    article_file.close()\n",
    "\n",
    "    summarized_file = io.open(\"train_summary/sport/summary (\" + str(i) +\").txt\")\n",
    "    summarized = summarized_file.read()\n",
    "    summarized_file.close()\n",
    "    \n",
    "    article_preprocessed = preprocessing(article)\n",
    "#     article_preprocessed = convert_list_to_string(article_preprocessed)\n",
    "    X_i = generate_X_labels(article_preprocessed)\n",
    "    Y_i, original_list_no_first_space = generate_Y_labels(article, summarized)\n",
    "    \n",
    "    if(len(X_i) != len(Y_i)):\n",
    "        print('Error! features and labels are not equal in length')\n",
    "        \n",
    "    Y.extend(Y_i)\n",
    "    X_matrix.extend(X_i)\n",
    "    sentences.extend(original_list_no_first_space)\n",
    "    \n",
    "for i in range (1, 101):   # loading tech articles\n",
    "    article_file = io.open(\"train_original/tech/article (\" + str(i) +\").txt\")\n",
    "    article_file.readline()\n",
    "    article = article_file.read()\n",
    "    article_file.close()\n",
    "\n",
    "    summarized_file = io.open(\"train_summary/tech/summary (\" + str(i) +\").txt\")\n",
    "    summarized = summarized_file.read()\n",
    "    summarized_file.close()\n",
    "    \n",
    "    article_preprocessed = preprocessing(article)\n",
    "#     article_preprocessed = convert_list_to_string(article_preprocessed)\n",
    "    X_i = generate_X_labels(article_preprocessed)\n",
    "    Y_i, original_list_no_first_space = generate_Y_labels(article, summarized)\n",
    "    \n",
    "    if(len(X_i) != len(Y_i)):\n",
    "        print('Error! features and labels are not equal in length')\n",
    "        \n",
    "    Y.extend(Y_i)\n",
    "    X_matrix.extend(X_i)\n",
    "    sentences.extend(original_list_no_first_space)\n",
    "\n",
    "for x in X_matrix:\n",
    "    X.append(x.tolist())\n",
    "    \n",
    "X = np.matrix(X)\n",
    "\n",
    "m = len(X)\n",
    "# m = 500\n",
    "print(len(X))\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d42a131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_input_dim = 2 # input layer size (we have two input features)\n",
    "nn_output_dim = 1  # output layer size (we have one output)\n",
    "\n",
    "# Gradient descent parameters\n",
    "alpha = 0.1  # learning rate for gradient descent\n",
    "# print(Y)\n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be4a9b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # TODO 1: Compute the sigmoid function at the given x (~1 line)\n",
    "    # For example: sigmoid(2) should compute the value of sigmoid function at x = 2.\n",
    "    # Hint: Use np.exp instead of math.exp to allow for vectorization.\n",
    "    #----------------------------------------------------------------------------------------------\n",
    "    sig = (1/(1+np.exp(-x)))\n",
    "    #----------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1468bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(nn_hdim, num_passes=20000, print_loss=False):\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(nn_hdim, nn_input_dim) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((nn_hdim, 1))\n",
    "    W2 = np.random.randn(nn_output_dim, nn_hdim) / np.sqrt(nn_hdim)\n",
    "    b2 = np.zeros((nn_output_dim, 1))\n",
    "\n",
    "    model = {}\n",
    "\n",
    "    for i in range(0, num_passes):\n",
    "        DW1 = 0\n",
    "        DW2 = 0\n",
    "        Db1 = 0\n",
    "        Db2 = 0\n",
    "        cost = 0\n",
    "\n",
    "        for j in range(0, m):\n",
    "            a0 = X[j, :].reshape(-1, 1)  # Every training example is a column vector.\n",
    "            y = Y[j]\n",
    "            \n",
    "            z1 = np.dot(W1 , a0 )+ b1\n",
    "            a1 = np.tanh(z1)\n",
    "            z2 = np.dot(W2 , a1) + b2\n",
    "            a2 = sigmoid(z2)\n",
    "            \n",
    "#             if (i == num_passes -1 ):\n",
    "#                 print('True value: %f, got: %f'% (y, a2))\n",
    "\n",
    "            cost_j = -1 * ((np.log(a2) * y + (1-y)* np.log(1-a2)))\n",
    "\n",
    "            da2 =  ( -y/a2  + (1-y)/(1-a2) )\n",
    "            dz2 =  da2 * a2 * ( 1 - a2)\n",
    "            dW2 = np.dot(dz2 , a1.T)\n",
    "            db2 = dz2\n",
    "\n",
    "            da1 =  np.dot(dz2,W2).T\n",
    "            dz1 = np.multiply(da1 , 1 - np.square(a1) )\n",
    "            dW1 = np.dot(dz1 , a0.T )\n",
    "            db1 = dz1\n",
    "\n",
    "            DW1 += dW1\n",
    "            DW2 += dW2\n",
    "            Db2 += db2\n",
    "            Db1 += db1\n",
    "            cost += cost_j\n",
    "        \n",
    "        DW1 /= m\n",
    "        DW2 /= m\n",
    "        Db1 /= m\n",
    "        Db2 /= m\n",
    "        cost /= m\n",
    "\n",
    "        W1 -= alpha * DW1\n",
    "        b1 -= alpha * Db1\n",
    "        W2 -= alpha * DW2\n",
    "        b2 -= alpha * Db2\n",
    "\n",
    "        model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\n",
    "        if print_loss and i % 1000 == 0:\n",
    "            print(\"Loss after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bd38e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to predict an output (0 or 1)\n",
    "def predict(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    a0 = x.T\n",
    "    \n",
    "    # TODO 6 (aka TODO 2): Apply forward propagation on every test example a0 (a column vector 2x1) with its\n",
    "    #  corresponding label y. It is required to compute z1, a1, z2, and a2  (SAME AS TODO2).\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    z1 = np.dot(W1 , a0) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = np.dot(W2 , a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    # ------------------------------------------------------------------------------------------------\n",
    "    # Applying a threshold of 0.5 (i.e. predictions greater than 0.5 are mapped to 1, and 0 otherwise)\n",
    "#     prediction = np.round(a2)\n",
    "    prediction = a2\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9854a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.784817\n",
      "Loss after iteration 1000: 0.496629\n",
      "Loss after iteration 2000: 0.494060\n",
      "Loss after iteration 3000: 0.493428\n",
      "Loss after iteration 4000: 0.492979\n",
      "Loss after iteration 5000: 0.492672\n",
      "Loss after iteration 6000: 0.492431\n",
      "Loss after iteration 7000: 0.492218\n",
      "Loss after iteration 8000: 0.492019\n",
      "Loss after iteration 9000: 0.491833\n",
      "Loss after iteration 10000: 0.491665\n"
     ]
    }
   ],
   "source": [
    "model = build_model(nn_hdim=5, num_passes=10001, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec7bb518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(article_preprocessed_test, original_test, summarized_text, compression_ratio, file_number):\n",
    "#     article_file = io.open(\"articles/original (\" + str(file_number) +\").txt\")\n",
    "#     article_file.readline()\n",
    "#     original_test = article_file.read()\n",
    "#     article_preprocessed_test = preprocessing(original_test)\n",
    "#     article_file.close()\n",
    "\n",
    "#     summarized_file = io.open(\"articles/summarized (\" + str(file_number) +\").txt\")\n",
    "#     summarized_text = summarized_file.read()\n",
    "#     summarized_file.close()\n",
    "    \n",
    "    X_test = generate_X_labels(article_preprocessed_test)\n",
    "    predicton = predict(model, X_test)\n",
    "    Y_test, original_sentences = generate_Y_labels(original_test, summarized_text)\n",
    "    num_sentences_summarized = math.ceil(compression_ratio * len(original_sentences))\n",
    "    \n",
    "    \n",
    "    highest = np.argsort(predicton[0]) [::-1]\n",
    "    highest = highest[: num_sentences_summarized]\n",
    "#     highest = sorted(highest) # uncomment to arrange the article\n",
    "    output_sentences = []\n",
    "    output_indices = []\n",
    "    \n",
    "\n",
    "    \n",
    "    for i in range (0, num_sentences_summarized):\n",
    "        output_sentences.append(original_sentences[highest[i]])\n",
    "        output_indices.append(highest[i])\n",
    "        \n",
    "    output_sentences = ''.join(output_sentences)\n",
    "    \n",
    "    rouge = Rouge(metrics=['rouge-n', 'rouge-l'], max_n=2)\n",
    "    scores_nn = rouge.get_scores(output_sentences, summarized_text)\n",
    "    \n",
    "    rouge_1_nn = scores_nn['rouge-1']['f']\n",
    "    rouge_2_nn = scores_nn['rouge-2']['f']\n",
    "    rouge_l_nn = scores_nn['rouge-l']['f']\n",
    "    \n",
    "#     print('article number: %d' % (file_number))\n",
    "#     print('nn accuracy')\n",
    "#     print('Rouge 1 score is: %f' % (rouge_1_nn))\n",
    "#     print('Rouge 2 score is: %f' % (rouge_2_nn))\n",
    "#     print('Rouge l score is: %f' % (rouge_l_nn))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### classified using TF_idf score\n",
    "\n",
    "    output_tf_idf = extractive_summary(original_test, num_sentences_summarized)\n",
    "    \n",
    "    scores_tf_idf = rouge.get_scores(output_tf_idf, summarized_text)\n",
    "    \n",
    "    rouge_1_tf_idf = scores_tf_idf['rouge-1']['f']\n",
    "    rouge_2_tf_idf = scores_tf_idf['rouge-2']['f']\n",
    "    rouge_l_tf_idf = scores_tf_idf['rouge-l']['f']\n",
    "    \n",
    "#     print('t-idf accuracy')\n",
    "#     print('Rouge 1 score is: %f' % (rouge_1_tf_idf))\n",
    "#     print('Rouge 2 score is: %f' % (rouge_2_tf_idf))\n",
    "#     print('Rouge l score is: %f' % (rouge_l_tf_idf))\n",
    "    \n",
    "    return rouge_1_nn, rouge_2_nn, rouge_l_nn, rouge_1_tf_idf, rouge_2_tf_idf, rouge_l_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c087f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractive_summary(text, num_sentences):\n",
    "    # Preprocess the text\n",
    "    text = text.lower()\n",
    "    text = ''.join(c for c in text if c not in '1234567890')\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = [word_tokenize(sent) for sent in sentences]\n",
    "    words_without_stopwords = [[word for word in sent if word not in stopwords.words('english')] for sent in words]\n",
    "    \n",
    "    # Calculate TF-IDF scores\n",
    "    flat_words = [word for sent in words_without_stopwords for word in sent]\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(flat_words)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    word_scores = {}\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        word_scores[feature] = tfidf_matrix[:, i].sum()\n",
    "    \n",
    "    # Calculate sentence scores\n",
    "    sentence_scores = []\n",
    "    for sent in words_without_stopwords:\n",
    "        score = 0\n",
    "        for word in sent:\n",
    "            score += word_scores.get(word, 0)\n",
    "        sentence_scores.append(score)\n",
    "    \n",
    "    # Select top N sentences with highest scores\n",
    "    top_sentences_idx = sorted(range(len(sentence_scores)), key=lambda i: sentence_scores[i], reverse=True)[:num_sentences]\n",
    "    summary = [sentences[i] for i in top_sentences_idx]\n",
    "    return ' '.join(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41f7af17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using nn\n",
      "Average Rouge 1 score is: 0.768017\n",
      "Average Rouge 2 score is: 0.693994\n",
      "Average Rouge l score is: 0.561756\n",
      "Using tf_idf only\n",
      "Average Rouge 1 score is: 0.686983\n",
      "Average Rouge 2 score is: 0.575671\n",
      "Average Rouge l score is: 0.503824\n"
     ]
    }
   ],
   "source": [
    "# precision_nn = []\n",
    "# recall_nn = []\n",
    "# precision_tf_idf = []\n",
    "# recall_tf_idf = []\n",
    "\n",
    "rouge_1_list_nn = []\n",
    "rouge_2_list_nn = []\n",
    "rouge_l_list_nn = []\n",
    "\n",
    "rouge_1_list_tf_idf = []\n",
    "rouge_2_list_tf_idf = []\n",
    "rouge_l_list_tf_idf = []\n",
    "\n",
    "article_types = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "\n",
    "for article_type in article_types:\n",
    "    for i in range(101, 131):\n",
    "\n",
    "        article_file = io.open(\"train_original/\" + article_type + \"/article (\" + str(i) +\").txt\")\n",
    "        article_file.readline()\n",
    "        article = article_file.read()\n",
    "        article_preprocessed = preprocessing(article)\n",
    "        article_file.close()\n",
    "\n",
    "        summarized_file = io.open(\"train_summary/\" + article_type + \"/summary (\" + str(i) +\").txt\")\n",
    "        summarized = summarized_file.read()\n",
    "        summarized_file.close()\n",
    "\n",
    "        rouge_1_nn, rouge_2_nn, rouge_l_nn, rouge_1_tf_idf, rouge_2_tf_idf, rouge_l_tf_idf = test(\n",
    "            article_preprocessed, article, summarized,0.35, i)\n",
    "\n",
    "        rouge_1_list_nn.append(rouge_1_nn)\n",
    "        rouge_2_list_nn.append(rouge_2_nn)\n",
    "        rouge_l_list_nn.append(rouge_l_nn)\n",
    "\n",
    "        rouge_1_list_tf_idf.append(rouge_1_tf_idf)\n",
    "        rouge_2_list_tf_idf.append(rouge_2_tf_idf)\n",
    "        rouge_l_list_tf_idf.append(rouge_l_tf_idf)\n",
    "\n",
    "\n",
    "print('Using nn')\n",
    "print('Average Rouge 1 score is: %f' % (np.average(rouge_1_list_nn)))\n",
    "print('Average Rouge 2 score is: %f' % (np.average(rouge_2_list_nn)))\n",
    "print('Average Rouge l score is: %f' % (np.average(rouge_l_list_nn)))\n",
    "\n",
    "print('Using tf_idf only')\n",
    "print('Average Rouge 1 score is: %f' % (np.average(rouge_1_list_tf_idf)))\n",
    "print('Average Rouge 2 score is: %f' % (np.average(rouge_2_list_tf_idf)))\n",
    "print('Average Rouge l score is: %f' % (np.average(rouge_l_list_tf_idf)))\n",
    "\n",
    "# print('Neural network accuracy: ')\n",
    "# print('Average precision score is: %f' % (np.average(precision_nn)))\n",
    "# print('Average recall score is: %f' % (np.average(recall_nn)))\n",
    "      \n",
    "# print('Classical approach accuracy using tf-idf: ')\n",
    "# print('Average precision score is: %f' % (np.average(precision_tf_idf)))\n",
    "# print('Average recall score is: %f' % (np.average(recall_tf_idf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f249d2db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e9207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
