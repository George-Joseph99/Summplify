{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56be9705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk\n",
    "import io\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from itertools import islice\n",
    "from rouge import Rouge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bafbdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download()  # uncomment these lines once they are not downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84d568dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(article):\n",
    "    # lines = article.split(\".\")   # splits the whole article into lines\n",
    "#     print(article)\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "#     article = re.sub(r\"\\n\\n([A-Za-z0-9])\", \". \", article)\n",
    "    article_preprocessed = []\n",
    "    \n",
    "    lines = sent_tokenize(article)\n",
    "#     print(lines)\n",
    "    for line in lines:\n",
    "        line_preprocessed = []\n",
    "#         line = re.sub(r'[\\d]|','',line)\n",
    "        line = re.sub(r'(\\d)\\.(\\d)', r'\\1\\2', line)\n",
    "        line = re.sub(r'[^\\w\\s\\d]|\\n',' ',line)\n",
    "        words = word_tokenize(line)\n",
    "#         print(words)\n",
    "        for word in words:\n",
    "            if (word not in stopwords_english and word not in string.punctuation):\n",
    "                word_stemmed = stemmer.stem(word)  \n",
    "                line_preprocessed.append(word_stemmed)\n",
    "        line_preprocessed = \" \".join(line_preprocessed)\n",
    "        article_preprocessed.append(line_preprocessed)\n",
    "    return article_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e46cfebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"articles/original (\" + str(1) +\").txt\", \"r\", encoding=\"utf8\") as file:\n",
    "    file.readline()\n",
    "    contents = file.read()\n",
    "    article_preprocessed = preprocessing(contents)\n",
    "    \n",
    "# print(article_preprocessed)\n",
    "# print(len(article_preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c60a9372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3714a510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_list_to_string(content):  # converts list of lists to list of strings\n",
    "#     content_modified = []   # list of strings\n",
    "#     for line in content:\n",
    "#         line_as_string = \" \".join(line)\n",
    "#         content_modified.append(line_as_string)\n",
    "# #     print(content_modified)\n",
    "#     return content_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04520405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(convert_list_to_string(article_preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "925e25cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature 1\n",
    "\n",
    "def calculate_TF_IDF(content):\n",
    "#     print(content)\n",
    "    vectorizer = TfidfVectorizer()   # Create a TfidfVectorizer object\n",
    "    vectorizer.fit(content)   # Fit the vectorizer to the documents\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "#     print(\"Feature names:\")   # Print the feature names\n",
    "#     print(feature_names)   \n",
    "#     print(len(feature_names))\n",
    "    tfidf_matrix = vectorizer.transform(content)   # Transform the documents into a TF-IDF matrix\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "#     print(\"TF-IDF matrix:\")   # Print the TF-IDF matrix\n",
    "#     print(tfidf_matrix.toarray())\n",
    "    return tfidf_matrix.toarray()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c293a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_modified = (article_preprocessed)\n",
    "# calculate_TF_IDF(article_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28a6ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_each_sentence_score(tf_idf_matrix):\n",
    "    score_dict = {}   # dictionary that stores keys as summation of tf_idf scores for all word in current line \n",
    "                      # and values of dictionary as index of the line in the article\n",
    "    for index, line in enumerate(tf_idf_matrix):\n",
    "#         print(line)\n",
    "#         print('at')\n",
    "#         print(index)\n",
    "        score = np.sum(line)\n",
    "        score_dict[score] = index\n",
    "        \n",
    "    sorted_keys = sorted(score_dict.items(), reverse=True)   # sort the dictionary by keys in the descending order\n",
    "    score_dict_reversed = dict(sorted_keys)\n",
    "\n",
    "#     print(score_dict_reversed)\n",
    "    return score_dict_reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bb039f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_generation(article, score_dict_reversed, number_of_sentences):\n",
    "    lines = article.split(\".\")   # list contains strings, where these strings are original lines\n",
    "    # note: this should split on different characters; for example: \\n\\n and dot followed by number\n",
    "    score_dict = dict(islice(score_dict_reversed.items(), number_of_sentences))   # select only certain number of lines\n",
    "                                                                                  # to be displayed \n",
    "        \n",
    "    sort_data = sorted(score_dict.items(), key=lambda x: x[1])   # sort the dictionary by value (index of lines) in\n",
    "                                                                 # the ascending order to display lines ordered as the \n",
    "                                                                 # original article \n",
    "    score_dict_ascending = dict(sort_data)\n",
    "#     print(score_dict_ascending)\n",
    "#     print(score_dict_reversed)\n",
    "    output_list = []\n",
    "    for key in score_dict_ascending:\n",
    "        output_list.append(lines[score_dict_ascending[key]])\n",
    "            \n",
    "    output_string = \".\".join(output_list)\n",
    "    output_string += \".\"\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1319f96d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quarterly profits at US media giant TimeWarner jumped 76% to $1. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.\n",
      "\n",
      "Time Warner's fourth quarter profits were slightly better than analysts' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results.09bn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "article_modified = (article_preprocessed)\n",
    "# for sentence in article_modified:\n",
    "#     print(sentence)\n",
    "tf_idf_matrix = calculate_TF_IDF(article_modified)\n",
    "dict_scores = calculate_each_sentence_score(tf_idf_matrix)\n",
    "output = summary_generation(contents, dict_scores, 8)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f91227cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature 2\n",
    "\n",
    "def sentence_length(content):\n",
    "    max_length = 0\n",
    "    for sentence in content:\n",
    "        # print(sentence)\n",
    "        if len(sentence) > max_length:\n",
    "            max_length = len(sentence)\n",
    "            \n",
    "    sentence_length_feature = []\n",
    "    for sentence in content:\n",
    "        sentence_length_feature.append(len(sentence) / max_length)\n",
    "    return sentence_length_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6271fdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6357615894039735,\n",
       " 0.6291390728476821,\n",
       " 0.3443708609271523,\n",
       " 0.4370860927152318,\n",
       " 0.31788079470198677,\n",
       " 0.2119205298013245,\n",
       " 0.48344370860927155,\n",
       " 0.6291390728476821,\n",
       " 0.7947019867549668,\n",
       " 0.6158940397350994,\n",
       " 0.423841059602649,\n",
       " 1.0,\n",
       " 0.5165562913907285,\n",
       " 0.7947019867549668,\n",
       " 0.6423841059602649,\n",
       " 0.46357615894039733,\n",
       " 0.3509933774834437,\n",
       " 0.5298013245033113,\n",
       " 0.7483443708609272,\n",
       " 0.2913907284768212]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_length(article_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfd7ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature 3\n",
    "\n",
    "def numerical_data(content):\n",
    "    numerical_data_feature = []\n",
    "#     temp = []\n",
    "    for sentence in content:\n",
    "#         sentence_removed_dots_commas = sentence.replace(\",\", \"\").replace(\".\", \"\")\n",
    "        numerical_data = re.findall(r'\\d+', sentence)\n",
    "        numerical_data_feature.append(len(numerical_data) / len(sentence.split()))\n",
    "#         print(sentence)\n",
    "#         print(len(numerical_data))\n",
    "#         print(len(sentence.split()))\n",
    "    return numerical_data_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c54819d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25, 0.0, 0.3333333333333333, 0.0, 0.1111111111111111, 0.0, 0.16666666666666666, 0.06666666666666667, 0.0, 0.13333333333333333, 0.0, 0.07692307692307693, 0.35714285714285715, 0.0, 0.125, 0.0, 0.1, 0.07142857142857142, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(numerical_data(article_preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88e1baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Y_labels(original, summarized):\n",
    "    Y_list = []\n",
    "    original_sentences = sent_tokenize(original)\n",
    "    \n",
    "    original_sentences[0] = original_sentences[0][1:] # to remove the \\n\n",
    "    summarized_sentences = sent_tokenize(summarized)\n",
    "    \n",
    "    \n",
    "#     for i in range (0, len(original_sentences)):\n",
    "#         original_sentences[i] = re.sub(r\"\\\\'\", \"'\", original_sentences[i])\n",
    "        \n",
    "    \n",
    "#     for original_sentence in original_sentences:\n",
    "#         print(original_sentence)\n",
    "        \n",
    "#     original_list = re.split(r\"\\n\\n|\\.(?!\\d)\", original)\n",
    "    \n",
    "#     original_list_removed_empty = [x for x in original_list if x]   # removes empty elements\n",
    "            \n",
    "#     original_list_no_quotation = [x.replace('\"', '') for x in original_list_removed_empty]\n",
    "#     original_list_no_quotation = [x.replace(\"'\", '') for x in original_list_no_quotation]\n",
    "#     original_list_no_first_space = [x.lstrip() for x in original_list_no_quotation]\n",
    "#     original_list_no_first_space = [x for x in original_list_no_first_space if x]\n",
    "    \n",
    "#     summarized_list = re.split(r\"\\.(?!\\d)\", summarized)\n",
    "    \n",
    "#     summarized_list_removed_empty = [x for x in summarized_list if x]   # removes empty elements\n",
    "            \n",
    "#     summarized_list_no_quotation = [x.replace('\"', '') for x in summarized_list_removed_empty]\n",
    "#     summarized_list_no_quotation = [x.replace(\"'\", '') for x in summarized_list_no_quotation]\n",
    "#     summarized_list_no_first_space = [x.lstrip() for x in summarized_list_no_quotation]\n",
    "#     summarized_list_no_first_space = [x for x in summarized_list_no_first_space if x]\n",
    "    \n",
    "    \n",
    "#     print(original_list_no_first_space)\n",
    "#     print(summarized_list_no_first_space)\n",
    "    \n",
    "#     print(len(original_list_no_first_space))\n",
    "#     print(len(summarized_list_no_first_space))\n",
    "    \n",
    "    for original_sentence in original_sentences:\n",
    "        added = 0\n",
    "        for summarized_sentence in summarized_sentences:\n",
    "            if original_sentence in summarized_sentence:\n",
    "                Y_list.append(1)\n",
    "                added = 1\n",
    "                break\n",
    "        if added == 0:\n",
    "            Y_list.append(0)\n",
    "    \n",
    "    return Y_list, original_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca86b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"articles/original (\" + str(301) +\").txt\", \"r\", encoding=\"utf8\") as file:\n",
    "    file.readline()\n",
    "    original = file.read()\n",
    "    article_preprocessed = preprocessing(original)\n",
    "\n",
    "with open(\"articles/summarized (\" + str(301) +\").txt\", \"r\", encoding=\"utf8\") as file:\n",
    "    summarized = file.read()\n",
    "\n",
    "Y = generate_Y_labels(original, summarized)\n",
    "# print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cafa7316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_X_labels(preprocessed_artcile):\n",
    "#     print(preprocessed_artcile)\n",
    "    sentence_length_feature = sentence_length(preprocessed_artcile)\n",
    "#     print(len(sentence_length_feature))\n",
    "#     article_modified = convert_list_to_string(preprocessed_artcile)\n",
    "    numerical_data_feature = numerical_data(preprocessed_artcile)\n",
    "#     print(len(article_modified))\n",
    "    tf_idf_matrix = calculate_TF_IDF(preprocessed_artcile)\n",
    "    tf_idf_score = []\n",
    "    for index, line in enumerate(tf_idf_matrix):\n",
    "#         print(line)\n",
    "#         print('at')\n",
    "#         print(index)\n",
    "        tf_idf_score.append(np.sum(line))\n",
    "    max_score = max(tf_idf_score)\n",
    "    tf_idf_score = tf_idf_score/max_score\n",
    "    \n",
    "    matrix = np.column_stack((tf_idf_score, sentence_length_feature, numerical_data_feature))\n",
    "#     matrix = np.column_stack((tf_idf_score, sentence_length_feature))\n",
    "    \n",
    "#     print(matrix)\n",
    "#     matrix = np.array(tf_idf_score).reshape(len(tf_idf_score), 1)\n",
    "#     print(len(matrix))\n",
    "    # matrix = matrix[:len(matrix)-1]\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a75b733f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.85459232 0.87378641 0.        ]\n",
      " [0.64306346 0.34951456 0.28571429]\n",
      " [0.9092098  0.86407767 0.        ]\n",
      " [0.79707349 0.66019417 0.        ]\n",
      " [0.86838014 0.86407767 0.07692308]\n",
      " [0.87598802 0.77669903 0.        ]\n",
      " [0.71890163 0.5631068  0.        ]\n",
      " [1.         0.86407767 0.05882353]\n",
      " [0.72100819 0.5631068  0.        ]\n",
      " [0.86809826 0.88349515 0.1875    ]\n",
      " [0.9092098  0.89320388 0.        ]\n",
      " [0.73098164 0.57281553 0.        ]\n",
      " [0.47543868 0.2038835  0.        ]\n",
      " [0.9957197  1.         0.        ]]\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "X = generate_X_labels(article_preprocessed)\n",
    "print(X)\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31ffa1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = len(Y)  # training set size\n",
    "# m2 = len(X)\n",
    "# print(m)\n",
    "# print(m2)\n",
    "nn_input_dim = 3 # input layer dimensionality (we have two input features)\n",
    "nn_output_dim = 1  # output layer dimensionality (we have one output)\n",
    "\n",
    "# Gradient descent parameters\n",
    "alpha = 0.1  # learning rate for gradient descent\n",
    "# print(Y)\n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04a248f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # TODO 1: Compute the sigmoid function at the given x (~1 line)\n",
    "    # For example: sigmoid(2) should compute the value of sigmoid function at x = 2.\n",
    "    # Hint: Use np.exp instead of math.exp to allow for vectorization.\n",
    "    #----------------------------------------------------------------------------------------------\n",
    "    sig = (1/(1+np.exp(-x)))\n",
    "    #----------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "841b4439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4764\n",
      "4764\n"
     ]
    }
   ],
   "source": [
    "X_matrix = []\n",
    "X = []\n",
    "Y = []\n",
    "sentences = []\n",
    "for i in range (1, 301):\n",
    "    with open(\"articles/original (\" + str(i) +\").txt\", \"r\", encoding=\"utf8\") as file:\n",
    "        file.readline() # to remove the title\n",
    "        original_test = file.read()\n",
    "        article_preprocessed_test = preprocessing(original_test)\n",
    "    \n",
    "    with open(\"articles/summarized (\" + str(i) +\").txt\", \"r\", encoding=\"utf8\") as file:\n",
    "        summarized_text = file.read()\n",
    "    # print(summarized_text)\n",
    "    \n",
    "    X_i = generate_X_labels(article_preprocessed_test)\n",
    "    Y_i, original_list_no_first_space = generate_Y_labels(original_test, summarized_text)\n",
    "    if(len(X_i) != len(Y_i)):\n",
    "        print('Error! features and labels are not equal in length')\n",
    "    Y.extend(Y_i)\n",
    "    X_matrix.extend(X_i)\n",
    "    sentences.extend(original_list_no_first_space)\n",
    "    # print(i)\n",
    "\n",
    "# for article in X_matrix:\n",
    "#     for x in article:\n",
    "#         X.append(x)\n",
    "\n",
    "for x in X_matrix:\n",
    "    X.append(x.tolist())\n",
    "    \n",
    "X = np.matrix(X)\n",
    "# print(X)\n",
    "# print(Y)\n",
    "\n",
    "\n",
    "    \n",
    "# for i in range (0, len(X)):\n",
    "#     print(\"tf-idf: %f and true value: %f at sentence\\n %s\\n\" % (X[i], Y[i], sentences[i]))\n",
    "\n",
    "# m = len(X)\n",
    "m = 500\n",
    "print(len(X))\n",
    "print(len(Y))\n",
    "# print(type(x))\n",
    "\n",
    "\n",
    "# #     predicton = predict(model, X_test)\n",
    "# #     print(predicton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20facec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(nn_hdim, num_passes=20000, print_loss=False):\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(nn_hdim, nn_input_dim) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((nn_hdim, 1))\n",
    "    W2 = np.random.randn(nn_output_dim, nn_hdim) / np.sqrt(nn_hdim)\n",
    "    b2 = np.zeros((nn_output_dim, 1))\n",
    "\n",
    "    model = {}\n",
    "\n",
    "    for i in range(0, num_passes):\n",
    "        DW1 = 0\n",
    "        DW2 = 0\n",
    "        Db1 = 0\n",
    "        Db2 = 0\n",
    "        cost = 0\n",
    "\n",
    "        for j in range(0, m):\n",
    "            a0 = X[j, :].reshape(-1, 1)  # Every training example is a column vector.\n",
    "            y = Y[j]\n",
    "            \n",
    "            z1 = np.dot(W1 , a0 )+ b1\n",
    "            a1 = np.tanh(z1)\n",
    "            z2 = np.dot(W2 , a1) + b2\n",
    "            a2 = sigmoid(z2)\n",
    "            \n",
    "#             if (i == num_passes -1 ):\n",
    "#                 print('True value: %f, got: %f'% (y, a2))\n",
    "\n",
    "            cost_j = -1 * ((np.log(a2) * y + (1-y)* np.log(1-a2)))\n",
    "\n",
    "            da2 =  ( -y/a2  + (1-y)/(1-a2) )\n",
    "            dz2 =  da2 * a2 * ( 1 - a2)\n",
    "            dW2 = np.dot(dz2 , a1.T)\n",
    "            db2 = dz2\n",
    "\n",
    "            da1 =  np.dot(dz2,W2).T\n",
    "            dz1 = np.multiply(da1 , 1 - np.square(a1) )\n",
    "            dW1 = np.dot(dz1 , a0.T )\n",
    "            db1 = dz1\n",
    "\n",
    "            DW1 += dW1\n",
    "            DW2 += dW2\n",
    "            Db2 += db2\n",
    "            Db1 += db1\n",
    "            cost += cost_j\n",
    "        \n",
    "        DW1 /= m\n",
    "        DW2 /= m\n",
    "        Db1 /= m\n",
    "        Db2 /= m\n",
    "        cost /= m\n",
    "\n",
    "        W1 -= alpha * DW1\n",
    "        b1 -= alpha * Db1\n",
    "        W2 -= alpha * DW2\n",
    "        b2 -= alpha * Db2\n",
    "\n",
    "        model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\n",
    "        if print_loss and i % 1000 == 0:\n",
    "            print(\"Loss after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0664206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to predict an output (0 or 1)\n",
    "def predict(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    a0 = x.T\n",
    "    \n",
    "    # TODO 6 (aka TODO 2): Apply forward propagation on every test example a0 (a column vector 2x1) with its\n",
    "    #  corresponding label y. It is required to compute z1, a1, z2, and a2  (SAME AS TODO2).\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    z1 = np.dot(W1 , a0) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = np.dot(W2 , a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    # ------------------------------------------------------------------------------------------------\n",
    "    # Applying a threshold of 0.5 (i.e. predictions greater than 0.5 are mapped to 1, and 0 otherwise)\n",
    "#     prediction = np.round(a2)\n",
    "    prediction = a2\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7508490d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.802750\n",
      "Loss after iteration 1000: 0.669059\n",
      "Loss after iteration 2000: 0.668345\n",
      "Loss after iteration 3000: 0.668035\n",
      "Loss after iteration 4000: 0.667802\n",
      "Loss after iteration 5000: 0.667587\n",
      "Loss after iteration 6000: 0.667381\n",
      "Loss after iteration 7000: 0.667179\n",
      "Loss after iteration 8000: 0.666977\n",
      "Loss after iteration 9000: 0.666775\n",
      "Loss after iteration 10000: 0.666570\n"
     ]
    }
   ],
   "source": [
    "model = build_model(nn_hdim=8, num_passes=10001, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b910c87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test(file_number, compression_ratio):\n",
    "    with open(\"articles/original (\" + str(file_number) +\").txt\", \"r\", encoding=\"utf8\") as file:\n",
    "        file.readline()\n",
    "        original_test = file.read()\n",
    "        article_preprocessed_test = preprocessing(original_test)\n",
    "        # print(original_test)\n",
    "\n",
    "    with open(\"articles/summarized (\" + str(file_number) +\").txt\", \"r\", encoding=\"utf8\") as file:\n",
    "        summarized_text = file.read()\n",
    "        # print(summarized_text)\n",
    "    \n",
    "    X_test = generate_X_labels(article_preprocessed_test)\n",
    "    predicton = predict(model, X_test)\n",
    "    Y_test, original_sentences = generate_Y_labels(original_test, summarized_text)\n",
    "    \n",
    "    highest = np.argsort(predicton[0]) [::-1]\n",
    "        \n",
    "    output_sentences = []\n",
    "    output_indices = []\n",
    "    \n",
    "    num_sentences_summarized = math.ceil(compression_ratio * len(original_sentences))\n",
    "#     print(num_sentences_summarized)\n",
    "    for i in range (0, num_sentences_summarized):\n",
    "        output_sentences.append(original_sentences[highest[i]])\n",
    "        output_indices.append(highest[i])\n",
    "        \n",
    "    output_sentences = ''.join(output_sentences)\n",
    "    \n",
    "#     correct = 0\n",
    "#     missed = 0\n",
    "    \n",
    "#     Y_true_indices = [i for i, x in enumerate(Y_test) if x == 1]\n",
    "# #     print(Y_test)\n",
    "#     for true_index in Y_true_indices:\n",
    "#         if true_index in output_indices:\n",
    "#             correct += 1\n",
    "#         else:\n",
    "#             missed += 1\n",
    "            \n",
    "# #     missed = num_sentences_summarized - correct\n",
    "#     wrong = num_sentences_summarized - correct\n",
    "    \n",
    "#     precision_nn = correct / (correct + wrong)\n",
    "#     recall_nn = correct / (correct + missed)\n",
    "    \n",
    "#     print('correct: %f , wrong: %f , missed: %f' % (correct, wrong, missed))\n",
    "#     print('Precision for document: %i is : %f' % (file_number, precision_nn))\n",
    "#     print('Recall for document: %i is : %f \\n' % (file_number, recall_nn))   \n",
    "    \n",
    "    rouge = Rouge(metrics=['rouge-n', 'rouge-l'], max_n=2)\n",
    "    scores_nn = rouge.get_scores(output_sentences, summarized_text)\n",
    "    print('article number: %d' % (file_number))\n",
    "    rouge_1_nn = scores_nn['rouge-1']['f']\n",
    "    rouge_2_nn = scores_nn['rouge-2']['f']\n",
    "    rouge_l_nn = scores_nn['rouge-l']['f']\n",
    "    print('nn accuracy')\n",
    "    print('Rouge 1 score is: %f' % (rouge_1_nn))\n",
    "    print('Rouge 2 score is: %f' % (rouge_2_nn))\n",
    "    print('Rouge l score is: %f' % (rouge_l_nn))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### classified using TF_idf score\n",
    "    \n",
    "#     tf_idf_matrix = calculate_TF_IDF(article_preprocessed_test)\n",
    "#     dict_scores = calculate_each_sentence_score(tf_idf_matrix)\n",
    "#     output = summary_generation(original_test, dict_scores, num_sentences_summarized)\n",
    "\n",
    "    output_tf_idf = extractive_summary(original_test, num_sentences_summarized)\n",
    "    \n",
    "    scores_tf_idf = rouge.get_scores(output_tf_idf, summarized_text)\n",
    "    \n",
    "    rouge_1_tf_idf = scores_tf_idf['rouge-1']['f']\n",
    "    rouge_2_tf_idf = scores_tf_idf['rouge-2']['f']\n",
    "    rouge_l_tf_idf = scores_tf_idf['rouge-l']['f']\n",
    "    print('t-idf accuracy')\n",
    "    print('Rouge 1 score is: %f' % (rouge_1_tf_idf))\n",
    "    print('Rouge 2 score is: %f' % (rouge_2_tf_idf))\n",
    "    print('Rouge l score is: %f' % (rouge_l_tf_idf))\n",
    "    \n",
    "#     tf_idf_score = calculate_each_sentence_score(calculate_TF_IDF(article_preprocessed_test))\n",
    "#     score_dict = dict(islice(tf_idf_score.items(), num_sentences_summarized))\n",
    "#     score_list = list(score_dict.values())\n",
    "    \n",
    "# #     print(score_list)\n",
    "#     correct_tf_idf = 0\n",
    "#     missed_tf_idf = 0\n",
    "    \n",
    "#     for true_index in Y_true_indices:\n",
    "#         if true_index in score_list:\n",
    "#             correct_tf_idf += 1\n",
    "#         else:\n",
    "#             missed_tf_idf += 1\n",
    "            \n",
    "#     wrong_tf_idf = num_sentences_summarized - correct_tf_idf\n",
    "    \n",
    "#     precision_tf_idf = correct_tf_idf / (correct_tf_idf + wrong_tf_idf)\n",
    "#     recall_tf_idf = correct_tf_idf / (correct_tf_idf + missed_tf_idf)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print('classified using tf-idf scores')\n",
    "#     print('correct: %f , wrong: %f , missed: %f' % (correct_tf_idf, wrong_tf_idf, missed_tf_idf))\n",
    "#     print('Precision for document: %i is : %f' % (file_number, precision_tf_idf))\n",
    "#     print('Recall for document: %i is : %f \\n' % (file_number, recall_tf_idf)) \n",
    "    \n",
    "    return rouge_1_nn, rouge_2_nn, rouge_l_nn, rouge_1_tf_idf, rouge_2_tf_idf, rouge_l_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d6e136a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# highest = np.argsort(predicton[0]) [::-1]\n",
    "# print(highest)\n",
    "# lines = original_test.split('.')\n",
    "# output = []\n",
    "# for i in range (0, 6):\n",
    "#     output.append(lines[highest[i]])\n",
    "# print(output)\n",
    "# test(1, 0.35)\n",
    "# test(16, 0.35)\n",
    "# test(17, 0.35)\n",
    "# test(8, 0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73be4ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractive_summary(text, num_sentences):\n",
    "    # Preprocess the text\n",
    "    text = text.lower()\n",
    "    text = ''.join(c for c in text if c not in '1234567890')\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = [word_tokenize(sent) for sent in sentences]\n",
    "    words_without_stopwords = [[word for word in sent if word not in stopwords.words('english')] for sent in words]\n",
    "    \n",
    "    # Calculate TF-IDF scores\n",
    "    flat_words = [word for sent in words_without_stopwords for word in sent]\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(flat_words)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    word_scores = {}\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        word_scores[feature] = tfidf_matrix[:, i].sum()\n",
    "    \n",
    "    # Calculate sentence scores\n",
    "    sentence_scores = []\n",
    "    for sent in words_without_stopwords:\n",
    "        score = 0\n",
    "        for word in sent:\n",
    "            score += word_scores.get(word, 0)\n",
    "        sentence_scores.append(score)\n",
    "    \n",
    "    # Select top N sentences with highest scores\n",
    "    top_sentences_idx = sorted(range(len(sentence_scores)), key=lambda i: sentence_scores[i], reverse=True)[:num_sentences]\n",
    "    summary = [sentences[i] for i in top_sentences_idx]\n",
    "    return ' '.join(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "960e8746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article number: 300\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.595041\n",
      "Rouge 2 score is: 0.466667\n",
      "Rouge l score is: 0.396694\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.481013\n",
      "Rouge 2 score is: 0.331915\n",
      "Rouge l score is: 0.362869\n",
      "article number: 301\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.608696\n",
      "Rouge 2 score is: 0.500000\n",
      "Rouge l score is: 0.504348\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.530973\n",
      "Rouge 2 score is: 0.366071\n",
      "Rouge l score is: 0.300885\n",
      "article number: 302\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.680851\n",
      "Rouge 2 score is: 0.557940\n",
      "Rouge l score is: 0.365957\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.625551\n",
      "Rouge 2 score is: 0.488889\n",
      "Rouge l score is: 0.361233\n",
      "article number: 303\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.608696\n",
      "Rouge 2 score is: 0.517073\n",
      "Rouge l score is: 0.367150\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.475728\n",
      "Rouge 2 score is: 0.313725\n",
      "Rouge l score is: 0.388350\n",
      "article number: 304\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.383673\n",
      "Rouge 2 score is: 0.123457\n",
      "Rouge l score is: 0.220408\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.517544\n",
      "Rouge 2 score is: 0.389381\n",
      "Rouge l score is: 0.280702\n",
      "article number: 305\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.490909\n",
      "Rouge 2 score is: 0.284404\n",
      "Rouge l score is: 0.381818\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.679245\n",
      "Rouge 2 score is: 0.609524\n",
      "Rouge l score is: 0.405660\n",
      "article number: 306\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.345178\n",
      "Rouge 2 score is: 0.061538\n",
      "Rouge l score is: 0.172589\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.828283\n",
      "Rouge 2 score is: 0.755102\n",
      "Rouge l score is: 0.606061\n",
      "article number: 307\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.590717\n",
      "Rouge 2 score is: 0.451064\n",
      "Rouge l score is: 0.413502\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.830645\n",
      "Rouge 2 score is: 0.780488\n",
      "Rouge l score is: 0.588710\n",
      "article number: 308\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.479638\n",
      "Rouge 2 score is: 0.292237\n",
      "Rouge l score is: 0.235294\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.547945\n",
      "Rouge 2 score is: 0.405530\n",
      "Rouge l score is: 0.383562\n",
      "article number: 309\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.724490\n",
      "Rouge 2 score is: 0.618557\n",
      "Rouge l score is: 0.602041\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.728205\n",
      "Rouge 2 score is: 0.601036\n",
      "Rouge l score is: 0.605128\n",
      "article number: 310\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.761290\n",
      "Rouge 2 score is: 0.705882\n",
      "Rouge l score is: 0.374194\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.714286\n",
      "Rouge 2 score is: 0.592105\n",
      "Rouge l score is: 0.649351\n",
      "article number: 311\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.628821\n",
      "Rouge 2 score is: 0.484581\n",
      "Rouge l score is: 0.497817\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.707965\n",
      "Rouge 2 score is: 0.616071\n",
      "Rouge l score is: 0.646018\n",
      "article number: 312\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.548837\n",
      "Rouge 2 score is: 0.394366\n",
      "Rouge l score is: 0.316279\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.803828\n",
      "Rouge 2 score is: 0.734300\n",
      "Rouge l score is: 0.497608\n",
      "article number: 313\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.653465\n",
      "Rouge 2 score is: 0.570000\n",
      "Rouge l score is: 0.554455\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.752577\n",
      "Rouge 2 score is: 0.687500\n",
      "Rouge l score is: 0.752577\n",
      "article number: 314\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.429224\n",
      "Rouge 2 score is: 0.221198\n",
      "Rouge l score is: 0.301370\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.779817\n",
      "Rouge 2 score is: 0.666667\n",
      "Rouge l score is: 0.385321\n",
      "article number: 315\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.585366\n",
      "Rouge 2 score is: 0.512315\n",
      "Rouge l score is: 0.341463\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.752475\n",
      "Rouge 2 score is: 0.730000\n",
      "Rouge l score is: 0.732673\n",
      "article number: 316\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.423423\n",
      "Rouge 2 score is: 0.236364\n",
      "Rouge l score is: 0.297297\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.675799\n",
      "Rouge 2 score is: 0.589862\n",
      "Rouge l score is: 0.657534\n",
      "article number: 317\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.615385\n",
      "Rouge 2 score is: 0.504854\n",
      "Rouge l score is: 0.346154\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.863850\n",
      "Rouge 2 score is: 0.805687\n",
      "Rouge l score is: 0.732394\n",
      "article number: 318\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.615385\n",
      "Rouge 2 score is: 0.522222\n",
      "Rouge l score is: 0.560440\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.587571\n",
      "Rouge 2 score is: 0.468571\n",
      "Rouge l score is: 0.531073\n",
      "article number: 319\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.630705\n",
      "Rouge 2 score is: 0.502092\n",
      "Rouge l score is: 0.298755\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.446352\n",
      "Rouge 2 score is: 0.242424\n",
      "Rouge l score is: 0.360515\n",
      "article number: 320\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.450980\n",
      "Rouge 2 score is: 0.277228\n",
      "Rouge l score is: 0.303922\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.566845\n",
      "Rouge 2 score is: 0.486486\n",
      "Rouge l score is: 0.545455\n",
      "article number: 321\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.459459\n",
      "Rouge 2 score is: 0.263636\n",
      "Rouge l score is: 0.324324\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.611111\n",
      "Rouge 2 score is: 0.476636\n",
      "Rouge l score is: 0.490741\n",
      "article number: 322\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.377682\n",
      "Rouge 2 score is: 0.086580\n",
      "Rouge l score is: 0.206009\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.626087\n",
      "Rouge 2 score is: 0.526316\n",
      "Rouge l score is: 0.400000\n",
      "article number: 323\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.528000\n",
      "Rouge 2 score is: 0.274194\n",
      "Rouge l score is: 0.232000\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.768627\n",
      "Rouge 2 score is: 0.671937\n",
      "Rouge l score is: 0.525490\n",
      "article number: 324\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.696833\n",
      "Rouge 2 score is: 0.547945\n",
      "Rouge l score is: 0.343891\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.678899\n",
      "Rouge 2 score is: 0.518519\n",
      "Rouge l score is: 0.330275\n",
      "article number: 325\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.361991\n",
      "Rouge 2 score is: 0.228311\n",
      "Rouge l score is: 0.325792\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.688372\n",
      "Rouge 2 score is: 0.610329\n",
      "Rouge l score is: 0.660465\n",
      "article number: 326\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.609442\n",
      "Rouge 2 score is: 0.528139\n",
      "Rouge l score is: 0.506438\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.720000\n",
      "Rouge 2 score is: 0.645740\n",
      "Rouge l score is: 0.711111\n",
      "article number: 327\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.407080\n",
      "Rouge 2 score is: 0.241071\n",
      "Rouge l score is: 0.265487\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.775862\n",
      "Rouge 2 score is: 0.721739\n",
      "Rouge l score is: 0.620690\n",
      "article number: 328\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.481328\n",
      "Rouge 2 score is: 0.259414\n",
      "Rouge l score is: 0.323651\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.718750\n",
      "Rouge 2 score is: 0.669291\n",
      "Rouge l score is: 0.656250\n",
      "article number: 329\n",
      "nn accuracy\n",
      "Rouge 1 score is: 0.695187\n",
      "Rouge 2 score is: 0.562162\n",
      "Rouge l score is: 0.577540\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.625000\n",
      "Rouge 2 score is: 0.452632\n",
      "Rouge l score is: 0.541667\n",
      "Using nn\n",
      "Average Rouge 1 score is: 0.548926\n",
      "Average Rouge 2 score is: 0.393183\n",
      "Average Rouge l score is: 0.365236\n",
      "Using tf_idf only\n",
      "Average Rouge 1 score is: 0.670307\n",
      "Average Rouge 2 score is: 0.565149\n",
      "Average Rouge l score is: 0.523679\n"
     ]
    }
   ],
   "source": [
    "# precision_nn = []\n",
    "# recall_nn = []\n",
    "# precision_tf_idf = []\n",
    "# recall_tf_idf = []\n",
    "\n",
    "rouge_1_list_nn = []\n",
    "rouge_2_list_nn = []\n",
    "rouge_l_list_nn = []\n",
    "\n",
    "rouge_1_list_tf_idf = []\n",
    "rouge_2_list_tf_idf = []\n",
    "rouge_l_list_tf_idf = []\n",
    "\n",
    "for i in range(300, 330):\n",
    "    rouge_1_nn, rouge_2_nn, rouge_l_nn, rouge_1_tf_idf, rouge_2_tf_idf, rouge_l_tf_idf = test(i, 0.35)\n",
    "    \n",
    "    rouge_1_list_nn.append(rouge_1_nn)\n",
    "    rouge_2_list_nn.append(rouge_2_nn)\n",
    "    rouge_l_list_nn.append(rouge_l_nn)\n",
    "    \n",
    "    rouge_1_list_tf_idf.append(rouge_1_tf_idf)\n",
    "    rouge_2_list_tf_idf.append(rouge_2_tf_idf)\n",
    "    rouge_l_list_tf_idf.append(rouge_l_tf_idf)\n",
    "    \n",
    "print('Using nn')\n",
    "print('Average Rouge 1 score is: %f' % (np.average(rouge_1_list_nn)))\n",
    "print('Average Rouge 2 score is: %f' % (np.average(rouge_2_list_nn)))\n",
    "print('Average Rouge l score is: %f' % (np.average(rouge_l_list_nn)))\n",
    "\n",
    "print('Using tf_idf only')\n",
    "print('Average Rouge 1 score is: %f' % (np.average(rouge_1_list_tf_idf)))\n",
    "print('Average Rouge 2 score is: %f' % (np.average(rouge_2_list_tf_idf)))\n",
    "print('Average Rouge l score is: %f' % (np.average(rouge_l_list_tf_idf)))\n",
    "\n",
    "# print('Neural network accuracy: ')\n",
    "# print('Average precision score is: %f' % (np.average(precision_nn)))\n",
    "# print('Average recall score is: %f' % (np.average(recall_nn)))\n",
    "      \n",
    "# print('Classical approach accuracy using tf-idf: ')\n",
    "# print('Average precision score is: %f' % (np.average(precision_tf_idf)))\n",
    "# print('Average recall score is: %f' % (np.average(recall_tf_idf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5383d56d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
