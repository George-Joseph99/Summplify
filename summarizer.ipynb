{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56be9705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from itertools import islice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bafbdcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e3fc0c9c9a89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stopwords'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84d568dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(article):\n",
    "    # lines = article.split(\".\")   # splits the whole article into lines\n",
    "    lines = re.split(r\"\\n\\n|\\.\", article)\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    article_preprocessed = []    # list that contains the main sentences after being preprocessed \n",
    "    for line in lines:\n",
    "        line_preprocessed = []\n",
    "        words_in_line = line.split()\n",
    "        for word in words_in_line:\n",
    "            if (word not in stopwords_english and word not in string.punctuation):   # make sure word is not a stop word\n",
    "                                                                                     # and not a punctuation \n",
    "                word_stemmed = stemmer.stem(word)  \n",
    "                line_preprocessed.append(word_stemmed)\n",
    "        article_preprocessed.append(line_preprocessed)\n",
    "    article_preprocessed = [x for x in article_preprocessed if x]\n",
    "    return article_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e46cfebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"001_original.txt\", \"r\", encoding=\"utf8\") as file:\n",
    "    contents = file.read()\n",
    "    article_preprocessed = preprocessing(contents)\n",
    "    # print(article_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c60a9372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claxton hunting first major medal.\n",
      "\n",
      "British hurdler Sarah Claxton is confident she can win her first major medal at next month's European Indoor Championships in Madrid.\n",
      "\n",
      "The 25-year-old has already smashed the British record over 60m hurdles twice this season, setting a new mark of 7.96 seconds to win the AAAs title. \"I am quite confident,\" said Claxton. \"But I take each race as it comes. \"As long as I keep up my training but not do too much I think there is a chance of a medal.\" Claxton has won the national 60m hurdles title for the past three years but has struggled to translate her domestic success to the international stage. Now, the Scotland-born athlete owns the equal fifth-fastest time in the world this year. And at last week's Birmingham Grand Prix, Claxton left European medal favourite Russian Irina Shevchenko trailing in sixth spot.\n",
      "\n",
      "For the first time, Claxton has only been preparing for a campaign over the hurdles - which could explain her leap in form. In previous seasons, the 25-year-old also contested the long jump but since moving from Colchester to London she has re-focused her attentions. Claxton will see if her new training regime pays dividends at the European Indoors which take place on 5-6 March.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3714a510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_string(content):  # converts list of lists to list of strings\n",
    "    content_modified = []   # list of strings\n",
    "    for line in content:\n",
    "        line_as_string = \" \".join(line)\n",
    "        content_modified.append(line_as_string)\n",
    "#     print(content_modified)\n",
    "    return content_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "925e25cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature 1\n",
    "\n",
    "def calculate_TF_IDF(content):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()   # Create a TfidfVectorizer object\n",
    "    vectorizer.fit(content)   # Fit the vectorizer to the documents\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "#     print(\"Feature names:\")   # Print the feature names\n",
    "#     print(feature_names)   \n",
    "\n",
    "    tfidf_matrix = vectorizer.transform(content)   # Transform the documents into a TF-IDF matrix\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "#     print(\"TF-IDF matrix:\")   # Print the TF-IDF matrix\n",
    "#     print(tfidf_matrix.toarray())\n",
    "    return tfidf_matrix.toarray()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c293a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.30149709, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.43549283, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.56948856, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.49110608, 0.        , 0.        , 0.39235584,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.24999514, 0.        , 0.        , 0.28989536,\n",
       "        0.        , 0.15347562, 0.        , 0.        , 0.28989536,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.22168549, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.22168549, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.28989536, 0.        ,\n",
       "        0.24999514, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.28989536, 0.24999514, 0.        , 0.        , 0.19972681,\n",
       "        0.28989536, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.28989536, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.28989536, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.24999514, 0.        ,\n",
       "        0.        ],\n",
       "       [0.24124718, 0.24124718, 0.        , 0.        , 0.27975119,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.24124718, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.21392816, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.27975119, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.24124718,\n",
       "        0.        , 0.        , 0.24124718, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.27975119, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.27975119,\n",
       "        0.        , 0.        , 0.        , 0.27975119, 0.        ,\n",
       "        0.        , 0.        , 0.27975119, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.27975119, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.27975119, 0.        , 0.        , 0.        ,\n",
       "        0.19273787],\n",
       "       [0.        , 0.        , 0.47206897, 0.47206897, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.47206897, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.40709499, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.40709499, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.2923092 , 0.        , 0.        , 0.        ,\n",
       "        0.55213384, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.55213384, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.55213384, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.51683413, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.51683413, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.51683413, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.44569883, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.40954692, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.40954692,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.35317827,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.28216217,\n",
       "        0.        , 0.        , 0.40954692, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.40954692,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.35317827,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.25075621, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.15394285, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.2907779 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.22236037, 0.        , 0.        ,\n",
       "        0.        , 0.2907779 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.2907779 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.2907779 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.2907779 ,\n",
       "        0.2907779 , 0.2907779 , 0.        , 0.        , 0.        ,\n",
       "        0.2907779 , 0.        , 0.25075621, 0.        , 0.        ,\n",
       "        0.2907779 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.20033485],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.31283105, 0.        , 0.        ,\n",
       "        0.31283105, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.31283105, 0.        , 0.        , 0.31283105, 0.        ,\n",
       "        0.31283105, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.31283105, 0.        , 0.31283105, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31283105, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.26977404, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.31283105,\n",
       "        0.21552863],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.25532371, 0.        , 0.        , 0.25532371,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.13517279, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.19524825, 0.        , 0.        , 0.25532371,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.25532371, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.25532371, 0.        , 0.        ,\n",
       "        0.25532371, 0.        , 0.25532371, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.17590827,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.25532371,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.25532371, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.25532371,\n",
       "        0.        , 0.25532371, 0.        , 0.25532371, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.25532371, 0.        ,\n",
       "        0.        , 0.        , 0.25532371, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.32980655, 0.        ,\n",
       "        0.        , 0.17460529, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.32980655, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.32980655, 0.        , 0.        ,\n",
       "        0.        , 0.25220592, 0.        , 0.32980655, 0.32980655,\n",
       "        0.        , 0.        , 0.25220592, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.32980655, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.32980655, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.28441309, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.21760152, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.25233159, 0.        , 0.        , 0.25233159, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.25233159, 0.        , 0.        ,\n",
       "        0.        , 0.25233159, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.25233159, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.25233159,\n",
       "        0.        , 0.        , 0.        , 0.25233159, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.25233159, 0.21760152,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.25233159, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.21760152, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.25233159, 0.        ,\n",
       "        0.        , 0.        , 0.25233159, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.25233159, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.25233159, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.17384682],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.16877426, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.3187925 , 0.        ,\n",
       "        0.        , 0.24378338, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.27491498, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.3187925 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.27491498,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.3187925 , 0.3187925 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.3187925 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.3187925 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.27491498, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.27491498,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_modified = convert_list_to_string(article_preprocessed)\n",
    "calculate_TF_IDF(article_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28a6ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_each_sentence_score(tf_idf_matrix):\n",
    "    score_dict = {}   # dictionary that stores keys as summation of tf_idf scores for all word in current line \n",
    "                      # and values of dictionary as index of the line in the article\n",
    "    for index, line in enumerate(tf_idf_matrix):\n",
    "#         print(line)\n",
    "#         print('at')\n",
    "#         print(index)\n",
    "        score = np.sum(line)\n",
    "        score_dict[score] = index\n",
    "        \n",
    "    sorted_keys = sorted(score_dict.items(), reverse=True)   # sort the dictionary by keys in the descending order\n",
    "    score_dict_reversed = dict(sorted_keys)\n",
    "\n",
    "#     print(score_dict_reversed)\n",
    "    return score_dict_reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bb039f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_generation(article, score_dict_reversed, number_of_sentences):\n",
    "    lines = article.split(\".\")   # list contains strings, where these strings are original lines\n",
    "    \n",
    "    score_dict = dict(islice(score_dict_reversed.items(), number_of_sentences))   # select only certain number of lines\n",
    "                                                                                  # to be displayed \n",
    "        \n",
    "    sort_data = sorted(score_dict.items(), key=lambda x: x[1])   # sort the dictionary by value (index of lines) in\n",
    "                                                                 # the ascending order to display lines ordered as the \n",
    "                                                                 # original article \n",
    "    score_dict_ascending = dict(sort_data)\n",
    "#     print(score_dict_ascending)\n",
    "#     print(score_dict_reversed)\n",
    "    output_list = []\n",
    "    for key in score_dict_ascending:\n",
    "        output_list.append(lines[score_dict_ascending[key]])\n",
    "            \n",
    "    output_string = \".\".join(output_list)\n",
    "    output_string += \".\"\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1319f96d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "article_modified = convert_list_to_string(article_preprocessed)\n",
    "tf_idf_matrix = calculate_TF_IDF(article_modified)\n",
    "dict_scores = calculate_each_sentence_score(tf_idf_matrix)\n",
    "output = summary_generation(contents, dict_scores, 3)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f91227cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature 2\n",
    "\n",
    "def sentence_length(content):\n",
    "    max_length = 0\n",
    "    for sentence in content:\n",
    "        print(sentence)\n",
    "        if len(sentence) > max_length:\n",
    "            max_length = len(sentence)\n",
    "            \n",
    "    sentence_length_feature = []\n",
    "    for sentence in content:\n",
    "        sentence_length_feature.append(len(sentence) / max_length)\n",
    "    return sentence_length_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6271fdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['claxton', 'hunt', 'first', 'major', 'medal']\n",
      "['british', 'hurdler', 'sarah', 'claxton', 'confid', 'win', 'first', 'major', 'medal', 'next', \"month'\", 'european', 'indoor', 'championship', 'madrid']\n",
      "['the', '25-year-old', 'alreadi', 'smash', 'british', 'record', '60m', 'hurdl', 'twice', 'season,', 'set', 'new', 'mark', '7']\n",
      "['96', 'second', 'win', 'aaa', 'titl']\n",
      "['\"i', 'quit', 'confident,\"', 'said', 'claxton']\n",
      "['\"but', 'i', 'take', 'race', 'come']\n",
      "['\"a', 'long', 'i', 'keep', 'train', 'much', 'i', 'think', 'chanc', 'medal']\n",
      "['claxton', 'nation', '60m', 'hurdl', 'titl', 'past', 'three', 'year', 'struggl', 'translat', 'domest', 'success', 'intern', 'stage']\n",
      "['now,', 'scotland-born', 'athlet', 'own', 'equal', 'fifth-fastest', 'time', 'world', 'year']\n",
      "['and', 'last', \"week'\", 'birmingham', 'grand', 'prix,', 'claxton', 'left', 'european', 'medal', 'favourit', 'russian', 'irina', 'shevchenko', 'trail', 'sixth', 'spot']\n",
      "['for', 'first', 'time,', 'claxton', 'prepar', 'campaign', 'hurdl', 'could', 'explain', 'leap', 'form']\n",
      "['in', 'previou', 'seasons,', '25-year-old', 'also', 'contest', 'long', 'jump', 'sinc', 'move', 'colchest', 'london', 're-focus', 'attent']\n",
      "['claxton', 'see', 'new', 'train', 'regim', 'pay', 'dividend', 'european', 'indoor', 'take', 'place', '5-6', 'march']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.29411764705882354,\n",
       " 0.8823529411764706,\n",
       " 0.8235294117647058,\n",
       " 0.29411764705882354,\n",
       " 0.29411764705882354,\n",
       " 0.29411764705882354,\n",
       " 0.5882352941176471,\n",
       " 0.8235294117647058,\n",
       " 0.5294117647058824,\n",
       " 1.0,\n",
       " 0.6470588235294118,\n",
       " 0.8235294117647058,\n",
       " 0.7647058823529411]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_length(article_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7f3e6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claxton hunting first major medal.\n",
      "\n",
      "British hurdler Sarah Claxton is confident she can win her first major medal at next month's European Indoor Championships in Madrid.\n",
      "\n",
      "The 25-year-old has already smashed the British record over 60m hurdles twice this season, setting a new mark of 7.96 seconds to win the AAAs title. \"I am quite confident,\" said Claxton. \"But I take each race as it comes. \"As long as I keep up my training but not do too much I think there is a chance of a medal.\" Claxton has won the national 60m hurdles title for the past three years but has struggled to translate her domestic success to the international stage. Now, the Scotland-born athlete owns the equal fifth-fastest time in the world this year. And at last week's Birmingham Grand Prix, Claxton left European medal favourite Russian Irina Shevchenko trailing in sixth spot.\n",
      "\n",
      "For the first time, Claxton has only been preparing for a campaign over the hurdles - which could explain her leap in form. In previous seasons, the 25-year-old also contested the long jump but since moving from Colchester to London she has re-focused her attentions. Claxton will see if her new training regime pays dividends at the European Indoors which take place on 5-6 March.\n",
      "\n",
      "For the first time, Claxton has only been preparing for a campaign over the hurdles - which could explain her leap in form.Claxton has won the national 60m hurdles title for the past three years but has struggled to translate her domestic success to the international stage.British hurdler Sarah Claxton is confident she can win her first major medal at next month's European Indoor Championships in Madrid.Claxton will see if her new training regime pays dividends at the European Indoors which take place on 5-6 March.\"I am quite confident,\" said Claxton.\n"
     ]
    }
   ],
   "source": [
    "with open(\"001_original.txt\", \"r\", encoding=\"utf8\") as file:\n",
    "    original = file.read()\n",
    "    article_preprocessed = preprocessing(original)\n",
    "    print(original)\n",
    "\n",
    "with open(\"001_summarized.txt\", \"r\", encoding=\"utf8\") as file:\n",
    "    summarized = file.read()\n",
    "    print(summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88e1baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Y_labels(original, summarized):\n",
    "    Y_list = []\n",
    "    original_list = re.split(r\"\\n\\n|\\.\", original)\n",
    "    original_list_removed_empty = [x for x in original_list if x.strip()]   # removes empty elements\n",
    "    original_list_no_quotation = [x.replace('\"', '') for x in original_list_removed_empty]\n",
    "    original_list_no_first_space = [x.lstrip() for x in original_list_no_quotation]\n",
    "    \n",
    "    # summarized_list = summarized.replace('\"', '')\n",
    "    summarized_list = summarized.split(\".\")\n",
    "    summarized_list_removed_empty = [x for x in summarized_list if x.strip()]   # removes empty elements\n",
    "    summarized_list_no_quotation = [x.replace('\"', '') for x in summarized_list_removed_empty]\n",
    "    summarized_list_no_first_space = [x.lstrip() for x in summarized_list_no_quotation]\n",
    "    \n",
    "    print(original_list_no_first_space)\n",
    "    print(summarized_list_no_first_space)\n",
    "    \n",
    "    print(len(original_list_no_first_space))\n",
    "    print(len(summarized_list_no_first_space))\n",
    "    \n",
    "    for sentences in original_list_no_first_space:\n",
    "        if sentences in summarized_list_no_first_space:\n",
    "            Y_list.append(1)\n",
    "            print(sentences)\n",
    "            \n",
    "        else:\n",
    "            Y_list.append(0)\n",
    "    return Y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca86b71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Claxton hunting first major medal', \"British hurdler Sarah Claxton is confident she can win her first major medal at next month's European Indoor Championships in Madrid\", 'The 25-year-old has already smashed the British record over 60m hurdles twice this season, setting a new mark of 7', '96 seconds to win the AAAs title', 'I am quite confident, said Claxton', 'But I take each race as it comes', 'As long as I keep up my training but not do too much I think there is a chance of a medal', 'Claxton has won the national 60m hurdles title for the past three years but has struggled to translate her domestic success to the international stage', 'Now, the Scotland-born athlete owns the equal fifth-fastest time in the world this year', \"And at last week's Birmingham Grand Prix, Claxton left European medal favourite Russian Irina Shevchenko trailing in sixth spot\", 'For the first time, Claxton has only been preparing for a campaign over the hurdles - which could explain her leap in form', 'In previous seasons, the 25-year-old also contested the long jump but since moving from Colchester to London she has re-focused her attentions', 'Claxton will see if her new training regime pays dividends at the European Indoors which take place on 5-6 March']\n",
      "['For the first time, Claxton has only been preparing for a campaign over the hurdles - which could explain her leap in form', 'Claxton has won the national 60m hurdles title for the past three years but has struggled to translate her domestic success to the international stage', \"British hurdler Sarah Claxton is confident she can win her first major medal at next month's European Indoor Championships in Madrid\", 'Claxton will see if her new training regime pays dividends at the European Indoors which take place on 5-6 March', 'I am quite confident, said Claxton']\n",
      "13\n",
      "5\n",
      "British hurdler Sarah Claxton is confident she can win her first major medal at next month's European Indoor Championships in Madrid\n",
      "I am quite confident, said Claxton\n",
      "Claxton has won the national 60m hurdles title for the past three years but has struggled to translate her domestic success to the international stage\n",
      "For the first time, Claxton has only been preparing for a campaign over the hurdles - which could explain her leap in form\n",
      "Claxton will see if her new training regime pays dividends at the European Indoors which take place on 5-6 March\n",
      "[0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "Y = generate_Y_labels(original, summarized)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cafa7316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_X_labels(preprocessed_artcile):\n",
    "    print(preprocessed_artcile)\n",
    "    sentence_length_feature = sentence_length(preprocessed_artcile)\n",
    "    \n",
    "    article_modified = convert_list_to_string(preprocessed_artcile)\n",
    "    tf_idf_matrix = calculate_TF_IDF(article_modified)\n",
    "    score = []\n",
    "    for index, line in enumerate(tf_idf_matrix):\n",
    "#         print(line)\n",
    "#         print('at')\n",
    "#         print(index)\n",
    "        score.append(np.sum(line))\n",
    "    matrix = np.column_stack((sentence_length_feature, score))\n",
    "    # matrix = matrix[:len(matrix)-1]\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a75b733f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['claxton', 'hunt', 'first', 'major', 'medal'], ['british', 'hurdler', 'sarah', 'claxton', 'confid', 'win', 'first', 'major', 'medal', 'next', \"month'\", 'european', 'indoor', 'championship', 'madrid'], ['the', '25-year-old', 'alreadi', 'smash', 'british', 'record', '60m', 'hurdl', 'twice', 'season,', 'set', 'new', 'mark', '7'], ['96', 'second', 'win', 'aaa', 'titl'], ['\"i', 'quit', 'confident,\"', 'said', 'claxton'], ['\"but', 'i', 'take', 'race', 'come'], ['\"a', 'long', 'i', 'keep', 'train', 'much', 'i', 'think', 'chanc', 'medal'], ['claxton', 'nation', '60m', 'hurdl', 'titl', 'past', 'three', 'year', 'struggl', 'translat', 'domest', 'success', 'intern', 'stage'], ['now,', 'scotland-born', 'athlet', 'own', 'equal', 'fifth-fastest', 'time', 'world', 'year'], ['and', 'last', \"week'\", 'birmingham', 'grand', 'prix,', 'claxton', 'left', 'european', 'medal', 'favourit', 'russian', 'irina', 'shevchenko', 'trail', 'sixth', 'spot'], ['for', 'first', 'time,', 'claxton', 'prepar', 'campaign', 'hurdl', 'could', 'explain', 'leap', 'form'], ['in', 'previou', 'seasons,', '25-year-old', 'also', 'contest', 'long', 'jump', 'sinc', 'move', 'colchest', 'london', 're-focus', 'attent'], ['claxton', 'see', 'new', 'train', 'regim', 'pay', 'dividend', 'european', 'indoor', 'take', 'place', '5-6', 'march']]\n",
      "['claxton', 'hunt', 'first', 'major', 'medal']\n",
      "['british', 'hurdler', 'sarah', 'claxton', 'confid', 'win', 'first', 'major', 'medal', 'next', \"month'\", 'european', 'indoor', 'championship', 'madrid']\n",
      "['the', '25-year-old', 'alreadi', 'smash', 'british', 'record', '60m', 'hurdl', 'twice', 'season,', 'set', 'new', 'mark', '7']\n",
      "['96', 'second', 'win', 'aaa', 'titl']\n",
      "['\"i', 'quit', 'confident,\"', 'said', 'claxton']\n",
      "['\"but', 'i', 'take', 'race', 'come']\n",
      "['\"a', 'long', 'i', 'keep', 'train', 'much', 'i', 'think', 'chanc', 'medal']\n",
      "['claxton', 'nation', '60m', 'hurdl', 'titl', 'past', 'three', 'year', 'struggl', 'translat', 'domest', 'success', 'intern', 'stage']\n",
      "['now,', 'scotland-born', 'athlet', 'own', 'equal', 'fifth-fastest', 'time', 'world', 'year']\n",
      "['and', 'last', \"week'\", 'birmingham', 'grand', 'prix,', 'claxton', 'left', 'european', 'medal', 'favourit', 'russian', 'irina', 'shevchenko', 'trail', 'sixth', 'spot']\n",
      "['for', 'first', 'time,', 'claxton', 'prepar', 'campaign', 'hurdl', 'could', 'explain', 'leap', 'form']\n",
      "['in', 'previou', 'seasons,', '25-year-old', 'also', 'contest', 'long', 'jump', 'sinc', 'move', 'colchest', 'london', 're-focus', 'attent']\n",
      "['claxton', 'see', 'new', 'train', 'regim', 'pay', 'dividend', 'european', 'indoor', 'take', 'place', '5-6', 'march']\n",
      "[[0.29411765 2.18994039]\n",
      " [0.88235294 3.8258215 ]\n",
      " [0.82352941 3.85091149]\n",
      " [0.29411765 2.23039689]\n",
      " [0.29411765 1.94871072]\n",
      " [0.29411765 1.99620122]\n",
      " [0.58823529 2.6267064 ]\n",
      " [0.82352941 3.69515155]\n",
      " [0.52941176 3.30078209]\n",
      " [1.         4.08086117]\n",
      " [0.64705882 3.27207609]\n",
      " [0.82352941 4.10696207]\n",
      " [0.76470588 3.42497253]]\n"
     ]
    }
   ],
   "source": [
    "X = generate_X_labels(article_preprocessed)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "31ffa1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "13\n",
      "[0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1]\n",
      "[[0.29411765 2.18994039]\n",
      " [0.88235294 3.8258215 ]\n",
      " [0.82352941 3.85091149]\n",
      " [0.29411765 2.23039689]\n",
      " [0.29411765 1.94871072]\n",
      " [0.29411765 1.99620122]\n",
      " [0.58823529 2.6267064 ]\n",
      " [0.82352941 3.69515155]\n",
      " [0.52941176 3.30078209]\n",
      " [1.         4.08086117]\n",
      " [0.64705882 3.27207609]\n",
      " [0.82352941 4.10696207]\n",
      " [0.76470588 3.42497253]]\n"
     ]
    }
   ],
   "source": [
    "m = len(Y)  # training set size\n",
    "m2 = len(X)\n",
    "print(m)\n",
    "print(m2)\n",
    "nn_input_dim = 2  # input layer dimensionality (we have two input features)\n",
    "nn_output_dim = 1  # output layer dimensionality (we have one output)\n",
    "\n",
    "# Gradient descent parameters\n",
    "alpha = 0.2  # learning rate for gradient descent\n",
    "print(Y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e60ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # TODO 1: Compute the sigmoid function at the given x (~1 line)\n",
    "    # For example: sigmoid(2) should compute the value of sigmoid function at x = 2.\n",
    "    # Hint: Use np.exp instead of math.exp to allow for vectorization.\n",
    "    #----------------------------------------------------------------------------------------------\n",
    "    sig = (1/(1+np.exp(-x)))\n",
    "    #----------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20facec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(nn_hdim, num_passes=20000, print_loss=False):\n",
    "    \n",
    "    # This function learns parameters for the neural network and returns the model.\n",
    "    # - nn_hdim: Number of nodes in the hidden layer\n",
    "    # - num_passes: Number of iterations (epochs) through the training data for gradient descent\n",
    "    # - print_loss: If True, print the loss every 1000 iterations\n",
    "\n",
    "    # Initialize the parameters to random values. We need to learn these at the end.\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(nn_hdim, nn_input_dim) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((nn_hdim, 1))\n",
    "    W2 = np.random.randn(nn_output_dim, nn_hdim) / np.sqrt(nn_hdim)\n",
    "    b2 = np.zeros((nn_output_dim, 1))\n",
    "\n",
    "    # This is what we return at the end\n",
    "    model = {}\n",
    "\n",
    "    # Batch Gradient descent (We accumulate the loss for each training point before updating the weights)\n",
    "    # For each iteration:\n",
    "    for i in range(0, num_passes):\n",
    "        DW1 = 0\n",
    "        DW2 = 0\n",
    "        Db1 = 0\n",
    "        Db2 = 0\n",
    "        cost = 0\n",
    "        # Loop on every training example...\n",
    "        for j in range(0, m):\n",
    "            a0 = X[j, :].reshape(-1, 1)  # Every training example is a column vector.\n",
    "            y = Y[j]\n",
    "            \n",
    "            # TODO 2: Apply forward propagation on every training example a0 (a column vector 2x1) with its\n",
    "            # corresponding label y. It is required to compute z1, a1, z2, and a2\n",
    "            #----------------------------------------------------------------------------------------------\n",
    "            # Forward propagation\n",
    "            z1 = np.dot(W1 , a0 )+ b1\n",
    "            a1 = np.tanh(z1)\n",
    "            z2 = np.dot(W2 , a1) + b2\n",
    "            a2 = sigmoid(z2)\n",
    "            \n",
    "            if (i == num_passes -1 ):\n",
    "                print('True value: %f, got: %f'% (y, a2))\n",
    "            #----------------------------------------------------------------------------------------------\n",
    "\n",
    "            # TODO 3: Compute the cost/loss function for every training example (Hint: use np.log)\n",
    "\n",
    "            # ---------------------------------------------------------------------------------------------\n",
    "            cost_j = -1 * ((np.log(a2) * y + (1-y)* np.log(1-a2)))\n",
    "            # ---------------------------------------------------------------------------------------------\n",
    "\n",
    "            # TODO 4: Derive the equations of backpropagation to find dW2, db2, dW1, and db1.\n",
    "            # Hint: Check the dimensions at each step. \n",
    "            # Hint: For element-wise multiplication use *, for matrix multiplication use @\n",
    "            # Example: y = A * B performs element wise multiplication \n",
    "            #          y = A @ B performs matrix multiplication\n",
    "            # ---------------------------------------------------------------------------------------------\n",
    "            da2 =  ( -y/a2  + (1-y)/(1-a2) )\n",
    "            dz2 =  da2 * a2 * ( 1 - a2)\n",
    "            dW2 = np.dot(dz2 , a1.T)\n",
    "            db2 = dz2\n",
    "\n",
    "            da1 =  np.dot(dz2,W2).T\n",
    "            dz1 = np.multiply(da1 , 1 - np.square(a1) )\n",
    "            dW1 = np.dot(dz1 , a0.T )\n",
    "            db1 = dz1\n",
    "            # ---------------------------------------------------------------------------------------------\n",
    "            \n",
    "            # Accumulating the sum of dW1, db1, dW2, db2 and cost_j into the variables DW1, Db1, DW2, Db2 and cost\n",
    "            # for all training set. \n",
    "            DW1 += dW1\n",
    "            DW2 += dW2\n",
    "            Db2 += db2\n",
    "            Db1 += db1\n",
    "            cost += cost_j\n",
    "        \n",
    "        # Averaging DW1, DW2, Db1, Db2 and cost over the m training examples. \n",
    "        DW1 /= m\n",
    "        DW2 /= m\n",
    "        Db1 /= m\n",
    "        Db2 /= m\n",
    "        cost /= m\n",
    "\n",
    "        # TODO 5: Perform the gradient descent parameter update.\n",
    "        # ---------------------------------------------------------------------------------------------------\n",
    "        # Gradient descent parameter update\n",
    "        W1 -= alpha * DW1\n",
    "        b1 -= alpha * Db1\n",
    "        W2 -= alpha * DW2\n",
    "        b2 -= alpha * Db2\n",
    "        # ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Assign new parameters to the model\n",
    "        model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\n",
    "        # Optionally print the loss.\n",
    "        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "        if print_loss and i % 1000 == 0:\n",
    "            print(\"Loss after iteration %i: %f\" % (i, cost))\n",
    "    # print(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0664206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to predict an output (0 or 1)\n",
    "def predict(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    a0 = x.T\n",
    "    \n",
    "    # TODO 6 (aka TODO 2): Apply forward propagation on every test example a0 (a column vector 2x1) with its\n",
    "    #  corresponding label y. It is required to compute z1, a1, z2, and a2  (SAME AS TODO2).\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    z1 = np.dot(W1 , a0) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(W2 , a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    # ------------------------------------------------------------------------------------------------\n",
    "    # Applying a threshold of 0.5 (i.e. predictions greater than 0.5 are mapped to 1, and 0 otherwise)\n",
    "#     prediction = np.round(a2)\n",
    "    prediction = a2\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7508490d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.684377\n",
      "Loss after iteration 1000: 0.621156\n",
      "Loss after iteration 2000: 0.602986\n",
      "Loss after iteration 3000: 0.592979\n",
      "Loss after iteration 4000: 0.586095\n",
      "Loss after iteration 5000: 0.579899\n",
      "Loss after iteration 6000: 0.572656\n",
      "Loss after iteration 7000: 0.562535\n",
      "Loss after iteration 8000: 0.545902\n",
      "Loss after iteration 9000: 0.518866\n",
      "True value: 0.000000, got: 0.158981\n",
      "True value: 1.000000, got: 0.661866\n",
      "True value: 0.000000, got: 0.545079\n",
      "True value: 0.000000, got: 0.127479\n",
      "True value: 1.000000, got: 0.417698\n",
      "True value: 0.000000, got: 0.362994\n",
      "True value: 0.000000, got: 0.807010\n",
      "True value: 1.000000, got: 0.697076\n",
      "True value: 0.000000, got: 0.112194\n",
      "True value: 0.000000, got: 0.527637\n",
      "True value: 1.000000, got: 0.670715\n",
      "True value: 0.000000, got: 0.193314\n",
      "True value: 1.000000, got: 0.778914\n",
      "Loss after iteration 10000: 0.505606\n"
     ]
    }
   ],
   "source": [
    "model = build_model(nn_hdim=8, num_passes=10001, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b910c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"o'sullivan\", 'could', 'run', 'world'], ['sonia', \"o'sullivan\", 'indic', 'would', 'like', 'particip', 'next', \"month'\", 'world', 'cross', 'countri', 'championship', 'st', 'etienn'], ['athlet', 'ireland', 'hint', '35-year-old', 'cobh', 'runner', 'may', 'includ', 'offici', 'line-up', 'event', 'franc', '19-20', 'march'], ['provinci', 'team', 'select', 'last', \"saturday'\", 'nation', 'santri', 'offici', 'announc', 'week'], [\"o'sullivan\", 'present', 'prepar', 'london', 'marathon', '17', 'april'], ['the', 'particip', \"o'sullivan,\", 'currentili', 'train', 'base', 'australia,', 'would', 'boost', 'ireland', 'team', 'bronz', 'three', 'year', 'agio'], ['the', 'first', 'three', 'santri', 'last', 'saturday,', 'jolen', 'byrne,', 'maria', 'mccambridg', 'fionnualla', 'britton,', 'automat', 'select', 'like', 'form', 'part', 'long-cours', 'team'], [\"o'sullivan\", 'also', 'take', 'part', 'bupa', 'great', 'ireland', 'run', '9', 'april', 'dublin']]\n",
      "[\"o'sullivan\", 'could', 'run', 'world']\n",
      "['sonia', \"o'sullivan\", 'indic', 'would', 'like', 'particip', 'next', \"month'\", 'world', 'cross', 'countri', 'championship', 'st', 'etienn']\n",
      "['athlet', 'ireland', 'hint', '35-year-old', 'cobh', 'runner', 'may', 'includ', 'offici', 'line-up', 'event', 'franc', '19-20', 'march']\n",
      "['provinci', 'team', 'select', 'last', \"saturday'\", 'nation', 'santri', 'offici', 'announc', 'week']\n",
      "[\"o'sullivan\", 'present', 'prepar', 'london', 'marathon', '17', 'april']\n",
      "['the', 'particip', \"o'sullivan,\", 'currentili', 'train', 'base', 'australia,', 'would', 'boost', 'ireland', 'team', 'bronz', 'three', 'year', 'agio']\n",
      "['the', 'first', 'three', 'santri', 'last', 'saturday,', 'jolen', 'byrne,', 'maria', 'mccambridg', 'fionnualla', 'britton,', 'automat', 'select', 'like', 'form', 'part', 'long-cours', 'team']\n",
      "[\"o'sullivan\", 'also', 'take', 'part', 'bupa', 'great', 'ireland', 'run', '9', 'april', 'dublin']\n",
      "[[0.21052632 1.96305972]\n",
      " [0.73684211 3.70863766]\n",
      " [0.73684211 4.22895712]\n",
      " [0.52631579 3.14457939]\n",
      " [0.36842105 2.60874332]\n",
      " [0.78947368 3.83028404]\n",
      " [1.         4.45085113]\n",
      " [0.57894737 3.12152737]]\n",
      "[[0.57996996 0.49175823 0.17251262 0.47251117 0.50264888 0.50192533\n",
      "  0.43751963 0.59080599]]\n"
     ]
    }
   ],
   "source": [
    "with open(\"002_original.txt\", \"r\", encoding=\"utf8\") as file:\n",
    "    original_test = file.read()\n",
    "    article_preprocessed_test = preprocessing(original_test)\n",
    "    # print(original_test)\n",
    "    \n",
    "with open(\"002_summarized.txt\", \"r\", encoding=\"utf8\") as file:\n",
    "    summarized_text = file.read()\n",
    "    # print(summarized_text)\n",
    "    \n",
    "X_test = generate_X_labels(article_preprocessed_test)\n",
    "print(X_test)\n",
    "predicton = predict(model, X_test)\n",
    "print(predicton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f56f0e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"O'Sullivan could run in Worlds\", \"Sonia O'Sullivan has indicated that she would like to participate in next month's World Cross Country Championships in St Etienne\", 'Athletics Ireland have hinted that the 35-year-old Cobh runner may be included in the official line-up for the event in France on 19-20 March', \"Provincial teams were selected after last Saturday's Nationals in Santry and will be officially announced this week\", \"O'Sullivan is at present preparing for the London marathon on 17 April\", \"The participation of O'Sullivan, currentily training at her base in Australia, would boost the Ireland team who won the bronze three years agio\", 'The first three at Santry last Saturday, Jolene Byrne, Maria McCambridge and Fionnualla Britton, are automatic selections and will most likely form part of the long-course team', \"O'Sullivan will also take part in the Bupa Great Ireland Run on 9 April in Dublin\"]\n",
      "[\"The participation of O'Sullivan, currentily training at her base in Australia, would boost the Ireland team who won the bronze three years agio\", \"O'Sullivan will also take part in the Bupa Great Ireland Run on 9 April in Dublin\", \"O'Sullivan is at present preparing for the London marathon on 17 April\"]\n",
      "8\n",
      "3\n",
      "O'Sullivan is at present preparing for the London marathon on 17 April\n",
      "The participation of O'Sullivan, currentily training at her base in Australia, would boost the Ireland team who won the bronze three years agio\n",
      "O'Sullivan will also take part in the Bupa Great Ireland Run on 9 April in Dublin\n",
      "[0, 0, 0, 0, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "Y_test = generate_Y_labels(original_test, summarized_text)\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a248f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6e136a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d368d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
