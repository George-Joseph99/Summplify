{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc3b0dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/Users/apple/opt/anaconda3/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4)/charset_normalizer (2.1.1) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk\n",
    "import io\n",
    "import csv\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import nltk\n",
    "import wikiwords\n",
    "import argparse\n",
    "import re\n",
    "import requests\n",
    "import pattern\n",
    "import syllables\n",
    "import json\n",
    "import enchant\n",
    "import gensim \n",
    "import random\n",
    "import sys\n",
    "import gensim.downloader as api\n",
    "import wiki_dump_parser as parser\n",
    "import xml.etree.ElementTree as Xet\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import brown\n",
    "from difflib import SequenceMatcher\n",
    "from textblob import TextBlob\n",
    "from autocorrect import Speller\n",
    "from enchant.checker import SpellChecker\n",
    "from array import array\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from wordfreq import zipf_frequency\n",
    "from wikiwords import occdb\n",
    "from nltk.corpus import wordnet\n",
    "from py_thesaurus import Thesaurus\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from itertools import islice\n",
    "from pattern.text.en import pluralize, singularize, comparative, superlative, conjugate\n",
    "from pattern.text.en import tenses, INFINITIVE, PRESENT, PAST, FUTURE\n",
    "from google_ngram_api.Downloader import Downloader\n",
    "from gensim.corpora import WikiCorpus\n",
    "from wiki_dump_reader import Cleaner, iterate\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from colorama import Fore, Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1462fd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca442921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d28809e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_corpus = brown.words()\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "fourgram_measures = nltk.collocations.QuadgramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1386bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_freq_dict = {}\n",
    "lexicon_dict = {}\n",
    "complex_words = []\n",
    "BIGHUGE_KEY = \"105a58f9d880af14af1ca1abf6b1f996\"\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-300\")\n",
    "ppdb_filepath = \"ppdb-2.0-m-lexical\"\n",
    "three_gram_filepath = \"wp_3gram.txt\"\n",
    "dump_xml_path = 'enwiki-latest-abstract.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcf4541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eff992e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_similar(word1, word2):\n",
    "    similarity_ratio = SequenceMatcher(None, word1, word2).ratio()\n",
    "    return similarity_ratio >= 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04d974af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used for flattening list of lists to a single list\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2de4db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateWikiFreqDict(text_file):\n",
    "    wiki_freq_dict = {}\n",
    "    with open(text_file, encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            (word, freq) = line.split()\n",
    "            wiki_freq_dict[word.lower()] = freq\n",
    "    return wiki_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9241ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateEnglishFreqDict(text_file):\n",
    "    english_freq_dict = {}\n",
    "    with open(text_file) as f:\n",
    "        file = csv.reader(f, delimiter=\"\\t\")\n",
    "        for line in file:\n",
    "            word_freq_list = line[0].split(',') \n",
    "            if(word_freq_list[1]=='count'): # Skip first line\n",
    "                continue\n",
    "            word = word_freq_list[0]\n",
    "            freq = word_freq_list[1]\n",
    "            english_freq_dict[word] = int(freq)\n",
    "    return english_freq_dict\n",
    "\n",
    "english_freq_dict = generateEnglishFreqDict('ngram_freq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ebb1650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateLexiconDict(tsv_file):\n",
    "    lexicon_dict = {}\n",
    "    with open(tsv_file) as f:\n",
    "        file = csv.reader(f, delimiter=\"\\t\")\n",
    "        for line in file:\n",
    "            word = line[0]\n",
    "            score = line[1]\n",
    "            lexicon_dict[word.lower()] = score\n",
    "    return lexicon_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "078c396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_dict = generateLexiconDict('lexicon.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acdacf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_special_characters(string):\n",
    "    pattern = r'[^\\w\\s]'\n",
    "    if re.search(pattern, string):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f00b7662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_number(string):\n",
    "    pattern = r'\\d'\n",
    "    if re.search(pattern, string):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec4e410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LS step1: Complex Word Identification\n",
    "\n",
    "def word_preceding(text, word):\n",
    "    words = []\n",
    "    word_list = word_tokenize(text)\n",
    "    words.extend(word_list)\n",
    "    index = word_list.index(word)\n",
    "    return word_list[index-1] if index > 0 else None\n",
    "\n",
    "def word_following(text, word):\n",
    "    words = []\n",
    "    word_list = word_tokenize(text)\n",
    "    words.extend(word_list)\n",
    "    index = word_list.index(word)\n",
    "    return word_list[index+1] if index+1 < len(word_list) else None\n",
    "\n",
    "def complexWordIdentif(article):\n",
    "    threshold_scores_dict = {}\n",
    "    word_sentence_dict = {}\n",
    "    for sentence in sent_tokenize(article):\n",
    "        for word_ in word_tokenize(sentence):\n",
    "            word = word_.lower()\n",
    "            index = word.find(\"'\")\n",
    "            word_split_hyphen = word_.split(\"-\")\n",
    "            word_split_underscore = word_.split(\"_\")\n",
    "            if(len(word_split_hyphen)>1):\n",
    "                total_freq = 0\n",
    "                total_lexicon = 0\n",
    "                for word_hyph in word_split_hyphen:\n",
    "                    total_freq+=int(wiki_freq_dict.get(word_hyph,0))\n",
    "                    total_lexicon+=float(lexicon_dict.get(word_hyph,0))\n",
    "                wiki_freq = int(total_freq/len(word_split_hyphen))\n",
    "                lexicon_score = float(total_lexicon/len(word_split_hyphen))\n",
    "                if(wiki_freq<12000 or lexicon_score>3.0):\n",
    "                    threshold_scores_dict[word_]=1\n",
    "                    word_sentence_dict[word_] = sentence\n",
    "                    \n",
    "            elif(len(word_split_underscore)>1):\n",
    "                total_freq = 0\n",
    "                total_lexicon = 0\n",
    "                for word_un in word_split_underscore:\n",
    "                    total_freq+=int(wiki_freq_dict.get(word_un,0))\n",
    "                    total_lexicon+=float(lexicon_dict.get(word_un,0))\n",
    "                wiki_freq = int(total_freq/len(word_split_underscore))\n",
    "                lexicon_score = float(total_lexicon/len(word_split_underscore))\n",
    "                if(wiki_freq<12000 or lexicon_score>3.0):\n",
    "                    threshold_scores_dict[word_]=1\n",
    "                    word_sentence_dict[word_] = sentence\n",
    "            elif(has_special_characters(word) or has_number(word)):\n",
    "                continue\n",
    "            else:\n",
    "                threshold_scores_dict[word]=0\n",
    "                if(word in wiki_freq_dict and word in lexicon_dict):\n",
    "                    wiki_freq = int(wiki_freq_dict[word])\n",
    "                    lexicon_score = float(lexicon_dict[word])\n",
    "                    if(wiki_freq<12000 or lexicon_score>3.2):\n",
    "                        threshold_scores_dict[word_]=1\n",
    "                        word_sentence_dict[word_] = sentence\n",
    "                elif(word in wiki_freq_dict):\n",
    "                    wiki_freq = int(wiki_freq_dict[word])\n",
    "                    if(wiki_freq<12000):\n",
    "                        threshold_scores_dict[word_]=1\n",
    "                        word_sentence_dict[word_] = sentence\n",
    "                elif(word in lexicon_dict):\n",
    "                    lexicon_score = float(lexicon_dict[word])\n",
    "                    if(lexicon_score>3.0):\n",
    "                        threshold_scores_dict[word_]=1\n",
    "                        word_sentence_dict[word_] = sentence\n",
    "                else:\n",
    "                    threshold_scores_dict[word_]=1\n",
    "                    word_sentence_dict[word_] = sentence\n",
    "    return threshold_scores_dict, word_sentence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9329f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pos tag of a certain word \n",
    "\n",
    "def getPosTag(word):\n",
    "    tag = nltk.pos_tag([word])\n",
    "    return tag[0][1]\n",
    "\n",
    "def getPosTagFromSentence(string, target_word):\n",
    "    pos_tag=\"\"\n",
    "    tokens = nltk.word_tokenize(string.lower())\n",
    "    tag = nltk.pos_tag(tokens)\n",
    "    for pair in tag:\n",
    "        if(pair[0]==target_word.lower()):\n",
    "            pos_tag = pair[1]\n",
    "    return pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56f78c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used with word net dictionary\n",
    "\n",
    "def getTypeFromTag(tag):\n",
    "    # Convert all forms of noun tags to \"n\" noun type\n",
    "    if(tag==\"NN\" or tag==\"NNS\" or tag==\"NNP\" or tag==\"NNPS\"):\n",
    "        return 'n'\n",
    "    # Convert all forms of adjective tags to \"a\" adjective type\n",
    "    elif(tag==\"JJ\" or tag==\"JJR\" or tag==\"JJS\"):\n",
    "        return 'a'\n",
    "    # Convert all forms of verb tags to \"v\" verb type\n",
    "    elif(tag==\"VBZ\" or tag==\"VB\" or tag==\"VBP\" or tag==\"VBN\" or tag==\"VBG\" or tag==\"VBD\"):\n",
    "        return 'v'\n",
    "    # Convert all forms of adverb tags to \"r\" adverb type\n",
    "    elif(tag==\"RBS\" or tag==\"RB\" or tag==\"RBR\"):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5e1d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Net Synonyms\n",
    "\n",
    "def getSynWordNet(complex_word):\n",
    "    synonyms = []\n",
    "    for synset in wordnet.synsets(complex_word):\n",
    "            for l in synset.lemmas():\n",
    "                synonyms.append(l.name())\n",
    "    return list(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "160e25ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Net Synonyms with specific pos tag\n",
    "\n",
    "def getSynWordNetSpec(complex_word, pos_tag):\n",
    "    synonyms = []\n",
    "    for synset in wordnet.synsets(complex_word):\n",
    "        if(synset.pos()==pos_tag):\n",
    "            for l in synset.lemmas():\n",
    "                synonyms.append(l.name())\n",
    "    return list(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5408db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigHuge thesaurus\n",
    "\n",
    "def getSynBigHuge(complex_word):\n",
    "    bighuge_synonyms = []\n",
    "    r = requests.get(url='http://words.bighugelabs.com/api/2/'+BIGHUGE_KEY+'/'+complex_word+'/json')  \n",
    "    if(r.status_code!=404 and r.status_code!=500):\n",
    "#         print(type(r.json()),\"\\n\",r.json())\n",
    "        if(type(r.json()) is dict):\n",
    "            synonym_dict = r.json()\n",
    "            for key in synonym_dict: # key may be: noun/verb/adjective/adverb\n",
    "                synonym_list = synonym_dict[key].get(\"syn\")\n",
    "                if(synonym_list):\n",
    "                    bighuge_synonyms.append(synonym_list)\n",
    "            flatList = [element for innerList in bighuge_synonyms for element in innerList] # Convert it to a single list\n",
    "            return flatList,synonym_dict \n",
    "        else:\n",
    "            return r.json(),{}\n",
    "    else:\n",
    "        return [],{}\n",
    "            \n",
    "\n",
    "# May not use this function to avoid doing multiple requests for the same word\n",
    "# Instead, use the aboive one and make one request per word, then search for the specific pos tag inside the returned dict\n",
    "def getSynBigHugeSpec(complex_word, pos_tag):\n",
    "    bighuge_synonyms = []\n",
    "    r = requests.get(url='http://words.bighugelabs.com/api/2/'+BIGHUGE_KEY+'/'+complex_word+'/json') \n",
    "    if(r.status_code!=404):\n",
    "        if(type(r.json()) is dict):\n",
    "            synonym_dict = r.json()\n",
    "            if(pos_tag in synonym_dict):\n",
    "                bighuge_synonyms = synonym_dict[pos_tag].get(\"syn\")\n",
    "        else:\n",
    "            bighuge_synonyms = r.json()\n",
    "    return bighuge_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41406408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenOffice thesaurus\n",
    "\n",
    "# This generates a dictionary \"thesaurus\" with (word,pos) as the key and a set of synonyms as the value\n",
    "# For example:\n",
    "# thesaurus[(\"happy\",\"adj\")] = {'glad', 'pleased', 'prosperous', 'cheerful', ... }\n",
    "# possible pos: noun,verb,adj,adv\n",
    "def generateTheSaurusDict():\n",
    "    thesaurus = {}\n",
    "    with open(\"th_en_US_new.dat\") as f:\n",
    "        code = f.readline()    # Skip the file encoding\n",
    "        while(True):\n",
    "            word_count_line = f.readline()\n",
    "            if(word_count_line == \"\"):\n",
    "                break\n",
    "            (word,count) = word_count_line.split('|')\n",
    "            for i in range(0,int(count)):\n",
    "                pos_synonyms = f.readline().split(\"|\")\n",
    "                synonyms_list = pos_synonyms[1:]\n",
    "                pos = re.sub(r\"[\\([{})\\]]\", \"\",pos_synonyms[0]) # Remove te brackest surronding the pos (noun - verb - adv - adj)\n",
    "                synonyms_list = [synonym.strip() for synonym in synonyms_list] # Remove unnecessary spaces\n",
    "                if((word,pos) not in thesaurus):\n",
    "                    thesaurus[(word,pos)]=set()\n",
    "                for synonym in synonyms_list:\n",
    "                    if(synonym not in thesaurus[(word,pos)] and synonym!=word):\n",
    "                        thesaurus[(word,pos)].add(synonym)\n",
    "    return thesaurus\n",
    "\n",
    "# thesaurus = generateTheSaurusDict()\n",
    "# print(thesaurus[\"happy\",\"adj\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8596d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPDB dictionary\n",
    "\n",
    "def genPPDBdict(file_path):\n",
    "    ppdb_dict = {}\n",
    "    with open(file_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            line = row[0]\n",
    "            word_list = line.split(\"|||\")\n",
    "            source = word_list[1].strip()\n",
    "            target = word_list[2].strip()\n",
    "            score_list = word_list[3].split()\n",
    "            if source in ppdb_dict:\n",
    "                ppdb_dict[source].append(target)\n",
    "            else:\n",
    "                ppdb_dict[source] = [target]\n",
    "        return ppdb_dict\n",
    "# with open(\"ppdb-2.0-tldr\") as f:\n",
    "#         dic = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "289d90af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LS step2: Substitution Generation\n",
    "\n",
    "thesaurus = generateTheSaurusDict()\n",
    "ppdb = genPPDBdict(ppdb_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "385147cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genSubstitution(complex_word):\n",
    "    thesaurus_candidates = []\n",
    "    wordnet_candidates = []\n",
    "    wordnet_candidates = getSynWordNet(complex_word)\n",
    "    for key in thesaurus:\n",
    "        if(key[0] == complex_word):\n",
    "            thesaurus_candidates.append(thesaurus.get((key[0], key[1])))\n",
    "    return wordnet_candidates, list(thesaurus_candidates)\n",
    "    \n",
    "def genSubstitutionSpec(complex_word, sentence):\n",
    "    thesaurus_candidates = []\n",
    "    bighuge_candidates = []\n",
    "    wordnet_candidates = []\n",
    "    ppdb_candidates = []\n",
    "    pos_tag = getPosTagFromSentence(sentence, complex_word)\n",
    "    word_type = getTypeFromTag(pos_tag)\n",
    "    wordnet_candidates = getSynWordNetSpec(complex_word.lower(), word_type)\n",
    "    flat_syn_list, bighuge_dict = [],{}\n",
    "#     flat_syn_list, bighuge_dict = getSynBigHuge(complex_word)\n",
    "    if(complex_word.lower() in ppdb):\n",
    "        ppdb_candidates = ppdb[complex_word.lower()]\n",
    "    if(flat_syn_list and not bighuge_dict):\n",
    "        bighuge_candidates = flat_syn_list\n",
    "    if(word_type == 'n'):\n",
    "        thesaurus_candidates = thesaurus.get((complex_word.lower(),\"noun\"))\n",
    "        if(\"noun\" in bighuge_candidates):\n",
    "            bighuge_candidates = bighuge_dict[\"noun\"].get(\"syn\")\n",
    "    elif(word_type == 'r'):\n",
    "        thesaurus_candidates = thesaurus.get((complex_word.lower(),\"adv\"))\n",
    "        if(\"adverb\" in bighuge_candidates):\n",
    "            bighuge_candidates = bighuge_dict[\"adverb\"].get(\"syn\")\n",
    "    elif(word_type == 'v'):\n",
    "        thesaurus_candidates = thesaurus.get((complex_word.lower(),\"verb\"))\n",
    "        if(\"verb\" in bighuge_candidates):\n",
    "            bighuge_candidates = bighuge_dict[\"verb\"].get(\"syn\") \n",
    "    elif(word_type == 'a'):\n",
    "        thesaurus_candidates = thesaurus.get((complex_word.lower(),\"adj\"))\n",
    "        if(\"adjective\" in bighuge_candidates):\n",
    "            bighuge_candidates = bighuge_dict[\"adjective\"].get(\"syn\")\n",
    "\n",
    "    return wordnet_candidates, thesaurus_candidates, bighuge_candidates, ppdb_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "207e9d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterSubstitutions(word_subs_dict):\n",
    "    word_subs_dict_filtered = {}\n",
    "    wnl = WordNetLemmatizer()\n",
    "    ps = PorterStemmer()\n",
    "    for word in word_subs_dict:\n",
    "        cosine_sim = 0\n",
    "        word_lemm = wnl.lemmatize(word.lower())\n",
    "        filtered_subs_list = []\n",
    "        filtered_subs_score = []\n",
    "        subs_set = word_subs_dict[word]\n",
    "        if(len(subs_set)>0):\n",
    "            for subs in subs_set:\n",
    "                if(word.lower() in word_vectors and subs.lower() in word_vectors):     \n",
    "                    target_word_vector = word_vectors[word.lower()]\n",
    "                    substitution_vector = word_vectors[subs.lower()]\n",
    "                    cosine_sim = getCosSim(target_word_vector,substitution_vector)\n",
    "                word_list = nltk.word_tokenize(subs)\n",
    "                word_split_hyphen = subs.split(\"-\")\n",
    "                word_split_underscore = subs.split(\"_\")\n",
    "                if(len(word_list) > 1):\n",
    "                    add_word = True\n",
    "                    for word_ in word_list:\n",
    "                        word_lemm = wnl.lemmatize(word.lower())\n",
    "                        subs_lemm = wnl.lemmatize(word_.lower())\n",
    "                        if(ps.stem(word_.lower())==ps.stem(word.lower()) or subs_lemm==word_lemm or is_similar(word_.lower(), word.lower()) or is_similar(word_.lower(), word_lemm) \n",
    "                          or is_similar(subs_lemm, word_lemm) or is_similar(subs_lemm, word.lower())):\n",
    "                            add_word = False\n",
    "                            break\n",
    "                    if(add_word):\n",
    "                        filtered_subs_list.append(subs)\n",
    "                        filtered_subs_score.append(cosine_sim)\n",
    "                        \n",
    "                elif(len(word_split_hyphen)>1):\n",
    "                    add_word = True\n",
    "                    for word_ in word_split_hyphen:\n",
    "                        word_lemm = wnl.lemmatize(word.lower())\n",
    "                        subs_lemm = wnl.lemmatize(word_.lower())\n",
    "                        if(ps.stem(word_.lower())==ps.stem(word.lower()) or subs_lemm==word_lemm or is_similar(word_.lower(), word.lower()) or is_similar(word_.lower(), word_lemm) \n",
    "                          or is_similar(subs_lemm, word_lemm) or is_similar(subs_lemm, word.lower())):\n",
    "                            add_word = False\n",
    "                            break\n",
    "                    if(add_word):\n",
    "                        filtered_subs_list.append(subs)\n",
    "                        filtered_subs_score.append(cosine_sim)\n",
    "                elif(len(word_split_underscore)>1):\n",
    "                    add_word = True\n",
    "                    for word_ in word_split_underscore:\n",
    "                        word_lemm = wnl.lemmatize(word.lower())\n",
    "                        subs_lemm = wnl.lemmatize(word_.lower())\n",
    "                        if(ps.stem(word_.lower())==ps.stem(word.lower()) or subs_lemm==word_lemm or is_similar(word_.lower(), word.lower()) or is_similar(word_.lower(), word_lemm)\n",
    "                          or is_similar(subs_lemm, word_lemm) or is_similar(subs_lemm, word.lower())):\n",
    "                            add_word = False\n",
    "                            break\n",
    "                    if(add_word):\n",
    "                        filtered_subs_list.append(subs)\n",
    "                        filtered_subs_score.append(cosine_sim)\n",
    "                else: \n",
    "                    subs_lemm = wnl.lemmatize(subs.lower())\n",
    "                    if(ps.stem(subs.lower())!=ps.stem(word.lower()) and subs_lemm!=word_lemm and not is_similar(subs.lower(), word.lower()) and not is_similar(subs.lower(), word_lemm)\n",
    "                      and not is_similar(subs_lemm, word_lemm) and not is_similar(subs_lemm, word.lower())):\n",
    "                        filtered_subs_list.append(subs)\n",
    "                        filtered_subs_score.append(cosine_sim)\n",
    "        \n",
    "        if(len(filtered_subs_score)<10):\n",
    "            new_list = []\n",
    "            list_len = len(filtered_subs_score)\n",
    "            for i in range(list_len):\n",
    "                max_index = filtered_subs_score.index(max(filtered_subs_score))\n",
    "                new_list.append(filtered_subs_list[max_index])\n",
    "                filtered_subs_score[max_index] = -1\n",
    "        else:\n",
    "            # Get the 10 highest values\n",
    "            new_list = []\n",
    "            for i in range(10):\n",
    "                max_index = filtered_subs_score.index(max(filtered_subs_score))\n",
    "                new_list.append(filtered_subs_list[max_index])\n",
    "                # Remove the maximum value from the list\n",
    "                filtered_subs_score[max_index] = -1\n",
    "        word_subs_dict_filtered[word] = new_list\n",
    "    return word_subs_dict_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7a3b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertGrammStructure(word_subs_dict, word_sentence_dict):\n",
    "    modified_word_subs_dict = word_subs_dict.copy()\n",
    "    for word in word_subs_dict:\n",
    "        subs_list = word_subs_dict[word]\n",
    "        word_tag = getPosTagFromSentence(word_sentence_dict[word],word)\n",
    "        word_type = getTypeFromTag(word_tag)\n",
    "        if(word_type == 'n'): #NOUNS\n",
    "            modified_subs = []\n",
    "            for subs in subs_list:\n",
    "                subs_tag = getPosTag(subs)\n",
    "                subs_type = getTypeFromTag(subs_tag)\n",
    "                if(word_tag==\"NN\" and subs_tag==\"NNS\"):\n",
    "                    new_subs = singularize(subs)\n",
    "                    modified_subs.append(new_subs)\n",
    "                elif(word_tag==\"NNS\" and subs_tag==\"NN\"):\n",
    "                    new_subs = pluralize(subs)\n",
    "                    modified_subs.append(new_subs)\n",
    "                elif((word_tag==\"NNS\" and subs_tag==\"NNS\") or (word_tag==\"NN\" and subs_tag==\"NN\")):\n",
    "                    modified_subs.append(subs)\n",
    "            modified_word_subs_dict[word] = modified_subs\n",
    "        elif(word_type == 'a'): #ADJECTIVES\n",
    "            modified_subs = []\n",
    "            for subs in subs_list:\n",
    "                subs_tag = getPosTag(subs)\n",
    "                subs_type = getTypeFromTag(subs_tag)\n",
    "                if(word_tag==\"JJR\" and (subs_tag==\"JJS\" or subs_tag==\"JJ\")):\n",
    "                    new_subs = comparative(subs)\n",
    "                    modified_subs.append(new_subs)\n",
    "                elif(word_tag==\"JJS\" and (subs_tag==\"JJR\" or subs_tag==\"JJ\")):\n",
    "                    new_subs = superlative(subs)\n",
    "                    modified_subs.append(new_subs)\n",
    "                elif((word_tag==\"JJR\" and subs_tag==\"JJR\") or (word_tag==\"JJS\" and subs_tag==\"JJS\") or (word_tag==\"JJ\" and subs_tag==\"JJ\")):\n",
    "                    modified_subs.append(subs)\n",
    "            modified_word_subs_dict[word] = modified_subs\n",
    "#         elif(word_type == 'v' and word_tag !=\"VBD\" and word_tag !=\"VBN\"): #VERBS\n",
    "#             modified_subs = []\n",
    "#             try:\n",
    "# #                 print(word)\n",
    "# #                 print(tenses(word))\n",
    "#                 complex_tense = tenses(word)\n",
    "#                 for subs in subs_list:\n",
    "#                     subs_tokens = nltk.word_tokenize(subs)\n",
    "#     #                 subs_split = subs.split(\" \")\n",
    "#                     subs_tag = getPosTag(subs)\n",
    "#                     subs_type = getTypeFromTag(subs_tag)\n",
    "#                     if(word_tag==\"VBD\" or word_tag==\"VBN\"): #-->PAST\n",
    "#                         if(len(subs_tokens)>1):\n",
    "#                             first_half_tense = tenses(subs)\n",
    "#                             if(subs_type=='v'):\n",
    "#                                 new_word = conjugate(subs_tokens[0], tense=PAST)\n",
    "#                                 subs_tokens[0] = new_word\n",
    "#                                 new_subs = ' '.join(subs_tokens)\n",
    "#                                 modified_subs.append(new_subs)\n",
    "#                         else:\n",
    "#                             new_subs = conjugate(subs, tense=PAST)\n",
    "#                             modified_subs.append(new_subs)\n",
    "#                     elif(word_tag==\"VBP\" or word_tag==\"VBZ\"): #-->PRESENT\n",
    "#                         new_subs = conjugate(subs, tense=PRESENT)\n",
    "#                         modified_subs.append(new_subs)\n",
    "#                     elif((len(complex_tense)>0 and complex_tense[0][0]==\"infinitive\") or word_tag==\"VBG\"): #-->INFINITIVE\n",
    "#                         new_subs = conjugate(subs, tense=INFINITIVE)\n",
    "#                         modified_subs.append(new_subs)\n",
    "#                     elif(len(complex_tense)>0 and complex_tense[0][0]==\"future\"): #-->FUTURE\n",
    "#     #                     print(word_tag, complex_tense, complex_tense[0][0], word, \"\\n\")\n",
    "#                         new_subs = conjugate(subs, tense=FUTURE)\n",
    "#                         modified_subs.append(new_subs)   \n",
    "#                     else:\n",
    "#                         modified_subs.append(subs)  \n",
    "#                 modified_word_subs_dict[word] = modified_subs\n",
    "#             except StopIteration as e:\n",
    "#                 print(\"exceptionn\") \n",
    "    return modified_word_subs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e42d924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LS Step3: Substitution Ranking\n",
    "\n",
    "def getThreeGramFreq(phrase):\n",
    "    freq = 0\n",
    "    with open(\"three_gram.txt\", \"r\") as infile:\n",
    "        reader = csv.reader(infile, delimiter=\"\\t\")\n",
    "        for i, row in enumerate(reader):\n",
    "            if row[0] == phrase:\n",
    "                freq = int(row[1])\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb1e4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate3GramDict(filepath):\n",
    "    three_gram_dict = {}\n",
    "    max_val = 0\n",
    "    \n",
    "    with open(filepath, newline='', encoding='utf-8') as f:\n",
    "        file = csv.reader(f, delimiter=\"\\t\")\n",
    "        for line in file:\n",
    "            if(len(line)>=2):\n",
    "                phrase = line[0]\n",
    "                freq = line[2]\n",
    "                if(phrase.lower() not in three_gram_dict):\n",
    "                    three_gram_dict[phrase.lower()] = int(freq)\n",
    "                else:\n",
    "                    three_gram_dict[phrase.lower()] += int(freq)\n",
    "    \n",
    "    # Filter the dict\n",
    "    keys_tobe_removed = []\n",
    "    for key in three_gram_dict:\n",
    "        word_list = key.split()\n",
    "        freq = three_gram_dict[key]\n",
    "        if(freq>max_val):\n",
    "            max_val = freq\n",
    "        if(freq<=50 or len(word_list)>3):\n",
    "            keys_tobe_removed.append(key)\n",
    "            \n",
    "    for key in keys_tobe_removed:\n",
    "        three_gram_dict.pop(key, None)\n",
    "        \n",
    "    return three_gram_dict\n",
    "                \n",
    "three_gram_dict_google = generate3GramDict('googlebooks-eng-all-3gram-20090715-0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "10b1d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNgramScore(phrase, start_year=1999, end_year=2000, corpus=26, smoothing=0):\n",
    "    avg_score = 0\n",
    "    google_ngram_url = \"https://books.google.com/ngrams/json?content=\"+phrase+'&year_start=' + str(start_year) + '&year_end=' + str(end_year) + '&corpus=' + str(corpus) + '&smoothing=' + str(smoothing)\n",
    "#     response = requests.get(google_ngram_url) # Commenting this line just for now\n",
    "    response = ''\n",
    "    if(response):\n",
    "        output = response.json()\n",
    "        scores_list = []\n",
    "        if(len(output) > 0):\n",
    "            scores_list = output[0]['timeseries']\n",
    "            if(len(scores_list) > 1):\n",
    "                avg_score = (scores_list[0] + scores_list[1])/2\n",
    "    return avg_score\n",
    "            \n",
    "def extractFeaturesFromWord(target_word, word_phrase_dict):\n",
    "    \n",
    "    # Features we have are:\n",
    "    # lex_exist_flag, complexity_score, word_length, syllable_count, wiki_freq, ngram_score\n",
    "    lex_exist_flag = -1\n",
    "    complexity_score = -1\n",
    "    word_length = -1\n",
    "    syllable_count = -1\n",
    "    wiki_freq = -1\n",
    "    ngram_score = -1\n",
    "    brown_freq = -1\n",
    "    eng_word_freq = -1\n",
    "    sem_ratio = -1\n",
    "\n",
    "    \n",
    "    # Before extracting features, check if it's a multi-word phrase, if so, we work on the longest word\n",
    "    longest_word = ''\n",
    "    word_list = nltk.word_tokenize(target_word)\n",
    "    word_split_hyphen = target_word.split(\"-\")\n",
    "    word_split_underscore = target_word.split(\"_\")\n",
    "    if(len(word_list) > 1):\n",
    "        longest_word = ''\n",
    "        max_length = 0\n",
    "        for word in word_list:\n",
    "            if(len(word)>max_length):\n",
    "                max_length=len(word)\n",
    "                longest_word = word\n",
    "    elif(len(word_split_hyphen) > 1):\n",
    "        longest_word = ''\n",
    "        max_length = 0\n",
    "        for word in word_split_hyphen:\n",
    "            if(len(word)>max_length):\n",
    "                max_length=len(word)\n",
    "                longest_word = word\n",
    "    elif(len(word_split_underscore) > 1):\n",
    "        longest_word = ''\n",
    "        max_length = 0\n",
    "        for word in word_split_underscore:\n",
    "            if(len(word)>max_length):\n",
    "                max_length=len(word)\n",
    "                longest_word = word\n",
    "        \n",
    "    # EXTRACTING FEATURES\n",
    "    \n",
    "    # Feature 1: Binary number representing word's presence in lexicon (1:existent 0:non existent)\n",
    "#     if(target_word in lexicon_dict):\n",
    "#         lex_exist_flag = 1\n",
    "#     else:\n",
    "#         lex_exist_flag = 0\n",
    "            \n",
    "    # Feature 2: Complexity score of the word in the lexicon\n",
    "    if(target_word in lexicon_dict):\n",
    "        complexity_score = float(lexicon_dict[target_word])\n",
    "    else:\n",
    "        complexity_score = 0 # If word is not found in lexicon, set its complexity score with 0 also\n",
    "      \n",
    "    # Feature 3: word length (character count)\n",
    "    word_length = len(target_word)\n",
    "    \n",
    "    # Feature 4: Syllable count\n",
    "    syllable_count = syllables.estimate(target_word)     \n",
    "    \n",
    "    # Feature 5: Frequency with respect to Wiki-Frequency\n",
    "    if(target_word in wiki_freq_dict):\n",
    "        wiki_freq = math.log(int(wiki_freq_dict[target_word]))\n",
    "    else:\n",
    "        wiki_freq = 0\n",
    "        \n",
    "    # Feature 6: Google Ngram average score\n",
    "    if(target_word in word_phrase_dict):\n",
    "        three_words = word_phrase_dict[target_word]\n",
    "        phrase = three_words[0] + \" \" + three_words[1] + \" \" + three_words[2]\n",
    "        ngram_score = three_gram_dict_google.get(phrase,0)\n",
    "#         ngram_score = getThreeGramFreq(phrase)\n",
    "    else:\n",
    "        ngram_score = 0\n",
    "        \n",
    "    # Feature 7: Frequency with respect to English Corpus\n",
    "    eng_word_freq = english_freq_dict.get(target_word, 0)\n",
    "#     brown_freq = brown_corpus.count(target_word)\n",
    "    \n",
    "    return [complexity_score, word_length, syllable_count, wiki_freq, ngram_score, eng_word_freq] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff2ad373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair wise features\n",
    "\n",
    "def getCosSim(vec_1, vec_2):\n",
    "    cos_sim = cosine_similarity([vec_1], [vec_2])\n",
    "    return cos_sim[0][0]\n",
    "\n",
    "def getJaccSim(word1, word2): \n",
    "    vectorizer = CountVectorizer()\n",
    "    corpus = [word1, word2]\n",
    "    bag_of_words = vectorizer.fit_transform(corpus)\n",
    "    word1_vector = bag_of_words[0].toarray()[0]\n",
    "    word2_vector = bag_of_words[1].toarray()[0]\n",
    "    intersection = sum([1 for i, j in zip(word1_vector, word2_vector) if i == j and i == 1])\n",
    "    union = sum([1 for i, j in zip(word1_vector, word2_vector) if i == 1 or j == 1])\n",
    "    similarity = intersection / union\n",
    "    return similarity\n",
    "    \n",
    "def similarityRatio(word1, word2):\n",
    "    similarity_ratio = SequenceMatcher(None, word1, word2).ratio()\n",
    "    return similarity_ratio \n",
    "\n",
    "def semanticSimilarityRatio(word1, word2):\n",
    "    word1_synsets = wordnet.synsets(word1)\n",
    "    word2_synsets = wordnet.synsets(word2)\n",
    "    max_similarity = 0\n",
    "    for word1_synset in word1_synsets:\n",
    "        for word2_synset in word2_synsets:\n",
    "            similarity = word1_synset.wup_similarity(word2_synset)\n",
    "            if similarity is not None and similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "    return max_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe89faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # TODO 1: Compute the sigmoid function at the given x (~1 line)\n",
    "    # For example: sigmoid(2) should compute the value of sigmoid function at x = 2.\n",
    "    # Hint: Use np.exp instead of math.exp to allow for vectorization.\n",
    "    #----------------------------------------------------------------------------------------------\n",
    "    sig = (1/(1+np.exp(-x)))\n",
    "    #----------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return sig\n",
    "\n",
    "def batch_normalize(X, eps=1e-5):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    var = np.var(X, axis=0)\n",
    "    X_norm = (X - mean) / np.sqrt(var + eps)\n",
    "    return X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7dd580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "feature_matrix_ = []\n",
    "cosine_similarities = []\n",
    "similarity_ratios = []\n",
    "sem_similarity_ratios = []\n",
    "candidates_matrix = []\n",
    "sentence_list = []\n",
    "word_candCount_dict = {}\n",
    "word_phrase_dict = {}\n",
    "scores_matrix = []\n",
    "complex_word_list = []\n",
    "target_word = ''\n",
    "X = []\n",
    "num_features = 8\n",
    "num_single_features = 6\n",
    "with open('BenchLS.txt') as f:\n",
    "        data_reader = csv.reader(f, delimiter='\\t')\n",
    "        data = list(data_reader)\n",
    "data = list(data)\n",
    "# print(data)\n",
    "train_count = math.ceil(0.8*len(data))\n",
    "test_count = len(data) - train_count\n",
    "train_data = data[:train_count]\n",
    "test_data = data[train_count:]\n",
    "\n",
    "num_candidates = 0\n",
    "for line in train_data:\n",
    "    candidates = []\n",
    "    scores = []\n",
    "    scores_candidates = line[3:]\n",
    "    sentence = line[0]\n",
    "    sentence_list.append(sentence)\n",
    "    target_word = line[1]\n",
    "    complex_word_list.append(target_word)\n",
    "    num_candidates += len(scores_candidates)\n",
    "    word_candCount_dict[target_word] = len(scores_candidates)\n",
    "    for score_candidate in scores_candidates:\n",
    "        candidates.append(score_candidate[2:])\n",
    "        scores.append(int(score_candidate[0]))\n",
    "    candidates_matrix.append(candidates)\n",
    "    scores_matrix.append(scores)\n",
    "\n",
    "indx = 0\n",
    "for line in candidates_matrix:\n",
    "    sentence = sentence_list[indx]\n",
    "    complex_word = complex_word_list[indx]\n",
    "    current_candidates_features = []\n",
    "    for candidate in line:\n",
    "        prev_word = word_preceding(sentence.lower(), complex_word)\n",
    "        next_word = word_following(sentence.lower(), complex_word)\n",
    "        if(prev_word and next_word):\n",
    "            word_phrase_dict[candidate] = [prev_word, candidate, next_word]\n",
    "        feature_list_ = extractFeaturesFromWord(candidate,word_phrase_dict)\n",
    "        current_candidates_features.append(feature_list_)\n",
    "        if(target_word in word_vectors and candidate in word_vectors):     \n",
    "            target_word_vector = word_vectors[target_word]\n",
    "            substitution_vector = word_vectors[candidate]\n",
    "            cos_similarity = getCosSim(target_word_vector, substitution_vector)\n",
    "        else:\n",
    "            cos_similarity = 0\n",
    "        cosine_similarities.append(cos_similarity)\n",
    "#         similarity_ratios.append(similarityRatio(target_word, candidate))\n",
    "        sem_similarity_ratios.append(semanticSimilarityRatio(target_word.lower(), candidate.lower()))\n",
    "    current_candidates_features = np.array(current_candidates_features).reshape(len(line),num_single_features)\n",
    "    max_in_column = np.max(current_candidates_features,axis=0)\n",
    "    for i in range(num_single_features):\n",
    "        if(max_in_column[i] != 0):\n",
    "            current_candidates_features[:, i] = current_candidates_features[:, i]/max_in_column[i]\n",
    "    feature_matrix_.append(current_candidates_features)\n",
    "#     print(feature_matrix_)\n",
    "    indx+=1\n",
    "\n",
    "feature_matrix_ = np.concatenate(feature_matrix_, axis=0)\n",
    "cosine_similarities = np.array(cosine_similarities).reshape(num_candidates,1)\n",
    "# similarity_ratios = np.array(similarity_ratios).reshape(num_candidates,1)\n",
    "sem_similarity_ratios = np.array(sem_similarity_ratios).reshape(num_candidates,1)\n",
    "\n",
    "\n",
    "X = np.hstack((feature_matrix_,cosine_similarities, sem_similarity_ratios))\n",
    "\n",
    "Y = np.ones((len(feature_matrix_),1))\n",
    "y_indx = 0\n",
    "max_score = max(flatten(scores_matrix))\n",
    "for line in scores_matrix: \n",
    "    for score in line:\n",
    "        Y[y_indx]=score/max_score\n",
    "        y_indx+=1\n",
    "\n",
    "m = len(X)  # training set size\n",
    "nn_input_dim = 8  # input layer dimensionality (we have seven input features)\n",
    "nn_output_dim = 1  # output layer dimensionality (we have one output :: score)\n",
    "\n",
    "# Gradient descent parameters\n",
    "alpha = 0.1 # learning rate for gradient descent\n",
    "\n",
    "def buildModel(nn_hdim, num_passes=20000, print_loss=False):\n",
    "\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(nn_hdim, nn_input_dim) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((nn_hdim, 1))\n",
    "    W2 = np.random.randn(nn_output_dim, nn_hdim) / np.sqrt(nn_hdim)\n",
    "    b2 = np.zeros((nn_output_dim, 1))\n",
    "    \n",
    "    model = {}\n",
    "\n",
    "    for i in range(0, num_passes):\n",
    "        DW1 = 0\n",
    "        DW2 = 0\n",
    "        Db1 = 0\n",
    "        Db2 = 0\n",
    "        cost = 0\n",
    "\n",
    "        for j in range(0, m):\n",
    "            a0 = X[j, :].reshape(-1, 1) \n",
    "            y = Y[j]\n",
    "            \n",
    "            # Forward propagation\n",
    "            z1 = np.dot(W1 , a0 )+ b1\n",
    "            a1 = np.tanh(z1)\n",
    "            z2 = np.dot(W2 , a1) + b2\n",
    "            a2 = z2\n",
    "\n",
    "            cost_j = abs(y-a2)\n",
    "       \n",
    "            da2 =  ( -y/a2  + (1-y)/(1-a2) )\n",
    "            dz2 =  da2 * a2 * ( 1 - a2)\n",
    "            dW2 = np.dot(dz2 , a1.T)\n",
    "            db2 = dz2\n",
    "\n",
    "            da1 =  np.dot(dz2,W2).T\n",
    "            dz1 = np.multiply(da1 , 1 - np.square(a1) )\n",
    "            dW1 = np.dot(dz1 , a0.T )\n",
    "            db1 = dz1\n",
    "\n",
    "            DW1 += dW1\n",
    "            DW2 += dW2\n",
    "            Db2 += db2\n",
    "            Db1 += db1\n",
    "            cost += cost_j\n",
    "            \n",
    "        # Averaging DW1, DW2, Db1, Db2 and cost over the m training examples. \n",
    "        DW1 /= m\n",
    "        DW2 /= m\n",
    "        Db1 /= m\n",
    "        Db2 /= m\n",
    "        cost /= m\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        W1 -= alpha * DW1\n",
    "        b1 -= alpha * Db1\n",
    "        W2 -= alpha * DW2\n",
    "        b2 -= alpha * Db2\n",
    "\n",
    "        # Assign new parameters to the model\n",
    "        model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print(\"Loss after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f4babd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.438118\n",
      "Loss after iteration 100: 0.172856\n",
      "Loss after iteration 200: 0.171690\n",
      "Loss after iteration 300: 0.170697\n",
      "Loss after iteration 400: 0.169864\n",
      "Loss after iteration 500: 0.169164\n",
      "Loss after iteration 600: 0.168556\n",
      "Loss after iteration 700: 0.168030\n",
      "Loss after iteration 800: 0.167585\n",
      "Loss after iteration 900: 0.167205\n",
      "Loss after iteration 1000: 0.166888\n",
      "Loss after iteration 1100: 0.166629\n",
      "Loss after iteration 1200: 0.166410\n",
      "Loss after iteration 1300: 0.166233\n",
      "Loss after iteration 1400: 0.166080\n",
      "Loss after iteration 1500: 0.165940\n",
      "Loss after iteration 1600: 0.165812\n",
      "Loss after iteration 1700: 0.165698\n",
      "Loss after iteration 1800: 0.165592\n",
      "Loss after iteration 1900: 0.165494\n",
      "Loss after iteration 2000: 0.165403\n",
      "Loss after iteration 2100: 0.165318\n",
      "Loss after iteration 2200: 0.165240\n",
      "Loss after iteration 2300: 0.165169\n",
      "Loss after iteration 2400: 0.165102\n",
      "Loss after iteration 2500: 0.165040\n",
      "Loss after iteration 2600: 0.164981\n",
      "Loss after iteration 2700: 0.164925\n",
      "Loss after iteration 2800: 0.164873\n",
      "Loss after iteration 2900: 0.164824\n",
      "Loss after iteration 3000: 0.164776\n",
      "Loss after iteration 3100: 0.164731\n",
      "Loss after iteration 3200: 0.164689\n",
      "Loss after iteration 3300: 0.164649\n",
      "Loss after iteration 3400: 0.164611\n",
      "Loss after iteration 3500: 0.164575\n",
      "Loss after iteration 3600: 0.164541\n",
      "Loss after iteration 3700: 0.164507\n",
      "Loss after iteration 3800: 0.164475\n",
      "Loss after iteration 3900: 0.164445\n",
      "Loss after iteration 4000: 0.164415\n",
      "Loss after iteration 4100: 0.164386\n",
      "Loss after iteration 4200: 0.164359\n",
      "Loss after iteration 4300: 0.164332\n",
      "Loss after iteration 4400: 0.164306\n",
      "Loss after iteration 4500: 0.164282\n",
      "Loss after iteration 4600: 0.164258\n",
      "Loss after iteration 4700: 0.164236\n",
      "Loss after iteration 4800: 0.164214\n",
      "Loss after iteration 4900: 0.164193\n",
      "Loss after iteration 5000: 0.164173\n",
      "Loss after iteration 5100: 0.164153\n",
      "Loss after iteration 5200: 0.164134\n",
      "Loss after iteration 5300: 0.164116\n",
      "Loss after iteration 5400: 0.164098\n",
      "Loss after iteration 5500: 0.164081\n",
      "Loss after iteration 5600: 0.164064\n",
      "Loss after iteration 5700: 0.164049\n",
      "Loss after iteration 5800: 0.164033\n",
      "Loss after iteration 5900: 0.164018\n",
      "Loss after iteration 6000: 0.164004\n",
      "Loss after iteration 6100: 0.163990\n",
      "Loss after iteration 6200: 0.163976\n",
      "Loss after iteration 6300: 0.163963\n",
      "Loss after iteration 6400: 0.163950\n",
      "Loss after iteration 6500: 0.163938\n",
      "Loss after iteration 6600: 0.163925\n",
      "Loss after iteration 6700: 0.163913\n",
      "Loss after iteration 6800: 0.163901\n",
      "Loss after iteration 6900: 0.163890\n",
      "Loss after iteration 7000: 0.163879\n",
      "Loss after iteration 7100: 0.163868\n",
      "Loss after iteration 7200: 0.163857\n",
      "Loss after iteration 7300: 0.163846\n",
      "Loss after iteration 7400: 0.163836\n",
      "Loss after iteration 7500: 0.163826\n",
      "Loss after iteration 7600: 0.163816\n",
      "Loss after iteration 7700: 0.163806\n",
      "Loss after iteration 7800: 0.163797\n",
      "Loss after iteration 7900: 0.163788\n",
      "Loss after iteration 8000: 0.163779\n",
      "Loss after iteration 8100: 0.163770\n",
      "Loss after iteration 8200: 0.163761\n",
      "Loss after iteration 8300: 0.163753\n",
      "Loss after iteration 8400: 0.163745\n",
      "Loss after iteration 8500: 0.163737\n",
      "Loss after iteration 8600: 0.163729\n",
      "Loss after iteration 8700: 0.163721\n",
      "Loss after iteration 8800: 0.163713\n",
      "Loss after iteration 8900: 0.163706\n",
      "Loss after iteration 9000: 0.163699\n",
      "Loss after iteration 9100: 0.163692\n",
      "Loss after iteration 9200: 0.163685\n",
      "Loss after iteration 9300: 0.163678\n",
      "Loss after iteration 9400: 0.163672\n",
      "Loss after iteration 9500: 0.163665\n",
      "Loss after iteration 9600: 0.163659\n",
      "Loss after iteration 9700: 0.163652\n",
      "Loss after iteration 9800: 0.163646\n",
      "Loss after iteration 9900: 0.163640\n",
      "Loss after iteration 10000: 0.163633\n"
     ]
    }
   ],
   "source": [
    "model = buildModel(8,10001,True)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "84fe645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    a0 = x.T\n",
    "\n",
    "    # Forward propagation\n",
    "    z1 = np.dot(W1 , a0) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = np.dot(W2 , a1) + b2\n",
    "    a2 = z2\n",
    "\n",
    "    prediction = a2\n",
    "    \n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "98aa3e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(text, printText=False):\n",
    "    \n",
    "    model = {'W1': [[ 0.63365875,  0.16030412,  0.36717862,  0.8089881 ,  0.65940084,\n",
    "        -0.32384469,  0.34409706, -0.04354326],\n",
    "       [-0.06464107,  0.12455244, -0.02127153,  0.54114634,  0.29371113,\n",
    "         0.06616963,  0.08400417,  0.10566778],\n",
    "       [ 0.48766571, -0.01203013,  0.05418793, -0.21048091, -0.87645255,\n",
    "         0.28714001,  0.15683364, -0.24613814],\n",
    "       [ 0.87057353, -0.51580978,  0.07007664, -0.12642449,  0.52250484,\n",
    "         0.49608236,  0.1170586 ,  0.11434842],\n",
    "       [-0.29943519, -0.64664839, -0.10002842,  0.02818125,  0.46409198,\n",
    "         0.30872823, -0.19980613, -0.12439785],\n",
    "       [-0.3509533 , -0.4751845 , -0.5887333 ,  0.71075325, -0.17305347,\n",
    "        -0.17136134, -0.45971738,  0.28070881],\n",
    "       [-0.53240267, -0.08483448, -0.28283218,  0.11454216, -0.2042574 ,\n",
    "        -0.3480407 ,  0.04765682,  0.14710273],\n",
    "       [ 0.02348486,  0.09976941, -0.20684344, -0.16056915, -0.24838074,\n",
    "        -0.14804525, -0.25721902, -0.6220139 ]], 'b1': [[ 0.06432677],\n",
    "       [-0.14982467],\n",
    "       [-0.32462762],\n",
    "       [ 0.1150695 ],\n",
    "       [-0.16710937],\n",
    "       [-0.00354433],\n",
    "       [ 0.12164962],\n",
    "       [ 0.04035753]], 'W2': [[ 0.01965975, -0.37816695, -0.40750705,  0.27023303, -0.14980607,\n",
    "        -0.08531321,  0.23804193,  0.14477529]], 'b2': [[0.42471895]]}\n",
    "    \n",
    "    color_red = \"\\033[31m\"\n",
    "    color_green = \"\\033[32m\"\n",
    "    color_reset = \"\\033[0m\"\n",
    "    \n",
    "    num_features = 8\n",
    "    num_single_features = 6\n",
    "    vowels = \"aeiouAEIOU\"\n",
    "    prediction = []\n",
    "    word_replace_dict = {}\n",
    "    thresh_scores = {} \n",
    "    thresh_scores, word_sentence_dict = complexWordIdentif(text)\n",
    "#     print(word_sentence_dict)\n",
    "    word_subst_dict = {}\n",
    "    new_text = text # initialize the new string with the original one\n",
    "    for word in word_sentence_dict:\n",
    "        word_subst_dict[word] = set()\n",
    "        sentence = word_sentence_dict[word]\n",
    "        word_net_cand, thesaurus_cand, bighuge_cand, ppdb_candidates = genSubstitutionSpec(word, sentence)\n",
    "        if(word_net_cand):\n",
    "            word_subst_dict[word].update(word_net_cand)\n",
    "        if(thesaurus_cand):\n",
    "            word_subst_dict[word].update(thesaurus_cand)\n",
    "        if(bighuge_cand):\n",
    "            word_subst_dict[word].update(bighuge_cand)\n",
    "        if(ppdb_candidates):\n",
    "            word_subst_dict[word].update(ppdb_candidates)\n",
    "    filtered_subs_dict = filterSubstitutions(word_subst_dict)\n",
    "#     print(word_subst_dict)\n",
    "#     print(filtered_subs_dict)\n",
    "    modified_word_subs_dict = convertGrammStructure(filtered_subs_dict, word_sentence_dict) \n",
    "#     print(modified_word_subs_dict)\n",
    "    \n",
    "    # EXTRACT FEATURES\n",
    "    for target_word in modified_word_subs_dict:\n",
    "        word_phrase_dict = {}\n",
    "        feature_matrix = []\n",
    "        cosine_similarities = []\n",
    "        similarity_ratios = []\n",
    "        sem_similarity_ratios = []\n",
    "        candidates = []\n",
    "        sentence = word_sentence_dict[target_word]\n",
    "        candidates = modified_word_subs_dict[target_word]\n",
    "#         print(candidates)\n",
    "#         candidates = [target_word] + modified_word_subs_dict[target_word]\n",
    "        if(len(candidates)>0):\n",
    "            for candidate in candidates:\n",
    "                three_gram_phrase = ''\n",
    "                prev_word = word_preceding(sentence, target_word)\n",
    "                next_word = word_following(sentence, target_word)\n",
    "                if(prev_word and next_word):\n",
    "                    three_gram_phrase = prev_word + \" \" + candidate + \" \" + next_word\n",
    "                    word_phrase_dict[candidate] = [prev_word, candidate, next_word]\n",
    "                features_list = extractFeaturesFromWord(candidate, word_phrase_dict)\n",
    "                if(target_word.lower() in word_vectors and candidate.lower() in word_vectors):     \n",
    "                    target_word_vector = word_vectors[target_word.lower()]\n",
    "                    substitution_vector = word_vectors[candidate.lower()]\n",
    "                    cos_similarity = getCosSim(target_word_vector, substitution_vector)\n",
    "                else:\n",
    "                    cos_similarity = 0\n",
    "                similarity_ratios.append(similarityRatio(target_word.lower(), candidate.lower()))\n",
    "                sem_similarity_ratios.append(semanticSimilarityRatio(target_word.lower(), candidate.lower()))\n",
    "                cosine_similarities.append(cos_similarity)\n",
    "                feature_matrix.append(features_list)\n",
    "            cosine_similarities = np.array(cosine_similarities).reshape(len(feature_matrix),1)\n",
    "            similarity_ratios = np.array(similarity_ratios).reshape(len(feature_matrix),1)\n",
    "            sem_similarity_ratios = np.array(sem_similarity_ratios).reshape(len(feature_matrix),1)\n",
    "            X = np.hstack((feature_matrix,cosine_similarities,sem_similarity_ratios))\n",
    "            max_in_column = np.max(X,axis=0)\n",
    "            for i in range(num_features):\n",
    "                if(max_in_column[i] != 0):\n",
    "                    X[:, i] = X[:, i]/max_in_column[i]\n",
    "#             print(candidates)\n",
    "#             print(X)\n",
    "            prediction = predict(model, X)\n",
    "#             print(candidates)\n",
    "#             print(prediction)\n",
    "            min_value = min(prediction)\n",
    "            prediction_list = prediction.tolist()\n",
    "            min_index=prediction_list.index(min_value)\n",
    "#             print(prediction_list,\"\\n\",candidates)\n",
    "            chosen_candidate = candidates[min_index]\n",
    "            if(target_word[0].isupper()):\n",
    "                chosen_candidate = chosen_candidate[0].upper() + chosen_candidate[1:]\n",
    "            if(target_word.isupper()):\n",
    "                chosen_candidate = chosen_candidate.upper()\n",
    "            if(prev_word == 'a' and chosen_candidate[0] in vowels):\n",
    "                new_text = new_text.replace('a '+ target_word, 'an ' + chosen_candidate)\n",
    "            elif(prev_word == 'an' and chosen_candidate[0] not in vowels):\n",
    "                new_text = new_text.replace('an '+ target_word, 'a ' + chosen_candidate)\n",
    "            else:\n",
    "                new_text = new_text.replace(target_word, chosen_candidate)\n",
    "            word_replace_dict[target_word] = chosen_candidate\n",
    "#             print(len(text), len(new_text))\n",
    "\n",
    "    if(printText):\n",
    "        text_list = word_tokenize(text)\n",
    "        newtext_list = word_tokenize(new_text)\n",
    "        for word in text_list:\n",
    "            if(word in word_sentence_dict):\n",
    "                print(f\"{color_red}{word}{color_reset}\",end=\" \")\n",
    "            else:\n",
    "                print(f\"{word}\",end=\" \")\n",
    "        print(\"\\n\")\n",
    "        for i in range(len(text_list)):  \n",
    "            if(text_list[i] in word_sentence_dict):\n",
    "                print(f\"{color_green}{newtext_list[i]}{color_reset}\",end=\" \")\n",
    "            else:\n",
    "                print(f\"{newtext_list[i]}\",end=\" \")\n",
    "        print(\"\\n\")\n",
    "            \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3216b0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whenever there is a condition with results in \u001b[31mcardiac\u001b[0m \u001b[31mdysfunction\u001b[0m stroke volume will \u001b[31meventually\u001b[0m declined . \n",
      "\n",
      "whenever there is a condition with results in \u001b[32mcardiac\u001b[0m \u001b[32mpathology\u001b[0m stroke volume will \u001b[32myet\u001b[0m declined . \n",
      "\n",
      "the \u001b[31mproductivity\u001b[0m of crops and \u001b[31mpastures\u001b[0m , as well as the health of other \u001b[31mvegetation\u001b[0m , \u001b[31mdeclines\u001b[0m as the \u001b[31msaline\u001b[0m \u001b[31mwatertable\u001b[0m reaches their root zones . \n",
      "\n",
      "the \u001b[32mindustry\u001b[0m of crops and \u001b[32mgrasses\u001b[0m , as well as the health of other \u001b[32mgrowth\u001b[0m , \u001b[32mdecreased\u001b[0m as the \u001b[32misosmotic\u001b[0m \u001b[32msolution\u001b[0m watertable reaches their root zones \n",
      "\n",
      "unhappy with being called to task for \u001b[31mdeclining\u001b[0m scores , political educators complained that the tests were unfair to some , so they \u001b[31mwatered\u001b[0m them down \u001b[31m-\u001b[0m \u001b[31m-\u001b[0m as if 2+2=4 was being challenged as unfair on racial , gender or other \u001b[31msocial-oriented\u001b[0m measures . \n",
      "\n",
      "unhappy with being called to task for \u001b[32mincreasing\u001b[0m scores , political educators complained that the tests were unfair to some , so they \u001b[32mirrigated\u001b[0m them down \u001b[32mls\u001b[0m \u001b[32mls\u001b[0m as if 2+2=4 was being challenged as unfair on racial , gender or other \u001b[32msociallsoriented\u001b[0m measures . \n",
      "\n",
      "in particular , per \u001b[31mcapita\u001b[0m \u001b[31mincomes\u001b[0m in Africa have declined relative to the industrial countries and in some countries have declined in absolute terms . \n",
      "\n",
      "in particular , per \u001b[32mhead\u001b[0m \u001b[32mbenefits\u001b[0m in Africa have declined relative to the industrial countries and in some countries have declined in absolute terms . \n",
      "\n",
      "the \u001b[31mGARCH\u001b[0m model is an infinite order arch model with a \u001b[31mgeometrically\u001b[0m \u001b[31mdeclining\u001b[0m set of weights . \n",
      "\n",
      "the \u001b[32mGARCH\u001b[0m model is an infinite order arch model with a \u001b[32mgeometrically\u001b[0m \u001b[32mincreasing\u001b[0m set of weights . \n",
      "\n",
      "i will not endure the laughter of the Court . \n",
      "\n",
      "i will not endure the laughter of the Court . \n",
      "\n",
      "and this combined annual \u001b[31mincidence-rate\u001b[0m will endure \u001b[31mindefinitely\u001b[0m too , until the annual \u001b[31mproduction-rate\u001b[0m is altered . \n",
      "\n",
      "and this combined annual \u001b[32mincidence-rate\u001b[0m will endure \u001b[32mforever\u001b[0m too , until the annual \u001b[32mproduction-rate\u001b[0m is altered . \n",
      "\n",
      "positive attitude \u001b[31movercomes\u001b[0m \u001b[31mpelvic\u001b[0m pain Former athlete Janet \u001b[31mH\u001b[0m has \u001b[31mendured\u001b[0m 20 \u001b[31moperations\u001b[0m and has been reduced to \u001b[31msurviving\u001b[0m on a special liquid diet after being \u001b[31mstricken\u001b[0m 24 years ago with \u001b[31mendometriosis\u001b[0m . \n",
      "\n",
      "positive attitude \u001b[32meliminates\u001b[0m \u001b[32mpelvic\u001b[0m pain Former athlete Janet \u001b[32mELEMENT\u001b[0m has \u001b[32mexperienced\u001b[0m 20 \u001b[32mactivities\u001b[0m and has been reduced to \u001b[32mlast\u001b[0m on a special liquid diet after being \u001b[32maffected\u001b[0m 24 years ago with \u001b[32mpathology\u001b[0m . \n",
      "\n",
      "what was n't my fault was the \u001b[31mordeal\u001b[0m I had to endure to fix it . \n",
      "\n",
      "what was n't my fault was the \u001b[32mexperience\u001b[0m I had to endure to fix it . \n",
      "\n",
      "developers are \u001b[31mroutinely\u001b[0m asked to endure the \u001b[31mhardships\u001b[0m of design \u001b[31mextremes\u001b[0m , such as a \u001b[31mlow-memory\u001b[0m \u001b[31mfootprint\u001b[0m , in order to reduce total system cost . \n",
      "\n",
      "developers are \u001b[32mfrequently\u001b[0m asked to endure the \u001b[32mdifficulties\u001b[0m of design \u001b[32mextremes\u001b[0m , such as a \u001b[32mlow-memory\u001b[0m \u001b[32marea\u001b[0m , in order to reduce total system cost . \n",
      "\n",
      "the White \u001b[31mSOx\u001b[0m , who are \u001b[31menduring\u001b[0m what I call the Curse of the 1919 Black \u001b[31mSOx\u001b[0m ? \n",
      "\n",
      "the White \u001b[32mSOx\u001b[0m , who are \u001b[32msuffering\u001b[0m what I call the Curse of the 1919 Black \u001b[32mSOx\u001b[0m ? \n",
      "\n",
      "and , to a large extent I have Harvard to thank for that \u001b[31m--\u001b[0m for the extraordinary role models I studied and studied under and the \u001b[31mfriendships\u001b[0m I made that have \u001b[31mendured\u001b[0m . \n",
      "\n",
      "and , to a large extent I have Harvard to thank for that \u001b[32m--\u001b[0m for the extraordinary role models I studied and studied under and the \u001b[32mrelations\u001b[0m I made that have \u001b[32mexperienced\u001b[0m . \n",
      "\n",
      "now here he is , \u001b[31mgowned\u001b[0m and on a \u001b[31mgurney\u001b[0m , forced to endure a medical procedure all because of some soup . \n",
      "\n",
      "now here he is , \u001b[32mgowned\u001b[0m and on a \u001b[32mstretcher\u001b[0m , forced to endure a medical procedure all because of some soup . \n",
      "\n",
      "however , people often \u001b[31mendured\u001b[0m \u001b[31minadequate\u001b[0m AT because of the lack of a viable \u001b[31malternative\u001b[0m . \n",
      "\n",
      "however , people often \u001b[32mexperienced\u001b[0m \u001b[32minsufficient\u001b[0m AT because of the lack of a viable \u001b[32mchoice\u001b[0m . \n",
      "\n",
      "the campaign should challenge the notion that \u001b[31mprostate\u001b[0m problems are an \u001b[31minevitable\u001b[0m part of \u001b[31mageing\u001b[0m to be \u001b[31mendured\u001b[0m rather than \u001b[31minvestigated\u001b[0m . \n",
      "\n",
      "the campaign should challenge the notion that \u001b[32mendocrine\u001b[0m problems are a \u001b[32mnecessary\u001b[0m part of \u001b[32mmature\u001b[0m to be \u001b[32mexperienced\u001b[0m rather than \u001b[32minvolved\u001b[0m . \n",
      "\n",
      "the \u001b[31mHispanic\u001b[0m FAMILY : an \u001b[31muntapped\u001b[0m resource One step that schools can take is to understand and tap into an important and \u001b[31munderutilized\u001b[0m source of strength \u001b[31m--\u001b[0m the \u001b[31mHispanic\u001b[0m extended family . \n",
      "\n",
      "the \u001b[32mAmerican\u001b[0m FAMILY : an \u001b[32munused\u001b[0m resource One step that schools can take is to understand and tap into an important and \u001b[32mutilised\u001b[0m source of strength \u001b[32m--\u001b[0m the \u001b[32mAmerican\u001b[0m extended family . \n",
      "\n",
      ") and recording an extended \u001b[31msoundcheck\u001b[0m on a little tape machine that may end up playing a big role on our next record . \n",
      "\n",
      ") and recording an extended \u001b[32msoundcheck\u001b[0m on a little tape machine that may end up playing a big role on our next record . \n",
      "\n",
      "discussion top In this study , we have shown that \u001b[31mrecombinant\u001b[0m gp120 increases the \u001b[31mpermeability\u001b[0m of brain \u001b[31mendothelium\u001b[0m cultures to \u001b[31malbumin\u001b[0m , most likely by altering the cell \u001b[31mmorphology\u001b[0m and \u001b[31minducing\u001b[0m extended \u001b[31mintercellular\u001b[0m gaps that allow passage of \u001b[31mmacromolecules\u001b[0m and probably \u001b[31mfacilitate\u001b[0m cell \u001b[31mtransmigration\u001b[0m across the \u001b[31mBBB\u001b[0m . \n",
      "\n",
      "discussion top In this study , we have shown that \u001b[32mrecombinant\u001b[0m gp120 increases the \u001b[32mporosity\u001b[0m of brain \u001b[32mepithelial\u001b[0m tissue cultures \u001b[32mto\u001b[0m albumin , most likely by altering the \u001b[32mcell\u001b[0m structure \u001b[32mand\u001b[0m inducing \u001b[32mextended\u001b[0m intercellular gaps that allow passage \u001b[32mof\u001b[0m supermolecules and \u001b[32mprobably\u001b[0m help \u001b[32mcell\u001b[0m reincarnation across \u001b[32mthe\u001b[0m BBB \n",
      "\n",
      "\u001b[31mnuclear\u001b[0m Family This unit \u001b[31mobserves\u001b[0m and \u001b[31massesses\u001b[0m the effectiveness of childcare in the \u001b[31mnuclear\u001b[0m family home environment , looking at the childrens ' experience with their parents and extended family . \n",
      "\n",
      "\u001b[32mcentral\u001b[0m Family This unit \u001b[32mdescribes\u001b[0m and \u001b[32mevaluate\u001b[0m the effectiveness of childcare in the \u001b[32mcentral\u001b[0m family home environment , looking at the childrens ' experience with their parents and extended family . \n",
      "\n",
      "at work : calling in sick , coming in late , leaving early , taking extended breaks , giving up your \u001b[31mterritory\u001b[0m ( `` Why do n't you let Mary take over that \u001b[31massignment\u001b[0m for you `` ) . \n",
      "\n",
      "at work : calling in sick , coming in late , leaving early , taking extended breaks , giving up your \u001b[32marea\u001b[0m ( `` Why do n't you let Mary take over that \u001b[32mdecision\u001b[0m for you `` ) . \n",
      "\n",
      "( \u001b[31mgrieve\u001b[0m 1927a , 19 ) [ 9 ] This book therefore marked the beginning of what was to be an extended battle between \u001b[31mMacDiarmid\u001b[0m , as well as the other \u001b[31mCelticist\u001b[0m \u001b[31mfundamentalists\u001b[0m in the \u001b[31mNPs\u001b[0m , and the \u001b[31mconservative\u001b[0m \u001b[31mnationalists\u001b[0m of the Scottish Party . \n",
      "\n",
      "( \u001b[32msuffer\u001b[0m 1927a , 19 ) [ 9 ] This book therefore marked the beginning of what was to be an extended battle between \u001b[32mMacDiarmid\u001b[0m , as well as the other \u001b[32mCelticist\u001b[0m \u001b[32mmuslims\u001b[0m in the \u001b[32mNeptunium\u001b[0m , and the \u001b[32mcautious\u001b[0m \u001b[32mpatriots\u001b[0m of the Scottish Party . \n",
      "\n",
      "not surprisingly , \u001b[31mEllison\u001b[0m spends a \u001b[31mconsiderable\u001b[0m amount of time describing the results of his own extended \u001b[31mfieldwork\u001b[0m among \u001b[31mEfe\u001b[0m \u001b[31mhunter-gatherers\u001b[0m and \u001b[31mLese\u001b[0m \u001b[31mhorticulturalists\u001b[0m living in the \u001b[31mrainforests\u001b[0m of the \u001b[31mDemocratic\u001b[0m Republic of \u001b[31mCongo\u001b[0m . \n",
      "\n",
      "not surprisingly , \u001b[32mAuthor\u001b[0m spends a \u001b[32msignificant\u001b[0m amount of time describing the results of his own extended \u001b[32mfortification\u001b[0m among \u001b[32mEfe\u001b[0m \u001b[32mhunter-gatherers\u001b[0m and \u001b[32mLese\u001b[0m \u001b[32mgrowers\u001b[0m living in the \u001b[32mtropics\u001b[0m of the \u001b[32mCommon\u001b[0m Republic of \u001b[32mRiver\u001b[0m . \n",
      "\n",
      "extended \u001b[31mwarranties\u001b[0m for desktop computers tend to be \u001b[31mrip-offs\u001b[0m , because you would usually pay an independent \u001b[31mtechnician\u001b[0m much less in repair costs over the same period that a typical extended warranty costs to provide repair cover . \n",
      "\n",
      "extended \u001b[32minterests\u001b[0m for desktop computers tend to be \u001b[32mheists\u001b[0m , because you would usually pay an independent \u001b[32mtechnician\u001b[0m much less in repair costs over the same period that a typical extended warranty costs to provide repair cover . \n",
      "\n",
      "I 've always been kind of at loose ends whenever I 'm given an extended period of free time . \n",
      "\n",
      "I 've always been kind of at loose ends whenever I 'm given an extended period of free time . \n",
      "\n",
      "i was taking a \u001b[31mprescription\u001b[0m \u001b[31mmedication\u001b[0m over an extended length of time under the \u001b[31msupervision\u001b[0m of a \u001b[31mphysician\u001b[0m who \u001b[31mspecializes\u001b[0m in the \u001b[31mmanagement\u001b[0m of pain . \n",
      "\n",
      "i was taking a \u001b[32mdirection\u001b[0m \u001b[32mtherapy\u001b[0m over an extended length of time under the \u001b[32mcontrol\u001b[0m of a \u001b[32mdoctor\u001b[0m who \u001b[32mcorporation\u001b[0m in the \u001b[32mcontrol\u001b[0m of pain . \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this external communication could be with external trading partners or within the \u001b[31morganization\u001b[0m . \n",
      "\n",
      "this external communication could be with external trading partners or within the \u001b[32massociation\u001b[0m . \n",
      "\n",
      "that is , we can be glad so long as we do not believe in external \u001b[31mauthority\u001b[0m ( for example , the \u001b[31mtestimony\u001b[0m of others ) alone for our religious beliefs . \n",
      "\n",
      "that is , we can be glad so long as we do not believe in external \u001b[32mpower\u001b[0m ( for example , the \u001b[32mevidence\u001b[0m of others ) alone for our religious beliefs . \n",
      "\n",
      "the external \u001b[31mappearance\u001b[0m of each type of \u001b[31mextinguisher\u001b[0m maybe different and each carries its own instructions for use . \n",
      "\n",
      "the external \u001b[32mshow\u001b[0m of each type of \u001b[32mextinguisher\u001b[0m maybe different and each carries its own instructions for use . \n",
      "\n",
      "external resources : assess external resources and the community 's \u001b[31minstitutional\u001b[0m , political , technical , legal and \u001b[31mfiscal\u001b[0m capability to engage in \u001b[31mhazard\u001b[0m \u001b[31mmitigation\u001b[0m and response . \n",
      "\n",
      "external resources : assess external resources and the community 's \u001b[32morganizational\u001b[0m , political , technical , legal and \u001b[32mfinancial\u001b[0m capability to engage in \u001b[32mrisk\u001b[0m \u001b[32msolution\u001b[0m and response . \n",
      "\n",
      "\u001b[31mQ\u001b[0m Can you kindly advise if Team roles change with different team members and external factors e.g . company setting , cultural context ? \n",
      "\n",
      "\u001b[32mQUESTION\u001b[0m Can you kindly advise if Team roles change with different team members and external factors e.g . company setting , cultural context ? \n",
      "\n",
      "external links to other Internet sites should not be \u001b[31mconstrued\u001b[0m as an \u001b[31mendorsement\u001b[0m of the views or privacy policies \u001b[31mcontained\u001b[0m \u001b[31mtherein\u001b[0m . \n",
      "\n",
      "external links to other Internet sites should not be \u001b[32minterpretation\u001b[0m as a \u001b[32msupport\u001b[0m of the views or privacy policies \u001b[32mwithin\u001b[0m \u001b[32mcontained\u001b[0m . \n",
      "\n",
      "when careful measurements are taken of current flows in the generator and in the external \u001b[31mcircuit\u001b[0m , evidence suggests that electric charges are appearing at the \u001b[31mperiphery\u001b[0m of the generator and \u001b[31mdisappearing\u001b[0m at the center of the generator that do not actually pass through the generator . \n",
      "\n",
      "when careful measurements are taken of current flows in the generator and in the external \u001b[32mtrack\u001b[0m , evidence suggests that electric charges are appearing at the \u001b[32mboundary\u001b[0m of the generator and \u001b[32mvanish\u001b[0m at the center of the generator that do not actually pass through the generator . \n",
      "\n",
      "many spiritual traditions still \u001b[31meschew\u001b[0m \u001b[31mtechnological\u001b[0m \u001b[31mtechniques\u001b[0m in favor of more `` \u001b[31mauthentic\u001b[0m `` ones ; so \u001b[31munassisted\u001b[0m \u001b[31mmeditation\u001b[0m is said to be better than use of mind machines , electronic \u001b[31mbiofeedback\u001b[0m , or \u001b[31mpsychotropic\u001b[0m chemicals , and the use of \u001b[31mtechnology\u001b[0m as a sign of \u001b[31mattachment\u001b[0m to `` \u001b[31mworldly\u001b[0m `` things or \u001b[31mreliance\u001b[0m on external \u001b[31mcrutches\u001b[0m rather than `` \u001b[31mself-realization\u001b[0m . \n",
      "\n",
      "many spiritual traditions still \u001b[32mavoid\u001b[0m \u001b[32mscientific\u001b[0m \u001b[32mmethods\u001b[0m in favor of more `` \u001b[32mreliable\u001b[0m `` ones ; so \u001b[32mnaked\u001b[0m \u001b[32mreflection\u001b[0m is said to be better than use of mind machines , electronic \u001b[32mtraining\u001b[0m program , \u001b[32mor\u001b[0m psychoactive chemicals , and the use \u001b[32mof\u001b[0m application as a sign \u001b[32mof\u001b[0m connection to \u001b[32m``\u001b[0m worldly `` things \u001b[32mor\u001b[0m dependence on \u001b[32mexternal\u001b[0m crutches rather than \u001b[32m``\u001b[0m fulfillment \n",
      "\n",
      "`` nothing came to our attention indicating evidence of influence or pressure from internal or external sources , `` he wrote . \n",
      "\n",
      "`` nothing came to our attention indicating evidence of influence or pressure from internal or external sources , `` he wrote . \n",
      "\n",
      "the end of the Cold War could not but sharply increase its international \u001b[31misolation\u001b[0m and \u001b[31mlegitimacy\u001b[0m \u001b[31mdeficit\u001b[0m ; \u001b[31mbartering\u001b[0m its \u001b[31manti-communist\u001b[0m \u001b[31mcredentials\u001b[0m for external assistance was no longer a \u001b[31mfeasible\u001b[0m option . \n",
      "\n",
      "the end of the Cold War could not but sharply increase its international \u001b[32mseparation\u001b[0m and \u001b[32mcredibility\u001b[0m \u001b[32mliability\u001b[0m ; \u001b[32mbartering\u001b[0m its \u001b[32manti-communist\u001b[0m \u001b[32mqualifications\u001b[0m for external assistance was no longer a \u001b[32mpossible\u001b[0m option . \n",
      "\n",
      "the \u001b[31memancipation\u001b[0m of women can only be completed when a \u001b[31mfundamental\u001b[0m \u001b[31mtransformation\u001b[0m of living is effected ; and \u001b[31mlife-styles\u001b[0m will change only with the \u001b[31mfundamental\u001b[0m \u001b[31mtransformation\u001b[0m of all \u001b[31mproduction\u001b[0m and the \u001b[31mestablishment\u001b[0m of a \u001b[31mcommunist\u001b[0m economy . \n",
      "\n",
      "the \u001b[32mrelease\u001b[0m of women can only be completed when an \u001b[32mimportant\u001b[0m \u001b[32mchange\u001b[0m of living is effected ; and \u001b[32mmodus_vivendis\u001b[0m will change only with the \u001b[32mfundamental\u001b[0m \u001b[32mchange\u001b[0m of all \u001b[32mindustry\u001b[0m and the \u001b[32morganization\u001b[0m of a \u001b[32mcommunist\u001b[0m economy . \n",
      "\n",
      "but as the approach moves into the commercial realm , especially the \u001b[31msoftware\u001b[0m business , it 's challenging \u001b[31mfundamental\u001b[0m notions about who owns ideas and how best to \u001b[31mfoster\u001b[0m \u001b[31minnovation\u001b[0m . \n",
      "\n",
      "but as the approach moves into the commercial realm , especially the \u001b[32mwork\u001b[0m business , it 's challenging \u001b[32mimportant\u001b[0m notions about who owns ideas and how best to \u001b[32mpromote\u001b[0m \u001b[32mindustry\u001b[0m . \n",
      "\n",
      "the occasion is often a personal failure of \u001b[31mfundamental\u001b[0m understanding , where understanding \u001b[31mtranscends\u001b[0m the grammar of \u001b[31massociative\u001b[0m \u001b[31mconsciousness\u001b[0m . \n",
      "\n",
      "the occasion is often a personal failure of \u001b[32mimportant\u001b[0m understanding , where understanding \u001b[32msupersedes\u001b[0m the grammar of \u001b[32massociative\u001b[0m \u001b[32mawareness\u001b[0m . \n",
      "\n",
      "first , he clearly \u001b[31mdistinguishes\u001b[0m norms or values within the social network as having a \u001b[31mfundamental\u001b[0m impact on the development of social capital . \n",
      "\n",
      "first , he clearly \u001b[32mdiffers\u001b[0m norms or values within the social network as having an \u001b[32mimportant\u001b[0m impact on the development of social capital . \n",
      "\n",
      "`` We 're trying to build things that work and to prove \u001b[31mtheorems\u001b[0m about why they work.We 're trying to \u001b[31marticulate\u001b[0m the more \u001b[31mfundamental\u001b[0m concepts of the field . `` \u001b[31mKoditschek\u001b[0m mentions Alan \u001b[31mTuring\u001b[0m , who in the early 20th century first \u001b[31menvisioned\u001b[0m machines capable of \u001b[31mmethodical\u001b[0m , \u001b[31mgoal-directed\u001b[0m \u001b[31mcomputation\u001b[0m . \n",
      "\n",
      "`` We 're trying to build things that work and to prove \u001b[32mtheorems\u001b[0m about why they work.We 're trying to \u001b[32mtell\u001b[0m the more \u001b[32mimportant\u001b[0m concepts of the field . `` \u001b[32mKoditschek\u001b[0m mentions Alan \u001b[32mTuring\u001b[0m , who in the early 20th century first \u001b[32mproposed\u001b[0m machines capable of \u001b[32mmethodical\u001b[0m , \u001b[32mgoal-directed\u001b[0m \u001b[32mprocess\u001b[0m . \n",
      "\n",
      "two of the \u001b[31mfundamental\u001b[0m limitations of northern \u001b[31mEIAs\u001b[0m are the lack of \u001b[31madequate\u001b[0m \u001b[31mecological\u001b[0m baseline data and the lack of an \u001b[31madequate\u001b[0m \u001b[31mframework\u001b[0m or method to link \u001b[31mecological\u001b[0m and social \u001b[31mcomponents\u001b[0m of the environment . \n",
      "\n",
      "two of the \u001b[32mimportant\u001b[0m limitations of northern \u001b[32mEIAs\u001b[0m are the lack of \u001b[32msuitable\u001b[0m \u001b[32menvironmental\u001b[0m baseline data and the lack of an \u001b[32msuitable\u001b[0m \u001b[32magreement\u001b[0m or method to link \u001b[32menvironmental\u001b[0m and social \u001b[32mproducts\u001b[0m of the environment . \n",
      "\n",
      "we generally do not permit \u001b[31mtransferral\u001b[0m into the second year due to the unique nature of our course and \u001b[31mfundamental\u001b[0m skills that are learnt in the first year , but you are welcome to discuss your case with the admissions Tutor . \n",
      "\n",
      "we generally do not permit \u001b[32mtransferral\u001b[0m into the second year due to the unique nature of our course and \u001b[32mimportant\u001b[0m skills that are learnt in the first year , but you are welcome to discuss your case with the admissions Tutor . \n",
      "\n",
      "would you rather have made the camera that shot Citizen Kane , or make Citizen Kane ? `` Silicon alley is unique , but the \u001b[31mfundamental\u001b[0m processes that have \u001b[31mfacilitated\u001b[0m its \u001b[31memergence\u001b[0m are not . \n",
      "\n",
      "would you rather have made the camera that shot Citizen Kane , or make Citizen Kane ? `` Silicon alley is unique , but the \u001b[32mimportant\u001b[0m processes that have \u001b[32mencouraged\u001b[0m its \u001b[32mgrowth\u001b[0m are not . \n",
      "\n",
      "they will also support two \u001b[31mfundamental\u001b[0m instructional objectives . \n",
      "\n",
      "they will also support two \u001b[32mimportant\u001b[0m instructional objectives . \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the training demands outlined above ; the needs of \u001b[31mgovernors\u001b[0m for trained , \u001b[31mwell-led\u001b[0m , and appropriately organized forces to defend against terrorist threats ; and possible political pressures not to deploy some \u001b[31mARNG\u001b[0m units during times of \u001b[31mheightened\u001b[0m terrorist threats will demand a \u001b[31mfundamental\u001b[0m \u001b[31mreexamination\u001b[0m of Army \u001b[31morganization\u001b[0m . \n",
      "\n",
      "the training demands outlined above ; the needs of \u001b[32mdirectors\u001b[0m for trained , \u001b[32mwell-led\u001b[0m , and appropriately organized forces to defend against terrorist threats ; and possible political pressures not to deploy some \u001b[32mARNG\u001b[0m units during times of \u001b[32mintensified\u001b[0m terrorist threats will demand an \u001b[32mimportant\u001b[0m \u001b[32mreview\u001b[0m of Army \u001b[32massociation\u001b[0m . \n",
      "\n",
      "to \u001b[31mfacilitate\u001b[0m that \u001b[31minvolvement\u001b[0m , the University will maintain appropriate processes within which students can communicate their views by \u001b[31mformal\u001b[0m and informal means , and can , directly or through their representatives , actively \u001b[31mcontribute\u001b[0m to decisions affecting the University community . \n",
      "\n",
      "to \u001b[32mhelp\u001b[0m that \u001b[32minterest\u001b[0m , the University will maintain appropriate processes within which students can communicate their views by \u001b[32mliterary\u001b[0m and inliterary means , and can , directly or through their representatives , actively \u001b[32msource\u001b[0m to decisions affecting the University community . \n",
      "\n",
      "this is one of the most important areas of informal \u001b[31memployment\u001b[0m and can take place in \u001b[31mhome-based\u001b[0m , \u001b[31mmicro-enterprise-based\u001b[0m or in \u001b[31mfactory-based\u001b[0m locations . \n",
      "\n",
      "this is one of the most important areas of informal \u001b[32mbusiness\u001b[0m and can take place in \u001b[32mhome-based\u001b[0m , \u001b[32mmicro-enterprise-based\u001b[0m or in \u001b[32mfactory-based\u001b[0m locations . \n",
      "\n",
      "there are also quite a number of informal barriers . \n",
      "\n",
      "there are also quite a number of informal barriers . \n",
      "\n",
      "for \u001b[31minstance\u001b[0m , \u001b[31mCOPLs\u001b[0m generally provide interface \u001b[31mspecifications\u001b[0m ( \u001b[31mformal\u001b[0m or informal ) for classes by running a tool such as \u001b[31mJavadoc\u001b[0m or \u001b[31mEiffel\u001b[0m 's ' short ' over the class definition , meaning that the poor person writing a class has to write to 4 different ' audiences ' at once : the \u001b[31mcompiler\u001b[0m , people writing code that uses the class , people writing \u001b[31mderived\u001b[0m classes and people maintaining the class itself . \n",
      "\n",
      "for \u001b[32minformation\u001b[0m , \u001b[32mCOPLs\u001b[0m generally provide interface \u001b[32mrequirements\u001b[0m ( \u001b[32mliterary\u001b[0m or inliterary ) for classes by running a tool such as \u001b[32mJavadoc\u001b[0m or \u001b[32mEngineer\u001b[0m 's ' short ' over the class definition , meaning that the poor person writing a class has to write to 4 different ' audiences ' at once : the \u001b[32mprogram\u001b[0m , people writing code that uses the class , people writing \u001b[32mutilized\u001b[0m classes and people maintaining the class itself . \n",
      "\n",
      "institutions can be informal or \u001b[31mformal\u001b[0m . \n",
      "\n",
      "institutions can be inliterary or \u001b[32mliterary\u001b[0m . \n",
      "\n",
      "things continued on an informal , personal basis , by phone , I [ remained ] close friends with two of them , but \u001b[31mIzzat\u001b[0m \u001b[31mal\u001b[0m \u001b[31mGazawi\u001b[0m died last year . \n",
      "\n",
      "things continued on an informmetal , personmetal basis , by phone , I [ remained ] close friends with two of them , but \u001b[32mIzzat\u001b[0m \u001b[32mmetal\u001b[0m \u001b[32mGazawi\u001b[0m died last year . \n",
      "\n",
      "\u001b[31mschoolyard\u001b[0m \u001b[31mHabitats\u001b[0m \u001b[31m-\u001b[0m National Wildlife \u001b[31mFederation\u001b[0m \u001b[31m-\u001b[0m Offers an \u001b[31marray\u001b[0m of resources and programs for \u001b[31mformal\u001b[0m and informal \u001b[31mK-12\u001b[0m educators in a traditional classroom situation , nature center or other youth service \u001b[31mfacility\u001b[0m . \n",
      "\n",
      "\u001b[32myard\u001b[0m \u001b[32mSpecies\u001b[0m \u001b[32mls\u001b[0m National Wildlife \u001b[32mAssociation\u001b[0m \u001b[32mls\u001b[0m Offers an \u001b[32marrangement\u001b[0m of resources and programs for \u001b[32mliterary\u001b[0m and inliterary \u001b[32mKls12\u001b[0m educators in a traditional classroom situation , nature center or other youth service \u001b[32mservice\u001b[0m . \n",
      "\n",
      "since \u001b[31mkt\u001b[0m is the established symbol for the \u001b[31mkilotonne\u001b[0m , \u001b[31mKN\u001b[0m is the best choice as a symbol for the knot . knot [ 2 ] an informal unit of distance equal to the \u001b[31mnautical\u001b[0m mile . \n",
      "\n",
      "since \u001b[32mcarat\u001b[0m is the established symbol for the \u001b[32mkilotonne\u001b[0m , \u001b[32mKN\u001b[0m is the best choice as a symbol for the knot . knot [ 2 ] an informal unit of distance equal to the \u001b[32mnautical\u001b[0m mile . \n",
      "\n",
      "\u001b[31mJeffs\u001b[0m and Smith suggest that informal educators are `` guided ... by their understanding of what makes for the good ; of what makes for human well being `` ( 1990 : 1 \u001b[31m7-1\u001b[0m 8 ) . \n",
      "\n",
      "\u001b[32mJeffs\u001b[0m and Smith suggest that informal educators are `` guided ... by their understanding of what makes for the good ; of what makes for human well being `` ( 1990 : 1 \u001b[32m7-1\u001b[0m 8 ) . \n",
      "\n",
      "key findings are available in the informal education \u001b[31marchives\u001b[0m : ; full report : `` \u001b[31mInstitute\u001b[0m of Public Policy Research 2003 reprinted here with the kind permission of the \u001b[31mInstitute\u001b[0m of Public Policy Research First placed in the \u001b[31marchives\u001b[0m : December 2003 \n",
      "\n",
      "key findings are available in the informal education \u001b[32mrecords\u001b[0m : ; full report : `` \u001b[32mAssociation\u001b[0m of Public Policy Research 2003 reprinted here with the kind permission of the \u001b[32mAssociation\u001b[0m of Public Policy Research First placed in the \u001b[32mrecords\u001b[0m : December 2003 \n",
      "\n",
      "the \u001b[31minvestigators\u001b[0m , all part of a team known as the \u001b[31mAntithrombotic\u001b[0m \u001b[31mTrialists\u001b[0m ' \u001b[31mCollaboration\u001b[0m ( \u001b[31mATC\u001b[0m ) , looked at whether the \u001b[31mantiplatelets\u001b[0m cut patients ' risk of heart attack , stroke and death from a \u001b[31mcardiovascular\u001b[0m cause . \n",
      "\n",
      "the \u001b[32mauthorities\u001b[0m , all part of a team known as the \u001b[32mAntithrombotic\u001b[0m \u001b[32mTrialists\u001b[0m ' \u001b[32mPartnership\u001b[0m ( \u001b[32mATC\u001b[0m ) , looked at whether the \u001b[32mantiplatelets\u001b[0m cut patients ' risk of heart attack , stroke and death from a \u001b[32mcardiovascular\u001b[0m cause . \n",
      "\n",
      "\u001b[31mReilly\u001b[0m says he lost interest in the phone tap after finding in the \u001b[31mwastebasket\u001b[0m a piece of \u001b[31mcarbon\u001b[0m paper with the impression of 15 questions which \u001b[31mOtepka\u001b[0m had \u001b[31mallegedly\u001b[0m typed out for \u001b[31mSenate\u001b[0m \u001b[31minvestigators\u001b[0m to ask \u001b[31mReilly\u001b[0m . \n",
      "\n",
      "\u001b[32mReilly\u001b[0m says he lost interest in the phone tap after finding in the \u001b[32mcontainer\u001b[0m a piece of \u001b[32mpaper\u001b[0m paper with the impression of 15 questions which \u001b[32mOtepka\u001b[0m had \u001b[32mallegations\u001b[0m typed out for \u001b[32mLegislature\u001b[0m \u001b[32mauthorities\u001b[0m to ask \u001b[32mReilly\u001b[0m . \n",
      "\n",
      "`` [ new York Times , 9/11/02 ] OCT 5 , 2002 : \u001b[31mCongressional\u001b[0m \u001b[31minvestigators\u001b[0m say the FBI 's efforts to block their inquiry makes them \u001b[31mskeptical\u001b[0m of FBI \u001b[31massertions\u001b[0m . \n",
      "\n",
      "`` [ new York Times , 9/11/02 ] OCT 5 , 2002 : \u001b[32mCongressional\u001b[0m \u001b[32mauthorities\u001b[0m say the FBI 's efforts to block their inquiry makes them \u001b[32mincredulous\u001b[0m of FBI \u001b[32mclaims\u001b[0m . \n",
      "\n",
      "the key to good science in this field is always to keep the \u001b[31mexperiment\u001b[0m totally under the control of the \u001b[31minvestigators\u001b[0m and to use blind and \u001b[31mdouble-blind\u001b[0m testing \u001b[31mprocedures\u001b[0m wherever possible . \n",
      "\n",
      "the key to good science in this field is always to keep the \u001b[32mresearch\u001b[0m totally under the control of the \u001b[32mauthorities\u001b[0m and to use blind and \u001b[32mdouble-blind\u001b[0m testing \u001b[32mrequirements\u001b[0m wherever possible . \n",
      "\n",
      "leading International private \u001b[31mdetective\u001b[0m and private \u001b[31minvestigator\u001b[0m firm for all private investigations , we stand ready with any private \u001b[31mdetective\u001b[0m or private \u001b[31minvestigator\u001b[0m matter . \n",
      "\n",
      "leading International private \u001b[32mofficer\u001b[0m and private \u001b[32mofficer\u001b[0m firm for all private investigations , we stand ready with any private \u001b[32mofficer\u001b[0m or private \u001b[32mofficer\u001b[0m matter . \n",
      "\n",
      "in addition , it is important for \u001b[31mclinicians\u001b[0m and \u001b[31minvestigators\u001b[0m to account for any herbs or natural products being taken by their patients or research subjects that might interact with traditional treatments . \n",
      "\n",
      "in addition , it is important for \u001b[32mphysicians\u001b[0m and \u001b[32mauthorities\u001b[0m to account for any herbs or natural products being taken by their patients or research subjects that might interact with traditional treatments . \n",
      "\n",
      "American \u001b[31minvestigators\u001b[0m had to be sent anywhere but \u001b[31mHolland\u001b[0m . \n",
      "\n",
      "American \u001b[32mauthorities\u001b[0m had to be sent anywhere but \u001b[32mKingdom\u001b[0m of \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they suggest that \u001b[31minvestigators\u001b[0m of new religious movements should look into \u001b[31madaptation\u001b[0m in host communities as well as \u001b[31mNRMs\u001b[0m , and that they pay special attention to the role of the mass media in this \u001b[31mtwo-way\u001b[0m process of \u001b[31madaptation\u001b[0m . \n",
      "\n",
      "they suggest that \u001b[32mauthorities\u001b[0m of new religious movements should look into \u001b[32mversion\u001b[0m in host communities as well as \u001b[32mNRMs\u001b[0m , and that they pay special attention to the role of the mass media in this \u001b[32mbilateral\u001b[0m process of \u001b[32mversion\u001b[0m . \n",
      "\n",
      "the present , \u001b[31mrejecting\u001b[0m attitude \u001b[31mrestricts\u001b[0m \u001b[31minvestigators\u001b[0m to using simple tools , which are \u001b[31mincapable\u001b[0m of answering the questions \u001b[31mskeptics\u001b[0m demand be answered . \n",
      "\n",
      "the present , \u001b[32maccepting\u001b[0m attitude \u001b[32mlimiting\u001b[0m \u001b[32mauthorities\u001b[0m to using simple tools , which are \u001b[32munable\u001b[0m of answering the questions \u001b[32mdoubters\u001b[0m demand be answered . \n",
      "\n",
      "there he was a section leader for \u001b[31minorganic\u001b[0m chemistry and principal \u001b[31minvestigator\u001b[0m on several research projects dealing with \u001b[31mactinide\u001b[0m \u001b[31mcoordination\u001b[0m and \u001b[31morganometallic\u001b[0m chemistry , \u001b[31morganometallic\u001b[0m chemical vapor \u001b[31mdeposition\u001b[0m , and \u001b[31mtechnetium-99\u001b[0m chemistry . \n",
      "\n",
      "there he was a section leader for \u001b[32martificial\u001b[0m chemistry and principal \u001b[32mofficer\u001b[0m on several research projects dealing with \u001b[32mactinide\u001b[0m \u001b[32mnation\u001b[0m and \u001b[32morganometallic\u001b[0m chemistry , \u001b[32morganometallic\u001b[0m chemical vapor \u001b[32mexamination\u001b[0m , and \u001b[32mtechnetium-99\u001b[0m chemistry . \n",
      "\n",
      "`` something will hopefully be done `` , to ensure that even if the mission were to close tomorrow , FM103 `` continues in some form or sort , to \u001b[31mconsolidate\u001b[0m the peace process . \n",
      "\n",
      "`` something will hopefully be done `` , to ensure that even if the mission were to close tomorrow , FM103 `` continues in some form or sort , to \u001b[32mbuilt\u001b[0m the peace process . \n",
      "\n",
      "`` what we 're seeing over time is the \u001b[31mequivalent\u001b[0m of mission \u001b[31mcreep\u001b[0m : cases that would not be terrorism cases before Sept. 11 are swept onto the terrorism \u001b[31mdocket\u001b[0m , `` said \u001b[31mJuliette\u001b[0m \u001b[31mKayyem\u001b[0m , a former Clinton \u001b[31madministration\u001b[0m Justice official who heads the national security program at Harvard University 's John F. Kennedy School of Government . \n",
      "\n",
      "`` what we 're seeing over time is the \u001b[32mamount\u001b[0m of mission \u001b[32mcrawl\u001b[0m : cases that would not be terrorism cases before Sept. 11 are swept onto the terrorism \u001b[32mprogram\u001b[0m , `` said \u001b[32mJuliette\u001b[0m \u001b[32mKayyem\u001b[0m , a former Clinton \u001b[32mgovernment\u001b[0m Justice official who heads the national security program at Harvard University 's John F. Kennedy School of Government . \n",
      "\n",
      "and are we doing it in a way that keeps us competitive economically ? His question defined his \u001b[31mpassion\u001b[0m , my \u001b[31mstewardship\u001b[0m and the mission of the Environmental Protection Agency . \n",
      "\n",
      "and are we doing it in a way that keeps us competitive economically ? His question defined his \u001b[32mlove\u001b[0m , my \u001b[32moffice\u001b[0m and the mission of the Environmental Protection Agency . \n",
      "\n",
      "local churches reproduce themselves in their neighborhood and on the mission field . \n",
      "\n",
      "local churches reproduce themselves in their neighborhood and on the mission field . \n",
      "\n",
      "`` 52 THE method OF \u001b[31mAL\u001b[0m \u001b[31mImam\u001b[0m \u001b[31mAL\u001b[0m \u001b[31mSHAFI\u001b[0m ' I IN his book , \u001b[31mAL\u001b[0m \u001b[31mRISALAH\u001b[0m \u001b[31mAl\u001b[0m \u001b[31mImam\u001b[0m \u001b[31mal\u001b[0m \u001b[31mShafi\u001b[0m ' i began his book by describing the state of mankind just before the mission of the \u001b[31mProphet\u001b[0m . \n",
      "\n",
      "`` 52 THE method OF \u001b[32mAL\u001b[0m \u001b[32mLeader\u001b[0m \u001b[32mAL\u001b[0m \u001b[32mSHAFI\u001b[0m ' I IN his book , \u001b[32mAL\u001b[0m \u001b[32mRISALAH\u001b[0m \u001b[32mAl\u001b[0m \u001b[32mLeader\u001b[0m \u001b[32mal\u001b[0m \u001b[32mShafi\u001b[0m ' i began his book by describing the state of mankind just before the mission of the \u001b[32mOracle\u001b[0m . \n",
      "\n",
      "there are many important and urgent \u001b[31mresponsibilities\u001b[0m to attract the \u001b[31mmissionary\u001b[0m 's attention on the mission field , e.g . adult literacy . social and \u001b[31mdevelopmental\u001b[0m projects , \u001b[31meducational\u001b[0m and medical work . \n",
      "\n",
      "there are many important and urgent \u001b[32mobligations\u001b[0m to attract the \u001b[32mteacher\u001b[0m 's attention on the mission field , e.g . adult literacy . social and \u001b[32mdevelopmental\u001b[0m projects , \u001b[32minstructional\u001b[0m and medical work . \n",
      "\n",
      "\u001b[31mE-mail\u001b[0m to : \u001b[31mMethodius\u001b[0m \u001b[31mBaptist\u001b[0m Union of Great Britain \u001b[31m-\u001b[0m \u001b[31mArchive\u001b[0m of \u001b[31mmission-related\u001b[0m \u001b[31mdissertations\u001b[0m with a focus on \u001b[31mBaptist\u001b[0m mission in the UK \u001b[31mE-mail\u001b[0m to : \u001b[31mDarrell\u001b[0m Jackson , Mission Adviser \u001b[31mBethany\u001b[0m Christian Services ( \u001b[31mBCS\u001b[0m ) \u001b[31m-\u001b[0m \u001b[31mBCS\u001b[0m is a \u001b[31mnon-profit\u001b[0m \u001b[31morganization\u001b[0m with a mission to protect and \u001b[31menhance\u001b[0m the lives of children and individuals through \u001b[31mprofessional\u001b[0m social services like \u001b[31mint\u001b[0m . and \u001b[31mdom\u001b[0m . adoption , \u001b[31mcounseling\u001b[0m ( pregnancy+family ) \u001b[31mfoster\u001b[0m care and a \u001b[31m1-800-BETHANY\u001b[0m hotline . \n",
      "\n",
      "\u001b[32mElectronic\u001b[0m communication to \u001b[32m:\u001b[0m \u001b[32mMethodius\u001b[0m Baptist Union of Great \u001b[32mBritain\u001b[0m \u001b[32mls\u001b[0m Record \u001b[32mof\u001b[0m \u001b[32mmissionlsrelated\u001b[0m articles with a focus \u001b[32mon\u001b[0m Baptist mission in the \u001b[32mUK\u001b[0m Electronic communication \u001b[32mto\u001b[0m : Darrell Jackson , \u001b[32mMission\u001b[0m Adviser Bethany Christian \u001b[32mServices\u001b[0m ( \u001b[32mBCS\u001b[0m \u001b[32m)\u001b[0m ls BCS \u001b[32mis\u001b[0m \u001b[32ma\u001b[0m nonlsprofit association with a mission to \u001b[32mprotect\u001b[0m and increase the lives of children and \u001b[32mindividuals\u001b[0m through professional social \u001b[32mservices\u001b[0m like int \u001b[32m.\u001b[0m and dom . \u001b[32madoption\u001b[0m , guidance ( \u001b[32mpregnancy+family\u001b[0m ) help care \u001b[32mand\u001b[0m a 1ls800lsBETHANY \n",
      "\n",
      "\u001b[31mNK\u001b[0m : we are the children of the \u001b[31mhippie\u001b[0m generation and Girl has been \u001b[31mimbued\u001b[0m with this mission from her mother from a very young age . \n",
      "\n",
      "\u001b[32mNK\u001b[0m : we are the children of the \u001b[32mreformer\u001b[0m generation and Girl has been \u001b[32mcharacterized\u001b[0m with this mission from her mother from a very young age . \n",
      "\n",
      "C. \u001b[31mOperational\u001b[0m role of \u001b[31mWEU\u001b[0m \u001b[31mWEU\u001b[0m 's \u001b[31moperational\u001b[0m role will be \u001b[31mstrengthened\u001b[0m by examining and defining appropriate missions , structures and means , covering in particular : \u001b[31mWEU\u001b[0m planning cell ; closer military \u001b[31mcooperation\u001b[0m \u001b[31mcomplementary\u001b[0m to the \u001b[31mAlliance\u001b[0m in particular in the fields of \u001b[31mlogistics\u001b[0m , transport , training and \u001b[31mstrategic\u001b[0m \u001b[31msurveillance\u001b[0m ; meetings of \u001b[31mWEU\u001b[0m Chiefs of Defence Staff ; military units \u001b[31manswerable\u001b[0m to \u001b[31mWEU\u001b[0m . \n",
      "\n",
      "C. \u001b[32mEffective\u001b[0m role of \u001b[32mWEU\u001b[0m \u001b[32mWEU\u001b[0m 's \u001b[32meffective\u001b[0m role will be \u001b[32mimproved\u001b[0m by examining and defining appropriate missions , structures and means , covering in particular : \u001b[32mWEU\u001b[0m planning cell ; closer military \u001b[32magreement\u001b[0m \u001b[32mconsideration\u001b[0m to the \u001b[32mOrganization\u001b[0m in particular in the fields of \u001b[32mprovisions\u001b[0m , transport , training and \u001b[32mimportant\u001b[0m \u001b[32mcontrol\u001b[0m ; meetings of \u001b[32mWEU\u001b[0m Chiefs of Defence Staff ; military units \u001b[32mresponsible\u001b[0m to \u001b[32mWEU\u001b[0m . \n",
      "\n",
      "that said , the best \u001b[31mfoundations\u001b[0m are focused on \u001b[31maccomplishing\u001b[0m \u001b[31mprogrammatic\u001b[0m missions . \n",
      "\n",
      "that said , the best \u001b[32morganizations\u001b[0m are focused on \u001b[32machieve\u001b[0m \u001b[32mprogrammatic\u001b[0m missions . \n",
      "\n",
      "there is something just so \u001b[31mprofound\u001b[0m about that . \n",
      "\n",
      "there is something just so \u001b[32mimportant\u001b[0m about that . \n",
      "\n",
      "\u001b[31mSegall\u001b[0m and \u001b[31mTimiras\u001b[0m discovered \u001b[31mprofound\u001b[0m changes in \u001b[31mneurotransmitter\u001b[0m levels in the brains of their \u001b[31mexperimental\u001b[0m animals , but were unable to explore the \u001b[31mimplications\u001b[0m of these changes with regard to aging because their funding ran out . \n",
      "\n",
      "\u001b[32mSegall\u001b[0m and \u001b[32mTimiras\u001b[0m discovered \u001b[32mimportant\u001b[0m changes in \u001b[32mneurotransmitter\u001b[0m levels in the brains of their \u001b[32mempirical\u001b[0m animals , but were unable to explore the \u001b[32mconsequences\u001b[0m of these changes with regard to aging because their funding ran out . \n",
      "\n",
      "the human \u001b[31mgenome\u001b[0m project is going to be of \u001b[31mprofound\u001b[0m \u001b[31msignificance\u001b[0m for the \u001b[31mpharmaceuticals\u001b[0m and healthcare sectors , offering real hope in the treatment of conditions such as \u001b[31mcystic\u001b[0m \u001b[31mfibrosis\u001b[0m and cancer . \n",
      "\n",
      "the human \u001b[32mordination\u001b[0m project is going to be of \u001b[32mimportant\u001b[0m \u001b[32mmatter\u001b[0m for the \u001b[32mproducts\u001b[0m and healthcare sectors , offering real hope in the treatment of conditions such as \u001b[32mcystic\u001b[0m \u001b[32mpathology\u001b[0m and cancer . \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 \u001b[31mAssociation\u001b[0m , of course , does not establish \u001b[31mcausality\u001b[0m \u001b[31m--\u001b[0m but it raises the possibility that the \u001b[31mprofound\u001b[0m and apparently \u001b[31mdeepening\u001b[0m \u001b[31mpessimism\u001b[0m about personal \u001b[31mcircumstances\u001b[0m widely reflected in an \u001b[31marray\u001b[0m of opinion \u001b[31mpolls\u001b[0m and surveys is among the \u001b[31mdeterminants\u001b[0m \u001b[31munderlying\u001b[0m Russia 's \u001b[31mfearsome\u001b[0m levels of \u001b[31mcardiovascular\u001b[0m death . \n",
      "\n",
      "25 \u001b[32mOrganization\u001b[0m , of course , does not establish \u001b[32mrelation\u001b[0m \u001b[32m--\u001b[0m but it raises the possibility that the \u001b[32mdeep\u001b[0m and apparently \u001b[32mincreasing\u001b[0m \u001b[32mdisposition\u001b[0m about personal \u001b[32mconditions\u001b[0m widely reflected in an \u001b[32marrangement\u001b[0m of opinion \u001b[32melections\u001b[0m and surveys is among the \u001b[32mfactors\u001b[0m \u001b[32mprinciples\u001b[0m Russia 's \u001b[32mterrible\u001b[0m levels of \u001b[32mcardiovascular\u001b[0m death . \n",
      "\n",
      "if you hesitate , therefore , a moment , or if , after reflection , you produce any \u001b[31mintricate\u001b[0m or \u001b[31mprofound\u001b[0m argument , you , in a manner , give up the question , and \u001b[31mconfess\u001b[0m that it is not reasoning which \u001b[31mengages\u001b[0m us to suppose the past \u001b[31mresembling\u001b[0m the future , and to expect similar effects from causes which are , to \u001b[31mappearance\u001b[0m , similar . \n",
      "\n",
      "if you hesitate , therefore , a moment , or if , after reflection , you produce any \u001b[32mcomplex\u001b[0m or \u001b[32mimportant\u001b[0m argument , you , in a manner , give up the question , and \u001b[32madmit\u001b[0m that it is not reasoning which \u001b[32mfacilitates\u001b[0m us to suppose the past \u001b[32mrepresenting\u001b[0m the future , and to expect similar effects from causes which are , to \u001b[32mappearance\u001b[0m , similar . \n",
      "\n",
      "a fresh miracle , one with a \u001b[31mprofound\u001b[0m political \u001b[31mdimension\u001b[0m and such spectacular possibilities was just what they had to be looking for . [ 8 ] \u001b[31mLourdes\u001b[0m ' Mother Superior is \u001b[31mdepicted\u001b[0m as a jealous , \u001b[31mresentful\u001b[0m and ambitious woman who viewed the newly arrived order as a \u001b[31mthreat-a\u001b[0m competitor for donations and \u001b[31mnovices\u001b[0m . \n",
      "\n",
      "a fresh miracle , one with an \u001b[32mimportant\u001b[0m political \u001b[32mconsideration\u001b[0m and such spectacular possibilities was just what they had to be looking for . [ 8 ] \u001b[32mLourdes\u001b[0m ' Mother Superior is \u001b[32mdescribed\u001b[0m as a jealous , \u001b[32macrimonious\u001b[0m and ambitious woman who viewed the newly arrived order as a \u001b[32mthreat-a\u001b[0m competitor for donations and \u001b[32mbeginners\u001b[0m . \n",
      "\n",
      "you have surely given an \u001b[31mexceptional\u001b[0m argument which raises many \u001b[31mprofound\u001b[0m questions \u001b[31mconcerning\u001b[0m the \u001b[31mfoundations\u001b[0m of artificial intelligence . \n",
      "\n",
      "you have surely given a \u001b[32mspecial\u001b[0m argument which raises many \u001b[32mimportant\u001b[0m questions \u001b[32mrelated\u001b[0m the \u001b[32morganizations\u001b[0m of artificial intelligence . \n",
      "\n",
      "return to contents Natural selection and other \u001b[31mmechanisms-such\u001b[0m as \u001b[31mchromosomal\u001b[0m changes , \u001b[31msymbiosis\u001b[0m and \u001b[31mhybridization-can\u001b[0m drive \u001b[31mprofound\u001b[0m changes in populations over time . \n",
      "\n",
      "return to contents Natural selection and other \u001b[32mmechanisms-such\u001b[0m as \u001b[32mchromosomal\u001b[0m changes , \u001b[32minterdependence\u001b[0m and \u001b[32mhybridization-can\u001b[0m drive \u001b[32mdeep\u001b[0m changes in populations over time . \n",
      "\n",
      "gaining the confidence of local residents , for example , so that they will allow the students to enter and test their homes requires \u001b[31mprofound\u001b[0m cultural understanding . \n",
      "\n",
      "gaining the confidence of local residents , for example , so that they will allow the students to enter and test their homes requires \u001b[32mimportant\u001b[0m cultural understanding . \n",
      "\n",
      "but even Willy 's \u001b[31mconception\u001b[0m of the past is not as \u001b[31midyllic\u001b[0m as it seems on the surface , as his split \u001b[31mconsciousness\u001b[0m , the \u001b[31mprofound\u001b[0m \u001b[31mrift\u001b[0m in his \u001b[31mpsyche\u001b[0m , shows through . \n",
      "\n",
      "but even Willy 's \u001b[32mthought\u001b[0m of the past is not as \u001b[32mpastoral\u001b[0m as it seems on the surface , as his split \u001b[32mawareness\u001b[0m , the \u001b[32mdeep\u001b[0m \u001b[32mseparation\u001b[0m in his \u001b[32mknowledge\u001b[0m , shows through . \n",
      "\n",
      "`` what \u001b[31mMeeks\u001b[0m did not mention is that \u001b[31mGUEA\u001b[0m bought the building from her church , Victory \u001b[31mApostolic\u001b[0m Faith Church in Chicago , where she and her father are \u001b[31mprominent\u001b[0m members , according to Cook County property records and the church 's Web site . \n",
      "\n",
      "`` what \u001b[32mMeeks\u001b[0m did not mention is that \u001b[32mGUEA\u001b[0m bought the building from her church , Victory \u001b[32mPontifical\u001b[0m Faith Church in Chicago , where she and her father are \u001b[32mimportant\u001b[0m members , according to Cook County property records and the church 's Web site . \n",
      "\n",
      "among the \u001b[31mrecipients\u001b[0m were \u001b[31mprominent\u001b[0m journalists and producers , \u001b[31mscions\u001b[0m of the \u001b[31malternative\u001b[0m press , and a \u001b[31msmattering\u001b[0m of current and former intelligence \u001b[31manalysts\u001b[0m who often serve as sources in news analyses and articles . \n",
      "\n",
      "among the \u001b[32mbenefits\u001b[0m were \u001b[32mimportant\u001b[0m journalists and producers , \u001b[32mscions\u001b[0m of the \u001b[32msecondary\u001b[0m press , and a \u001b[32mhandful\u001b[0m of current and former intelligence \u001b[32mscientists\u001b[0m who often serve as sources in news analyses and articles . \n",
      "\n",
      "update Posted at 10:00 AM \u001b[31m--\u001b[0m \u001b[31mPermalink\u001b[0m Back in this post , I reported that the \u001b[31mTropicana\u001b[0m Hotel in \u001b[31mLas\u001b[0m Vegas had stopped accepting \u001b[31mreservations\u001b[0m for after April 15 and that a \u001b[31mprominent\u001b[0m \u001b[31mdemolition\u001b[0m company had told a reporter that it was studying how to blow up the place . \n",
      "\n",
      "update Posted at 10:00 AM \u001b[32m--\u001b[0m \u001b[32mPermalink\u001b[0m Back in this post , I reported that the \u001b[32mTropicana\u001b[0m Hotel in \u001b[32mLas\u001b[0m Vegas had stopped accepting \u001b[32mprovisions\u001b[0m for after April 15 and that an \u001b[32mimportant\u001b[0m \u001b[32mconclusion\u001b[0m company had told a reporter that it was studying how to blow up the place . \n",
      "\n",
      "`` evidence is easily found of \u001b[31mmultipolarity\u001b[0m and China 's \u001b[31mprominent\u001b[0m role in promoting it . \n",
      "\n",
      "`` evidence is easily found of \u001b[32mmulti-polarization\u001b[0m and China 's \u001b[32mimportant\u001b[0m role in promoting it . \n",
      "\n",
      "\u001b[31mreeling\u001b[0m from the effects of World War \u001b[31mII\u001b[0m , \u001b[31mprominent\u001b[0m local \u001b[31mbusinessmen\u001b[0m led by the \u001b[31mvisionary\u001b[0m \u001b[31mArchie\u001b[0m \u001b[31mJewell\u001b[0m , purchased the building in 1943 and \u001b[31mrenovated\u001b[0m it to become `` The International House `` , a \u001b[31mnewly-created\u001b[0m \u001b[31mnon-profit\u001b[0m trade \u001b[31massociation\u001b[0m dedicated to world peace , trade and understanding . \n",
      "\n",
      "\u001b[32mspin\u001b[0m from the effects of World War \u001b[32mFIGURE\u001b[0m , \u001b[32mimportant\u001b[0m local \u001b[32mmanufacturers\u001b[0m led by the \u001b[32mimpractical\u001b[0m \u001b[32mArchie\u001b[0m \u001b[32mJewell\u001b[0m , purchased the building in 1943 and \u001b[32mrestored\u001b[0m it to become `` The International House `` , a \u001b[32mnewly-established\u001b[0m \u001b[32mnon-profit\u001b[0m trade \u001b[32morganization\u001b[0m dedicated to world peace , trade and understanding . \n",
      "\n",
      "there were five \u001b[31mreferences\u001b[0m to \u001b[31mCrawford\u001b[0m : three dealing with his present claims that he lied on behalf of the tobacco companies , one dealing with a property \u001b[31mdispute\u001b[0m , and another , which identified him as a \u001b[31mprominent\u001b[0m criminal lawyer , who had been involved in 33 capital cases . \n",
      "\n",
      "there were five \u001b[32marticles\u001b[0m to \u001b[32mCrawford\u001b[0m : three dealing with his present claims that he lied on behalf of the tobacco companies , one dealing with a property \u001b[32mdifference\u001b[0m , and another , which identified him as an \u001b[32mimportant\u001b[0m criminal lawyer , who had been involved in 33 capital cases . \n",
      "\n",
      "\u001b[31mexpansion\u001b[0m of markets , improvement of profit potential , and reduction of cost factors join the \u001b[31menhancement\u001b[0m of competitive position as \u001b[31mprominent\u001b[0m forces moving \u001b[31morganizations\u001b[0m toward \u001b[31mglobalization\u001b[0m . \n",
      "\n",
      "\u001b[32mincrease\u001b[0m of markets , improvement of profit potential , and reduction of cost factors join the \u001b[32mimprovement\u001b[0m of competitive position as \u001b[32mimportant\u001b[0m forces moving \u001b[32minstitutions\u001b[0m toward \u001b[32meconomic\u001b[0m process \n",
      "\n",
      "i have argued in my previous article for PDF that `` [ \u001b[31mf\u001b[0m ] or \u001b[31mArmstrong\u001b[0m \u001b[31mWilliams-like\u001b[0m bloggers actually paid by campaigns or other political \u001b[31mcommittees\u001b[0m to promote or attack a \u001b[31mcandidate\u001b[0m for federal office , \u001b[31mprominent\u001b[0m and \u001b[31mon-the-spot\u001b[0m \u001b[31mdisclosure\u001b[0m should be \u001b[31mmandated\u001b[0m . `` A \u001b[31mdisclosure\u001b[0m on a campaign 's website in a quarterly report after the election \u001b[31mdeprives\u001b[0m voters of valuable information about the possible \u001b[31mmotivations\u001b[0m for an analysis or \u001b[31mcommentary\u001b[0m appearing on a website . \n",
      "\n",
      "i have argued in my previous article letteror PDF that `` [ \u001b[32mletter\u001b[0m ] or \u001b[32mArmstrong\u001b[0m \u001b[32mWilliams-like\u001b[0m bloggers actually paid by campaigns or other political \u001b[32morganizations\u001b[0m to promote or attack a \u001b[32mperson\u001b[0m letteror letterederal oletterletterice , \u001b[32mimportant\u001b[0m and \u001b[32mon-site\u001b[0m \u001b[32mbreach\u001b[0m should be \u001b[32mregulations\u001b[0m . `` A \u001b[32mbreach\u001b[0m on a campaign 's website in a quarterly report aletterter the election \u001b[32mprevents\u001b[0m voters oletter valuable inletterormation about the possible \u001b[32mcircumstances\u001b[0m letteror an analysis or \u001b[32mcommentary\u001b[0m appearing on a website . \n",
      "\n",
      "this pattern of bad journalism is a \u001b[31mprominent\u001b[0m feature of every \u001b[31mpro-hoax\u001b[0m text and video . \n",
      "\n",
      "this pattern of bad journalism is an \u001b[32mimportant\u001b[0m feature of every \u001b[32mpro-hoax\u001b[0m text and video . \n",
      "\n",
      "apple vs. Microsoft My \u001b[31mAltaVista\u001b[0m \u001b[31msurvey\u001b[0m results revealed that `` Gates Hate `` is more \u001b[31mprominent\u001b[0m among Mac fans than with other groups \u001b[31m--\u001b[0m providing a \u001b[31mrationale\u001b[0m for starting with Apple in our examination of the various conflicts listed above . \n",
      "\n",
      "apple vs. Microsoft My \u001b[32mAltaVista\u001b[0m \u001b[32mstudy\u001b[0m results revealed that `` Gates Hate `` is more \u001b[32mimportant\u001b[0m among Mac fans than with other groups \u001b[32m--\u001b[0m providing a \u001b[32mreason\u001b[0m for starting with Apple in our examination of the various conflicts listed above . \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in short , he may be \u001b[31mrendered\u001b[0m \u001b[31minsane\u001b[0m upon every subject which is not founded on , and which does not remain in \u001b[31mnever-varying\u001b[0m \u001b[31mconsistency\u001b[0m with , the facts that surround mankind . \n",
      "\n",
      "in short , he may be \u001b[32mbeen\u001b[0m \u001b[32mlunatic\u001b[0m upon every subject which is not founded on , and which does not remain in \u001b[32mnever-varying\u001b[0m \u001b[32mbody\u001b[0m with , the facts that surround mankind . \n",
      "\n",
      "a shared interest in \u001b[31mprosperity\u001b[0m has not been enough to \u001b[31mrender\u001b[0m \u001b[31mbenign\u001b[0m \u001b[31malienage\u001b[0m and the political \u001b[31merasures\u001b[0m of borders . \n",
      "\n",
      "a shared interest in \u001b[32msuccessfulness\u001b[0m has not been enough to \u001b[32mmake\u001b[0m \u001b[32mbenign\u001b[0m \u001b[32mstrangeness\u001b[0m and the political \u001b[32mexpunctions\u001b[0m of borders . \n",
      "\n",
      "it is , therefore , without any substance to urge that the services under the scheme are \u001b[31mrendered\u001b[0m free of charge and , therefore , the scheme is not a `` service `` under the Act . \n",
      "\n",
      "it is , therefore , without any substance to urge that the services under the scheme are \u001b[32mbeen\u001b[0m free of charge and , therefore , the scheme is not a `` service `` under the Act . \n",
      "\n",
      "posted by : \u001b[31mRobrob\u001b[0m at June 23 , 2005 02:04 AM Where will the \u001b[31mgreed\u001b[0m of corporations intent on \u001b[31mrendering\u001b[0m skilled American workers \u001b[31mobsolete\u001b[0m end ? \n",
      "\n",
      "posted by : \u001b[32mRobrob\u001b[0m at June 23 , 2005 02:04 AM Where will the \u001b[32mdesire\u001b[0m of corporations intent on \u001b[32mproviding\u001b[0m skilled American workers \u001b[32mobsolete\u001b[0m end ? \n",
      "\n",
      "in general , the speech \u001b[31mstylesheet\u001b[0m will not attempt to specify the mapping between visual layout tags and speech properties , instead leaving it to specific \u001b[31mbrowser\u001b[0m \u001b[31mimplementations\u001b[0m and \u001b[31muser-specific\u001b[0m \u001b[31mstylesheets\u001b[0m to decide how such tags are \u001b[31mrendered\u001b[0m . \n",
      "\n",
      "in general , the speech \u001b[32mstylesheet\u001b[0m will not attempt to specify the mapping between visual layout tags and speech properties , instead leaving it to specific \u001b[32mapplication\u001b[0m \u001b[32msystems\u001b[0m and \u001b[32muser-specific\u001b[0m \u001b[32mstylesheets\u001b[0m to decide how such tags are \u001b[32mbeen\u001b[0m . \n",
      "\n",
      "you produce the content , and my \u001b[31mbrowser\u001b[0m decides \u001b[31m--\u001b[0m along with some input from me \u001b[31m--\u001b[0m how to \u001b[31mrender\u001b[0m the content . \n",
      "\n",
      "you produce the content , and my \u001b[32mapplication\u001b[0m decides \u001b[32m--\u001b[0m along with some input from me \u001b[32m--\u001b[0m how to \u001b[32mmake\u001b[0m the content . \n",
      "\n",
      "in particular , \u001b[31mprovision\u001b[0m of an \u001b[31malternative\u001b[0m route for \u001b[31mcyclists\u001b[0m should never be regarded as an excuse for \u001b[31mrendering\u001b[0m the original road or \u001b[31mjunction\u001b[0m \u001b[31munsuitable\u001b[0m for \u001b[31mcyclists\u001b[0m . \n",
      "\n",
      "in particular , \u001b[32marticle\u001b[0m of a \u001b[32msecondary\u001b[0m route for \u001b[32mmotorcycles\u001b[0m should never be regarded as an excuse for \u001b[32mproviding\u001b[0m the original road or \u001b[32mplace\u001b[0m \u001b[32mbad\u001b[0m for \u001b[32mmotorcycles\u001b[0m . \n",
      "\n",
      "although \u001b[31mtechnological\u001b[0m \u001b[31madvancements\u001b[0m make it possible to ship products from one side of the world to the other within a few days , abuse of a \u001b[31mgood-quality\u001b[0m food before , during or after transportation and storage may \u001b[31mrender\u001b[0m it \u001b[31mspoiled\u001b[0m or unsafe by the time it reaches the consumer . \n",
      "\n",
      "although \u001b[32mscientific\u001b[0m \u001b[32mdevelopments\u001b[0m make it possible to ship products from one side of the world to the other within a few days , abuse of a \u001b[32mgood-quality\u001b[0m food before , during or after transportation and storage may \u001b[32mmake\u001b[0m it \u001b[32mbaby\u001b[0m or unsafe by the time it reaches the consumer . \n",
      "\n",
      "`` this \u001b[31mrenders\u001b[0m necessary a brief review of the position occupied by the \u001b[31mAdministration\u001b[0m on this important and vital question . \n",
      "\n",
      "`` this \u001b[32mremoves\u001b[0m necessary a brief review of the position occupied by the \u001b[32mGovernment\u001b[0m on this important and vital question . \n",
      "\n",
      "\u001b[31mSchon\u001b[0m ( 1983,1987 ) has noted that `` practice is \u001b[31mcharacterized\u001b[0m by \u001b[31mindeterminacy\u001b[0m , and what \u001b[31mdistinguishes\u001b[0m the \u001b[31mexcellent\u001b[0m \u001b[31mpractitioner\u001b[0m from the merely \u001b[31madequate\u001b[0m one is the ability to \u001b[31mrender\u001b[0m \u001b[31mindeterminate\u001b[0m situations \u001b[31mdeterminate\u001b[0m . \n",
      "\n",
      "\u001b[32mSchon\u001b[0m ( 1983,1987 ) has noted that `` practice is \u001b[32mdescribed\u001b[0m by \u001b[32muncertainty\u001b[0m , and what \u001b[32mdiffers\u001b[0m the \u001b[32msuperior\u001b[0m \u001b[32mprofessional\u001b[0m person from the \u001b[32mmerely\u001b[0m suitable one is the ability \u001b[32mto\u001b[0m \u001b[32mmake\u001b[0m uncertain \u001b[32msituations\u001b[0m determinate \n",
      "\n",
      "( missing last minute ) ( GSN/wo/c/A+/133 ) \u001b[31mBiography\u001b[0m Bob Barker ( A+ ) Mark \u001b[31mGoodson\u001b[0m ( A+ ) Deal A very interesting and \u001b[31mintimate\u001b[0m \u001b[31mbehind-the\u001b[0m scenes look at `` Let 's Make a Deal `` , one of the most popular game shows of all time . \n",
      "\n",
      "( missing last minute ) ( GSN/wo/c/A+/133 ) \u001b[32mLife\u001b[0m Bob Barker ( A+ ) Mark \u001b[32mGoodson\u001b[0m ( A+ ) Deal A very interesting and \u001b[32msexual\u001b[0m \u001b[32mbehind-the\u001b[0m scenes look at `` Let 's Make a Deal `` , one of the most popular game shows of all time . \n",
      "\n",
      "hence becoming the scene of more than one major battle between the \u001b[31molde\u001b[0m enemy England . \n",
      "\n",
      "hence becoming the scene of more than one major battle between the \u001b[32molde\u001b[0m enemy England . \n",
      "\n",
      "framed by the remains of a smoking , burning \u001b[31mdoorway\u001b[0m and \u001b[31msilhouetted\u001b[0m against the light , Ethan \u001b[31msquats\u001b[0m and surveys the \u001b[31mbutchered\u001b[0m dead , and his head drops as he \u001b[31mimagines\u001b[0m the torture that three family members suffered , including Martha 's rape . [ A similar scene of Luke \u001b[31mSkywalker\u001b[0m 's discovery of his aunt and uncle 's burning home after their murder by \u001b[31mImperial\u001b[0m \u001b[31mstormtroopers\u001b[0m is found in George Lucas ' Star Wars ( 1977 ) . \n",
      "\n",
      "framed by the remains of a smoking , burning \u001b[32mentrance\u001b[0m and \u001b[32msilhouetted\u001b[0m against the light , Ethan \u001b[32mjacks\u001b[0m and surveys the \u001b[32mslaughter\u001b[0m dead , and his head drops as he \u001b[32mthinks\u001b[0m the torture that three family members suffered , including Martha 's rape . [ A similar scene of Luke \u001b[32mSkywalker\u001b[0m 's discovery of his aunt and uncle 's burning home after their murder by \u001b[32mNoble\u001b[0m \u001b[32mstormtroopers\u001b[0m is found in George Lucas ' Star Wars ( 1977 ) . \n",
      "\n",
      "`` It does n't come into play unless you 're actually at a scene or taking action photographs , but digital cameras do not have a shutter speed fast enough to \u001b[31mphotograph\u001b[0m action , `` says \u001b[31mPasqualone\u001b[0m . \n",
      "\n",
      "`` It does n't come into play unless you 're actually at a scene or taking action records , but digital cameras do not have a shutter speed fast enough to \u001b[32mrecord\u001b[0m action , `` says \u001b[32mPasqualone\u001b[0m . \n",
      "\n",
      "every scene seems totally natural like it could have really happened , and yet the movie is not a dull \u001b[31mslice-of-life\u001b[0m \u001b[31mdiorama\u001b[0m either . \n",
      "\n",
      "every scene seems totally natural like it could have really happened , and yet the movie is not a dull \u001b[32mslice-of-life\u001b[0m \u001b[32mpicture\u001b[0m either . \n",
      "\n",
      "\u001b[31mundoubtedly\u001b[0m a \u001b[31mmainstay\u001b[0m of the Texas music scene , one of \u001b[31mLloyd\u001b[0m 's best known credits is his daughter , \u001b[31mNatalie\u001b[0m \u001b[31mMaines\u001b[0m , the lead singer for the \u001b[31mGrammy\u001b[0m Award winning Dixie chicks . \n",
      "\n",
      "\u001b[32mcertainly\u001b[0m a \u001b[32msupport\u001b[0m of the Texas music scene , one of \u001b[32mPlayer\u001b[0m 's best known credits is his daughter , \u001b[32mNatalie\u001b[0m \u001b[32mPine_Tree_States\u001b[0m , the lead singer for the \u001b[32mGrammy\u001b[0m Award winning Dixie chicks . \n",
      "\n",
      "on the plus side , the immediate mode offers the possibility of exploring \u001b[31mdynamic\u001b[0m scenes . \n",
      "\n",
      "on the plus side , the immediate mode offers the possibility of exploring \u001b[32mactive\u001b[0m scenes . \n",
      "\n",
      "both times we have a \u001b[31mCajus\u001b[0m who goes out to walk ; both times a carriage full of people , who both times sing and shout ; both times \u001b[31mCajus\u001b[0m meets with the \u001b[31mcarnage\u001b[0m ; both times a family is mentioned ; both times an aged woman figures in the scene ; both times the hand is kissed . \n",
      "\n",
      "both times we have a \u001b[32mCajus\u001b[0m who goes out to walk ; both times a carriage full of people , who both times sing and shout ; both times \u001b[32mCajus\u001b[0m meets with the \u001b[32mexecution\u001b[0m ; both times a family is mentioned ; both times an aged woman figures in the scene ; both times the hand is kissed . \n",
      "\n",
      "`` reverse : \u001b[31mwhaling\u001b[0m scene with two boats , one being stove by a whale ; ship in background , labeled `` 18 Jan 1851 / The accident off New Zealand . \n",
      "\n",
      "`` reverse : \u001b[32mwhaling\u001b[0m scene with two boats , one being stove by a whale ; ship in background , labeled `` 18 Jan 1851 / The accident off New Zealand . \n",
      "\n",
      "if we do not take a lead on the national scene , our fate will be in the hands of others \u001b[31m--\u001b[0m others who are not likely to share our mission or concerns . \n",
      "\n",
      "if we do not take a lead on the national scene , our fate will be in the hands of others \u001b[32m--\u001b[0m others who are not likely to share our mission or concerns . \n",
      "\n",
      "3 [ 432 U.S. 312 , 332 ] I suspect that the only \u001b[31mjustification\u001b[0m for the Court 's decision today is its belief that the \u001b[31mstatute\u001b[0m is unfair in its application . \n",
      "\n",
      "3 [ 432 U.S. 312 , 332 ] I suspect that the only \u001b[32mconsideration\u001b[0m for the Court 's decision today is its belief that the \u001b[32mact\u001b[0m is unfair in its application . \n",
      "\n",
      "it is \u001b[31mdoubtful\u001b[0m that many viewers who witness the process and effects of such \u001b[31mnefarious\u001b[0m and \u001b[31minsidious\u001b[0m `` mind control `` in their \u001b[31mentertainment\u001b[0m , ever suspect that they themselves are the victims of similar programs of influence and plots of \u001b[31mintrigue\u001b[0m in real life . \n",
      "\n",
      "it is \u001b[32muncertain\u001b[0m that many viewers who witness the process and effects of such \u001b[32mvillainous\u001b[0m and \u001b[32mdangerous\u001b[0m `` mind control `` in their \u001b[32mrecreation\u001b[0m , ever suspect that they themselves are the victims of similar programs of influence and plots of \u001b[32mgame\u001b[0m in real life . \n",
      "\n",
      "this variable degree of symptoms can make the \u001b[31mdiagnosis\u001b[0m of an \u001b[31moxidative\u001b[0m \u001b[31mmetabolic\u001b[0m disorder more difficult to recognize , but should be highly suspected when \u001b[31mmyopathy\u001b[0m and \u001b[31mencephalopathy\u001b[0m are present together . \n",
      "\n",
      "this variable degree of symptoms can make the \u001b[32midentification\u001b[0m of an \u001b[32maerophilous\u001b[0m \u001b[32mmetabolic\u001b[0m disorder more difficult to recognize , but should be highly suspected when \u001b[32mpathology\u001b[0m and \u001b[32mneurological\u001b[0m disorder are present together \n",
      "\n",
      "if you suspect something is out of the ordinary then its time to call the police and either pass on the information or request to see an officer if necessary . \n",
      "\n",
      "if you suspect something is out of the ordinary then its time to call the police and either pass on the information or request to see an officer if necessary . \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i suspect that all the other \u001b[31mcommittees\u001b[0m will have an away day and that all of those will have a European element . \n",
      "\n",
      "i suspect that all the other \u001b[32morganizations\u001b[0m will have an away day and that all of those will have a European element . \n",
      "\n",
      "suspected of \u001b[31mconspiracy\u001b[0m , \u001b[31msecurities\u001b[0m fraud and \u001b[31mobstruction\u001b[0m of justice , Stewart is ultimately \u001b[31mindicted\u001b[0m , found guilty of all charges and \u001b[31msentenced\u001b[0m to five months in prison followed by five months of house arrest . \n",
      "\n",
      "suspected of \u001b[32mset\u001b[0m , \u001b[32moperations\u001b[0m fraud and \u001b[32minterference\u001b[0m of justice , Stewart is ultimately \u001b[32mdefendants\u001b[0m , found guilty of all charges and \u001b[32mcharged\u001b[0m to five months in prison followed by five months of house arrest . \n",
      "\n",
      "here 's a \u001b[31mcross-section\u001b[0m : \u001b[31m--\u001b[0m \u001b[31mWizbang\u001b[0m 's Kevin \u001b[31mAlyward\u001b[0m : `` I remain \u001b[31mskeptical\u001b[0m of the argument the \u001b[31mSchindler\u001b[0m 's lawyers have put forth ( they claimed that Terri \u001b[31mSchiavo\u001b[0m 's religious and due process rights have been violated ) in their federal case , and suspect the Supreme Court may decided not to hear the case . \n",
      "\n",
      "here 's a \u001b[32mcross-section\u001b[0m : \u001b[32m--\u001b[0m \u001b[32mWizbang\u001b[0m 's Kevin \u001b[32mAlyward\u001b[0m : `` I remain \u001b[32mincredulous\u001b[0m of the argument the \u001b[32mSchindler\u001b[0m 's lawyers have put forth ( they claimed that Terri \u001b[32mSchiavo\u001b[0m 's religious and due process rights have been violated ) in their federal case , and suspect the Supreme Court may decided not to hear the case . \n",
      "\n",
      "either way , it appears that the \u001b[31mSPDC\u001b[0m Commander suspects that the rice has gone to the \u001b[31mKNLA\u001b[0m . \n",
      "\n",
      "either way , it appears that the \u001b[32mSPDC\u001b[0m Commander suspects that the rice has gone to the \u001b[32mKNLA\u001b[0m . \n",
      "\n",
      "i suspect I will be able to \u001b[31mnitpick\u001b[0m all \u001b[31mcounterarguments\u001b[0m . \n",
      "\n",
      "i suspect I will be able to \u001b[32mknock\u001b[0m all \u001b[32mcounterarguments\u001b[0m . \n",
      "\n",
      "\u001b[31mplaintiff\u001b[0m presents , as an example of `` government \u001b[31mspying\u001b[0m , `` an \u001b[31mincident\u001b[0m in which another person is suspected , without \u001b[31mfactual\u001b[0m basis , of being an \u001b[31minfiltrator\u001b[0m : on October 10 , 2001 , Seattle Central Community College Freedom \u001b[31mSocialists\u001b[0m Club held a form \u001b[31mentitled\u001b[0m `` A \u001b[31mSpeakout\u001b[0m Against War . \n",
      "\n",
      "\u001b[32mapplicant\u001b[0m presents , as an example of `` government \u001b[32mintelligence\u001b[0m , `` an \u001b[32moccurrence\u001b[0m in which another person is suspected , without \u001b[32mreal\u001b[0m basis , of being a \u001b[32mspy\u001b[0m : on October 10 , 2001 , Seattle Central Community College Freedom \u001b[32mDemocrats\u001b[0m Club held a form \u001b[32mopportunity\u001b[0m `` A \u001b[32mSpeakout\u001b[0m Against War . \n",
      "\n",
      "\u001b[31mSpaghettini\u001b[0m with \u001b[31mmullet\u001b[0m \u001b[31mroe\u001b[0m and \u001b[31mgrated\u001b[0m vegetables , and \u001b[31mpenne\u001b[0m with \u001b[31msundried\u001b[0m tomatoes and basil were both full of \u001b[31mflavour\u001b[0m and \u001b[31mpleasantly\u001b[0m light , which was lucky as I was almost defeated by an \u001b[31mexceptionally\u001b[0m tender and tasty lamb \u001b[31mshank\u001b[0m to follow . \n",
      "\n",
      "\u001b[32mPasta\u001b[0m with \u001b[32mfish\u001b[0m \u001b[32megg\u001b[0m and \u001b[32mgrind\u001b[0m vegetables , and \u001b[32mpasta\u001b[0m with \u001b[32msundried\u001b[0m tomatoes and basil were both full of \u001b[32mtaste\u001b[0m and \u001b[32magreeably\u001b[0m light , which was lucky as I was almost defeated by an \u001b[32mextremely\u001b[0m tender and tasty lamb \u001b[32mcylinder\u001b[0m to follow . \n",
      "\n",
      "women usually notice little change in their breasts , but if you are a man , your breasts may become slightly larger and may be tender . \n",
      "\n",
      "women usually notice little change in their breasts , but if you are a man , your breasts may become slightly larger and may be tender . \n",
      "\n",
      "`` i can be very , very strong , but on the other hand , I have very tender \u001b[31msentiments\u001b[0m . \n",
      "\n",
      "`` i can be very , very strong , but on the other hand , I have very tender \u001b[32mcomments\u001b[0m . \n",
      "\n",
      "rabbits often feed on young , tender \u001b[31mperennial\u001b[0m growth as it emerges in spring , or on young \u001b[31mtransplants\u001b[0m . \n",
      "\n",
      "rabbits often feed on young , tender \u001b[32mcontinual\u001b[0m growth as it emerges in spring , or on young \u001b[32mpatients\u001b[0m . \n",
      "\n",
      "the \u001b[31mgrilled\u001b[0m \u001b[31moctopus\u001b[0m was very tender . \n",
      "\n",
      "the \u001b[32mgrilled\u001b[0m \u001b[32mseafood\u001b[0m was very tender . \n",
      "\n",
      "Cook the rice , covered , over boiling water for about 30 minutes , or until the grains are tender . \n",
      "\n",
      "Cook the rice , covered , over boiling water for about 30 minutes , or until the grains are tender . \n",
      "\n",
      "they feed at that period on the opening \u001b[31mbuds\u001b[0m of \u001b[31mmaples\u001b[0m , and others that are equally tender and juicy . \n",
      "\n",
      "they feed at that period on the opening \u001b[32mbuds\u001b[0m of \u001b[32mmaples\u001b[0m , and others that are equally tender and juicy . \n",
      "\n",
      "Anne \u001b[31mBradstreet\u001b[0m 's poetry is so tender and \u001b[31mpoignant\u001b[0m , Cotton \u001b[31mMather\u001b[0m 's \u001b[31mpreaching\u001b[0m is both \u001b[31mornery\u001b[0m and passionate , Richard \u001b[31mBaxter\u001b[0m 's \u001b[31mReformed\u001b[0m \u001b[31mPastor\u001b[0m is one of the classics , and Jonathan Edwards , is , well , Jonathan Edwards ( if you consider him a \u001b[31mPuritan\u001b[0m , not everyone does . ) Oh , by the way , great article about \u001b[31mPuritans\u001b[0m here . \n",
      "\n",
      "Anne \u001b[32mPoet\u001b[0m 's poetry is so tender and \u001b[32mpoignant\u001b[0m , Cotton \u001b[32mMather\u001b[0m 's \u001b[32mdiscourse\u001b[0m is both \u001b[32mornery\u001b[0m and passionate , Richard \u001b[32mBaxter\u001b[0m 's \u001b[32mRe-established\u001b[0m \u001b[32mMinister\u001b[0m is one of the classics , and Jonathan Edwards , is , well , Jonathan Edwards ( if you consider him a \u001b[32mPrude\u001b[0m , not everyone does . ) Oh , by the way , great article about \u001b[32mPrudes\u001b[0m here . \n",
      "\n",
      "i fancy you would , perhaps , like the rule of the Russians , who are very great friends of India and of \u001b[31mMahomedans\u001b[0m , and under whom the \u001b[31mHindus\u001b[0m will live in great comfort , and who will protect with the \u001b[31mtenderest\u001b[0m care the wealth and property which they have \u001b[31macquired\u001b[0m under English rule ? \n",
      "\n",
      "i fancy you would , perhaps , like the rule of the Russians , who are very great friends of India and of \u001b[32mMahomedans\u001b[0m , and under whom the \u001b[32mHindus\u001b[0m will live in great comfort , and who will protect with the \u001b[32mmost\u001b[0m untoughened care the wealth and property which they \u001b[32mhave\u001b[0m developed under English rule \n",
      "\n",
      "after breaking though into the first team at the \u001b[31mtenderest\u001b[0m of ages , the \u001b[31myoungster\u001b[0m received his full international call up shortly afterwards . \n",
      "\n",
      "after breaking though into the first team at the \u001b[32mtenderest\u001b[0m of ages , the \u001b[32mchild\u001b[0m received his full international call up shortly afterwards . \n",
      "\n",
      "yes , this had happened : the pirates to \u001b[31mWindward\u001b[0m and the pirates to \u001b[31mleeward\u001b[0m of the \u001b[31mAgra\u001b[0m had found out , at one and the same moment , that the merchant captain they had \u001b[31mlashed\u001b[0m , and \u001b[31mbullied\u001b[0m , and \u001b[31mtortured\u001b[0m was a patient but \u001b[31mtremendous\u001b[0m man . \n",
      "\n",
      "yes , this had happened : the pirates to \u001b[32mWindward\u001b[0m and the pirates to \u001b[32mleeward\u001b[0m of the \u001b[32mCity\u001b[0m had found out , at one and the same moment , that the merchant captain they had \u001b[32mwhip\u001b[0m , and \u001b[32mhector\u001b[0m , and \u001b[32mmurdered\u001b[0m was a patient but \u001b[32mgreat\u001b[0m man . \n",
      "\n",
      "these occurrences are never reported to the general public because they would cause a \u001b[31mtremendous\u001b[0m \u001b[31mbacklash\u001b[0m against the present system . \n",
      "\n",
      "these occurrences are never reported to the general public because they would cause a \u001b[32mgreat\u001b[0m \u001b[32mmovement\u001b[0m against the present system . \n",
      "\n",
      "`` it does not \u001b[31mfoster\u001b[0m the \u001b[31mtremendous\u001b[0m sense of accomplishment that a child receives when another person can read his writing , `` she said . \n",
      "\n",
      "`` it does not \u001b[32mpromote\u001b[0m the \u001b[32mgreat\u001b[0m sense of accomplishment that a child receives when another person can read his writing , `` she said . \n",
      "\n",
      "\u001b[31m--\u001b[0m Posted by jess on August 12 , 2003 12:23 PM I think this idea has \u001b[31mtremendous\u001b[0m promise . \n",
      "\n",
      "\u001b[32m--\u001b[0m Posted by jess on August 12 , 2003 12:23 PM I think this idea has \u001b[32mgreat\u001b[0m promise . \n",
      "\n",
      "over the last several decades , both the town and county have seen \u001b[31mtremendous\u001b[0m growth . \n",
      "\n",
      "over the last several decades , both the town and county have seen \u001b[32mgreat\u001b[0m growth . \n",
      "\n",
      "\u001b[31morganizational\u001b[0m \u001b[31mRe-Design\u001b[0m We have seen the need in our \u001b[31mtransformation\u001b[0m efforts to \u001b[31mre-design\u001b[0m some of our military \u001b[31morganizations\u001b[0m to harness the \u001b[31mtremendous\u001b[0m power of new technologies and exploit the \u001b[31msynergy\u001b[0m of joint forces . \n",
      "\n",
      "\u001b[32minstitutional\u001b[0m \u001b[32mRe-engineering\u001b[0m We have seen the need in our \u001b[32mchange\u001b[0m efforts to \u001b[32mre-engineering\u001b[0m some of our military \u001b[32minstitutions\u001b[0m to harness the \u001b[32mgreat\u001b[0m power of new technologies and exploit the \u001b[32maction\u001b[0m of joint forces . \n",
      "\n",
      "listen to the \u001b[31mKangaroo\u001b[0m , to her low \u001b[31mmoans\u001b[0m and \u001b[31msighs\u001b[0m , She 's just so upset about her \u001b[31mtremendous\u001b[0m \u001b[31mthighs\u001b[0m . \n",
      "\n",
      "listen to the \u001b[32mMarsupial\u001b[0m , to her low \u001b[32mgroans\u001b[0m and \u001b[32msuspirations\u001b[0m , She 's just so upset about her \u001b[32mgreat\u001b[0m \u001b[32msecond_joints\u001b[0m . \n",
      "\n",
      "the \u001b[31mtremendous\u001b[0m \u001b[31mmobilization\u001b[0m of resources made the Allied victory possible . \n",
      "\n",
      "the \u001b[32mgreat\u001b[0m \u001b[32mmovement\u001b[0m of resources made the Allied victory possible . \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at the September \u001b[31mgroundbreaking\u001b[0m for the new academic building at George \u001b[31mMason\u001b[0m University , Larry \u001b[31mCzarda\u001b[0m , vice president for \u001b[31moperations\u001b[0m , Prince William Campus said : the surge of \u001b[31meconomic\u001b[0m development in the Manassas/Prince William County area offers \u001b[31mtremendous\u001b[0m \u001b[31mopportunities\u001b[0m for growth and \u001b[31mprogramming\u001b[0m at the Prince William Campus as well as the potential for \u001b[31mforging\u001b[0m academic and research \u001b[31mpartnerships\u001b[0m with our corporate neighbors . \n",
      "\n",
      "at the September \u001b[32mcommencement\u001b[0m for the new academic building at George \u001b[32mMason\u001b[0m University , Larry \u001b[32mCzarda\u001b[0m , vice president for \u001b[32mactivities\u001b[0m , Prince William Campus said : the surge of \u001b[32mfinancial\u001b[0m development in the Manassas/Prince William County area offers \u001b[32mgreat\u001b[0m \u001b[32mbenefits\u001b[0m for growth and \u001b[32mproject\u001b[0m at the Prince William Campus as well as the potential for \u001b[32mestablishing\u001b[0m academic and research \u001b[32mcompanies\u001b[0m with our corporate neighbors . \n",
      "\n",
      "there 's a \u001b[31mtremendous\u001b[0m amount of information on the \u001b[31mKitimat\u001b[0m pipeline . \n",
      "\n",
      "there 's a \u001b[32mgreat\u001b[0m amount of information on the \u001b[32mKitimat\u001b[0m pipeline . \n",
      "\n",
      "a first negative social effect of \u001b[31mretrenchment\u001b[0m \u001b[31mprogrammes\u001b[0m is the \u001b[31mredistribution\u001b[0m of \u001b[31mincentives\u001b[0m given for \u001b[31mvoluntary\u001b[0m \u001b[31mretirement\u001b[0m ; these tend to attract the most skilled employees who have the best chances for advancement in the private sector but who are vital to the successful \u001b[31mimplementation\u001b[0m of the reform . \n",
      "\n",
      "a first negative social effect of \u001b[32meconomy\u001b[0m \u001b[32mactivities\u001b[0m is the \u001b[32mreallocation\u001b[0m of \u001b[32mbenefits\u001b[0m given for \u001b[32mwilling\u001b[0m \u001b[32mposition\u001b[0m ; these tend to attract the most skilled employees who have the best chances for advancement in the private sector but who are vital to the successful \u001b[32mapplication\u001b[0m of the reform . \n",
      "\n",
      "4 . ) bone is being \u001b[31mreabsorbed\u001b[0m from the skeleton in order to keep the vital blood level of calcium normal . \n",
      "\n",
      "4 . ) bone is being \u001b[32mreabsorbed\u001b[0m from the skeleton in order to keep the vital blood level of calcium normal . \n",
      "\n",
      "it says that it has been \u001b[31minundated\u001b[0m by \u001b[31mcorrespondence\u001b[0m from clubs that risk losing vital income that helps to keep their \u001b[31morganisations\u001b[0m alive . \n",
      "\n",
      "it says that it has been \u001b[32mflood\u001b[0m by \u001b[32mreference\u001b[0m from clubs that risk losing vital income that helps to keep their \u001b[32minstitutions\u001b[0m alive . \n",
      "\n",
      "( see Resources . ) Consulting them is a vital piece of the selection process as you 'll get a good idea of what 's appropriate or historic for your area . \n",
      "\n",
      "( see Resources . ) Consulting them is a vital piece of the selection process as you 'll get a good idea of what 's appropriate or historic for your area . \n",
      "\n",
      "information \u001b[31mtechnology\u001b[0m Keeping Out The Wrong People \u001b[31mtightened\u001b[0m visa rules are slowing the vital flow of \u001b[31mprofessionals\u001b[0m into the U.S. Like a \u001b[31mforlorn\u001b[0m \u001b[31mEstragon\u001b[0m from Samuel \u001b[31mBeckett\u001b[0m 's Waiting for \u001b[31mGodot\u001b[0m , \u001b[31mZubair\u001b[0m \u001b[31mMalik\u001b[0m has been waiting . \n",
      "\n",
      "information \u001b[32mapplication\u001b[0m Keeping Out The Wrong People \u001b[32mincreased\u001b[0m visa rules are slowing the vital flow of \u001b[32mphysicians\u001b[0m into the U.S. Like a \u001b[32mforlorn\u001b[0m \u001b[32mHerb\u001b[0m from Samuel \u001b[32mAuthor\u001b[0m 's Waiting for \u001b[32mGodot\u001b[0m , \u001b[32mZubair\u001b[0m \u001b[32mLeader\u001b[0m has been waiting . \n",
      "\n",
      "if anything , the mission \u001b[31munderstates\u001b[0m its own vital role in order to keep the focus on local churches and their \u001b[31mmissionaries\u001b[0m , but those of us who are served so well by \u001b[31mGFA\u001b[0m can \u001b[31mtestify\u001b[0m that we would not be anywhere near as effective without these skilled , dedicated \u001b[31myoke-fellows\u001b[0m in the Lord 's work . \n",
      "\n",
      "if anything , the mission \u001b[32mminimize\u001b[0m its own vital role in order to keep the focus on local churches and their \u001b[32mpriests\u001b[0m , but those of us who are served so well by \u001b[32mGFA\u001b[0m can \u001b[32mevidence\u001b[0m that we would not be anywhere near as effective without these skilled , dedicated \u001b[32myoke-fellows\u001b[0m in the Lord 's work . \n",
      "\n",
      "replacing \u001b[31mMideast\u001b[0m oil is vital , but not by \u001b[31msubstituting\u001b[0m equally or more \u001b[31mvulnerable\u001b[0m domestic sources . \n",
      "\n",
      "replacing \u001b[32mMideast\u001b[0m oil is vital , but not by \u001b[32minsert\u001b[0m equally or more \u001b[32mdangerous\u001b[0m domestic sources . \n",
      "\n",
      "protest is vital , but small , \u001b[31mincessant\u001b[0m protests are \u001b[31mimpotent\u001b[0m . * * * What Do You Do When The Message \u001b[31mWo\u001b[0m n't Get Through ? \n",
      "\n",
      "protest is vital , but small , \u001b[32mconstant\u001b[0m protests are \u001b[32munable\u001b[0m . * * * What Do You Do When The Message \u001b[32mWo\u001b[0m n't Get Through ? \n",
      "\n",
      "hearing aid \u001b[31mwearers\u001b[0m were provided access to the telephone system because Congress recognized the vital nature of the phone system and the importance of everyone , including people with hearing loss , being able to access it . \n",
      "\n",
      "hearing aid \u001b[32mwearers\u001b[0m were provided access to the telephone system because Congress recognized the vital nature of the phone system and the importance of everyone , including people with hearing loss , being able to access it . \n",
      "\n",
      "`` \u001b[31mtroops\u001b[0m continue to work to provide vital \u001b[31minfrastructure\u001b[0m \u001b[31mthroughout\u001b[0m Iraq . \n",
      "\n",
      "`` \u001b[32mforces\u001b[0m continue to work to provide vital \u001b[32msupport\u001b[0m \u001b[32maround\u001b[0m Iraq . \n",
      "\n",
      "China , India , Latin America and the rest of Asia will help grow worldwide demand for electric power by 59 percent over the next two decades . `` The \u001b[31mhydrogen\u001b[0m economy is based on the idea that fuel cells that run on \u001b[31mhydrogen\u001b[0m have the potential to replace current energy systems in all forms , from vehicle \u001b[31mpropulsion\u001b[0m to \u001b[31mstationary\u001b[0m power generation to mobile phone batteries . \n",
      "\n",
      "China , India , Latin America and the rest of Asia will help grow worldwide demand for electric power by 59 percent over the next two decades . `` The \u001b[32mgas\u001b[0m economy is based on the idea that fuel cells that run on \u001b[32mgas\u001b[0m have the potential to replace current energy systems in all forms , from vehicle \u001b[32mforce\u001b[0m to \u001b[32mstationary\u001b[0m power generation to mobile phone batteries . \n",
      "\n",
      "this has been , at times , merely an effort to gain greater public \u001b[31mrenown\u001b[0m for their group or cause , but more \u001b[31mtroubling\u001b[0m have been the groups seeking to push forward \u001b[31mal-Qaida\u001b[0m 's \u001b[31magenda\u001b[0m of worldwide terror . \n",
      "\n",
      "this has been , at times , merely an effort to gain greater public \u001b[32mreputation\u001b[0m for their group or cause , but more \u001b[32mbafflings\u001b[0m have been the groups seeking to push forward \u001b[32mforeign\u001b[0m terrorist \u001b[32morganization\u001b[0m 's matter of worldwide \n",
      "\n",
      "this is where the information \u001b[31mrevolution\u001b[0m is happening \u001b[31m--\u001b[0m and our share of these voices is smaller and smaller \u001b[31m--\u001b[0m so America must find way to reach and engage a wider , younger , more \u001b[31mdiverse\u001b[0m worldwide audience . \n",
      "\n",
      "this is where the information \u001b[32mchange\u001b[0m is happening \u001b[32m--\u001b[0m and our share of these voices is smaller and smaller \u001b[32m--\u001b[0m so America must find way to reach and engage a wider , younger , more \u001b[32mdifferent\u001b[0m worldwide audience . \n",
      "\n",
      "that same thing could happen if we had a \u001b[31mManhattan\u001b[0m type project focusing on \u001b[31mrenewables\u001b[0m , potential worldwide markets , if we are the leader , and we have every reason to be the leader because we have the biggest problem . \n",
      "\n",
      "that same thing could happen if we had a \u001b[32mManhattan\u001b[0m type project focusing on \u001b[32menergies\u001b[0m , potential worldwide markets , if we are the leader , and we have every reason to be the leader because we have the biggest problem . \n",
      "\n",
      "at the time of writing , the \u001b[31manti-semitic\u001b[0m policies of the \u001b[31mSoviet\u001b[0m Union are also a subject of worldwide protest . \n",
      "\n",
      "at the time of writing , the \u001b[32manti-jewish\u001b[0m policies of the \u001b[32mSoviet\u001b[0m Union are also a subject of worldwide protest . \n",
      "\n",
      "he suggested building an \u001b[31mexperimental\u001b[0m \u001b[31mhypertext\u001b[0m ' web ' for the worldwide community of \u001b[31mphysicists\u001b[0m who used \u001b[31mCERN\u001b[0m and its \u001b[31mpublications\u001b[0m . \n",
      "\n",
      "he suggested building an \u001b[32mempirical\u001b[0m \u001b[32mmachine-readable\u001b[0m text ' web ' for the worldwide community \u001b[32mof\u001b[0m physicists who \u001b[32mused\u001b[0m CERN and \u001b[32mits\u001b[0m books \n",
      "\n",
      "\u001b[31mfurthermore\u001b[0m , \u001b[31mnon-residents\u001b[0m can apply to be treated like a resident if his/her income from German sources is either at least 90 percent of his/her worldwide income or his/her \u001b[31mnon-German\u001b[0m source income is less than DM12,000 ( 24,000 for married couples ) . \n",
      "\n",
      "\u001b[32malso\u001b[0m , \u001b[32minvestors\u001b[0m can apply to be treated like a resident if his/her income from German sources is either at least 90 percent of his/her worldwide income or his/her \u001b[32mnon-German\u001b[0m source income is less than DM12,000 ( 24,000 for married couples ) . \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after adding the separate \u001b[31mpartnerships\u001b[0m of \u001b[31mJLW\u001b[0m Ireland and \u001b[31mJLW\u001b[0m Scotland , the combined total of our European and North American businesses \u001b[31mrepresented\u001b[0m \u001b[31mapproximately\u001b[0m 75 % of worldwide \u001b[31mrevenue\u001b[0m in 1998 , up from \u001b[31mapproximately\u001b[0m 63 % in 1997 . \n",
      "\n",
      "after adding the separate \u001b[32mcompanies\u001b[0m of \u001b[32mJLW\u001b[0m Ireland and \u001b[32mJLW\u001b[0m Scotland , the combined total of our European and North American businesses \u001b[32mpart\u001b[0m \u001b[32mabout\u001b[0m 75 % of worldwide \u001b[32mincome\u001b[0m in 1998 , up from \u001b[32mabout\u001b[0m 63 % in 1997 . \n",
      "\n",
      "\u001b[31mOSIS\u001b[0m also provides its users with direct , yet protected , access to the Internet and its broad range of worldwide open source information resources . \n",
      "\n",
      "\u001b[32mOSIS\u001b[0m also provides its users with direct , yet protected , access to the Internet and its broad range of worldwide open source information resources . \n",
      "\n",
      "like \u001b[31mMaypoles\u001b[0m , \u001b[31mwreaths\u001b[0m too , as she points out , figure in May Day as well as Summer \u001b[31msolstice\u001b[0m celebrations . http : //www.schooloftheseasons.com/signssummer.html [ Link updated 5/20/02 ] [ Added 26 May 2000 ] : this fun page collects worldwide \u001b[31minsights\u001b[0m from people who write to \u001b[31mWaverly\u001b[0m and comment on how summer begins for them . \n",
      "\n",
      "like \u001b[32mMaypoles\u001b[0m , \u001b[32mgarlands\u001b[0m too , as she points out , figure in May Day as well as Summer \u001b[32mcosmic\u001b[0m time celebrations . http : //www.schooloftheseasons.com/signssummer.html [ Link updated 5/20/02 ] [ Added 26 May 2000 ] : this fun page collects \u001b[32mworldwide\u001b[0m information from people who write \u001b[32mto\u001b[0m Waverly and comment on how summer begins for them \n",
      "\n",
      "may 13 , 2002 \u001b[31m-\u001b[0m Dunn 's \u001b[31mexecution\u001b[0m , scheduled for May 14th , was stayed by the Fifth \u001b[31mCircuit\u001b[0m pending \u001b[31mresolution\u001b[0m of the \u001b[31mappeal\u001b[0m . \n",
      "\n",
      "may 13 , 2002 \u001b[32mls\u001b[0m Dunn 's \u001b[32maction\u001b[0m , scheduled for May 14th , was stayed by the Fifth \u001b[32mTrack\u001b[0m pending \u001b[32mstatement\u001b[0m of the \u001b[32mproceedings\u001b[0m . \n",
      "\n",
      "No . 19504 \u001b[31mRegents\u001b[0m Action date : December 20 , 2001 Action : application for consent order granted ; Penalty agreed upon : 2 year \u001b[31msuspension\u001b[0m , \u001b[31mexecution\u001b[0m of \u001b[31msuspension\u001b[0m stayed , \u001b[31mprobation\u001b[0m 2 years , $ 5,000 fine . \n",
      "\n",
      "No . 19504 \u001b[32mTrustees\u001b[0m Action date : December 20 , 2001 Action : application for consent order granted ; Penalty agreed upon : 2 year \u001b[32mbreak\u001b[0m , \u001b[32maction\u001b[0m of \u001b[32mbreak\u001b[0m stayed , \u001b[32mrelease\u001b[0m 2 years , $ 5,000 fine . \n",
      "\n",
      "more broadly , it provides context for the question of what immediate \u001b[31mexecutions\u001b[0m are worth to investors . \n",
      "\n",
      "more broadly , it provides context for the question of what immediate \u001b[32mprisoners\u001b[0m are worth to investors . \n",
      "\n",
      "especially in a \u001b[31mlegislature\u001b[0m such as the English , in which the \u001b[31mresponsibility\u001b[0m for the \u001b[31mexecution\u001b[0m of the laws is and must be felt . \n",
      "\n",
      "especially in a \u001b[32mlaw-maker\u001b[0m such as the English , in which the \u001b[32mliability\u001b[0m for the \u001b[32maction\u001b[0m of the laws is and must be felt . \n",
      "\n",
      "found is set this way when the FOR loop \u001b[31mexits\u001b[0m ; inside the \u001b[31mexecution\u001b[0m of the loop , found is not modified by the FOR statement , although it may be changed by the \u001b[31mexecution\u001b[0m of other statements within the loop body . \n",
      "\n",
      "found is set this way when the FOR loop \u001b[32moutlets\u001b[0m ; inside the \u001b[32maction\u001b[0m of the loop , found is not modified by the FOR statement , although it may be changed by the \u001b[32maction\u001b[0m of other statements within the loop body . \n",
      "\n",
      "of course , this approach requires good \u001b[31mexecution\u001b[0m including strong \u001b[31mencryption\u001b[0m and \u001b[31mauthentication\u001b[0m , and secure key \u001b[31mmanagement\u001b[0m . \n",
      "\n",
      "of course , this approach requires good \u001b[32maction\u001b[0m including strong \u001b[32mcryptography\u001b[0m and \u001b[32mcertification\u001b[0m , and secure key \u001b[32mcontrol\u001b[0m . \n",
      "\n",
      "but a new \u001b[31msurvey\u001b[0m shows that the \u001b[31mexecution\u001b[0m of Ms. Tucker and the resulting debate led some residents of the Lone Star State to have second thoughts about capital punishment . \n",
      "\n",
      "but a new \u001b[32mstudy\u001b[0m shows that the \u001b[32maction\u001b[0m of Ms. Tucker and the resulting debate led some residents of the Lone Star State to have second thoughts about capital punishment . \n",
      "\n",
      "i understand that the presence of those individuals that came to the congress to \u001b[31mtestify\u001b[0m about the years they had spent awaiting \u001b[31mexecution\u001b[0m before having being declared innocent was particularly moving and valuable . \n",
      "\n",
      "i understand that the presence of those individuals that came to the congress to \u001b[32mevidence\u001b[0m about the years they had spent awaiting \u001b[32maction\u001b[0m before having being declared innocent was particularly moving and valuable . \n",
      "\n",
      "the most \u001b[31mpoignant\u001b[0m part of the book is when we learn of RAW 's loss of his \u001b[31m15-year\u001b[0m old daughter to a \u001b[31mmurderer\u001b[0m and how he \u001b[31mdealt\u001b[0m with the pain and grief ( it was not , \u001b[31mincidentally\u001b[0m , by calling the \u001b[31mmurderer\u001b[0m names and demanding his \u001b[31mexecution\u001b[0m ) . \n",
      "\n",
      "the most \u001b[32mpoignant\u001b[0m part of the book is when we learn of RAW 's loss of his \u001b[32m15-year\u001b[0m old daughter to a \u001b[32mkiller\u001b[0m and how he \u001b[32mmatters\u001b[0m with the pain and grief ( it was not , \u001b[32mapropos\u001b[0m , by calling the \u001b[32mkiller\u001b[0m names and demanding his \u001b[32maction\u001b[0m ) . \n",
      "\n",
      "although I told you after the program how pleased I was and how successful I felt everything went , what I did n't tell you after the program was this : on a scale of 1 to 10 , your \u001b[31mexecution\u001b[0m was also a `` 10 ! \n",
      "\n",
      "although I told you after the program how pleased I was and how successful I felt everything went , what I did n't tell you after the program was this : on a scale of 1 to 10 , your \u001b[32maction\u001b[0m was also a `` 10 ! \n",
      "\n",
      "happiness is a \u001b[31mby-product\u001b[0m of function . \n",
      "\n",
      "happiness is a \u001b[32mconsequence\u001b[0m of function . \n",
      "\n",
      "4 How can one generate the \u001b[31mprobability\u001b[0m \u001b[31mdensity\u001b[0m function of an \u001b[31mErlang\u001b[0m \u001b[31mdistribution\u001b[0m using Stella ? \n",
      "\n",
      "4 How can one generate the \u001b[32mamount\u001b[0m \u001b[32mconcentration\u001b[0m function of an \u001b[32mErlang\u001b[0m \u001b[32msystem\u001b[0m using Stella ? \n",
      "\n",
      "these \u001b[31mfunctions\u001b[0m also use \u001b[31mscheduling\u001b[0m \u001b[31mpriority\u001b[0m to decide which thread gets to \u001b[31mexecute\u001b[0m when there is \u001b[31mcontention\u001b[0m . \n",
      "\n",
      "these \u001b[32mpurposes\u001b[0m also use \u001b[32mplan\u001b[0m \u001b[32mprecedence\u001b[0m to decide which thread gets to \u001b[32maccomplish\u001b[0m when there is \u001b[32mdifference\u001b[0m . \n",
      "\n",
      "\u001b[31mfunctions\u001b[0m of the \u001b[31mPresidency\u001b[0m One way of bringing focus to the \u001b[31mpresidency\u001b[0m is to determine what the \u001b[31mfunctions\u001b[0m of the position should be . \n",
      "\n",
      "\u001b[32mpurposes\u001b[0m of the \u001b[32mOffice\u001b[0m One way of bringing focus to the \u001b[32moffice\u001b[0m is to determine what the \u001b[32mpurposes\u001b[0m of the position should be . \n",
      "\n",
      "the \u001b[31morientation\u001b[0m of \u001b[31munionids\u001b[0m in rivers as a function of the \u001b[31mhydrological\u001b[0m \u001b[31mvariability\u001b[0m . \n",
      "\n",
      "the \u001b[32mcourse\u001b[0m of \u001b[32munionids\u001b[0m in rivers as a function of the \u001b[32mhydrological\u001b[0m \u001b[32mvariation\u001b[0m . \n",
      "\n",
      "because of the obvious \u001b[31mincapacity\u001b[0m of these \u001b[31mbourgeois\u001b[0m classes to fulfill the normal \u001b[31meconomic\u001b[0m function of a \u001b[31mbourgeoisie\u001b[0m , each of them faces a \u001b[31msubversion\u001b[0m based on the \u001b[31mbureaucratic\u001b[0m model , more or less adapted to local \u001b[31mpeculiarities\u001b[0m , and eager to \u001b[31mseize\u001b[0m the \u001b[31mheritage\u001b[0m of this \u001b[31mbourgeoisie\u001b[0m . \n",
      "\n",
      "because of the obvious \u001b[32minability\u001b[0m of these \u001b[32mcapitalist\u001b[0m classes to fulfill the normal \u001b[32mfinancial\u001b[0m function of a \u001b[32mcapitalistie\u001b[0m , each of them faces a \u001b[32mcorruption\u001b[0m based on the \u001b[32mfunctionary\u001b[0m model , more or less adapted to local \u001b[32mcharacteristics\u001b[0m , and eager to \u001b[32mtake\u001b[0m the \u001b[32mpractice\u001b[0m of this \u001b[32mcapitalistie\u001b[0m . \n",
      "\n",
      "the \u001b[31mcomposition\u001b[0m , structure , function and \u001b[31mdistribution\u001b[0m of populations and the biology of population communities ( producers , consumers , and \u001b[31mdecomposers\u001b[0m ) , \u001b[31mecosystems\u001b[0m , \u001b[31mnutrient\u001b[0m \u001b[31mcycles\u001b[0m , energy flow , \u001b[31mbio-geographical\u001b[0m \u001b[31mcycles\u001b[0m , human impact on environment will be covered . \n",
      "\n",
      "the \u001b[32mmusic\u001b[0m , structure , function and \u001b[32msystem\u001b[0m of populations and the biology of population communities ( producers , consumers , and \u001b[32mdecomposers\u001b[0m ) , \u001b[32mspecies\u001b[0m , \u001b[32mnutritional\u001b[0m \u001b[32myears\u001b[0m , energy flow , \u001b[32mbio-geographical\u001b[0m \u001b[32myears\u001b[0m , human impact on environment will be covered . \n",
      "\n",
      "however , their arguments are identical in function to the \u001b[31mcreationists\u001b[0m ' arguments : rather than provide positive evidence for their own position , they mainly try to find weaknesses in natural selection . \n",
      "\n",
      "however , their arguments are identical in function to the \u001b[32mcreationists\u001b[0m ' arguments : rather than provide positive evidence for their own position , they mainly try to find weaknesses in natural selection . \n",
      "\n",
      "its main function is to act as a bridge between your \u001b[31mhi-fi\u001b[0m and computer , so that you can stream music from one to the other . \n",
      "\n",
      "its main function is to act as a bridge between your \u001b[32mreproducer\u001b[0m and computer , so that you can stream music from one to the other . \n",
      "\n",
      "where most experiments show only `` shadows `` of the wave function in the form of measurement outcomes , \u001b[31mKonrad\u001b[0m was able to go one step further , designing and \u001b[31mimplementing\u001b[0m a measurement scheme that is sensitive enough to reveal the complete function . \n",
      "\n",
      "where most experiments show only `` shadows `` of the wave function in the form of measurement outcomes , \u001b[32mKonrad\u001b[0m was able to go one step further , designing and \u001b[32mmeasures\u001b[0m a measurement scheme that is sensitive enough to reveal the complete function . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Test Data Before And After (Uncomment the below lines)\n",
    "\n",
    "\n",
    "# for line in test_data:\n",
    "#     sentence = line[0]\n",
    "#     new_sentence = simplify(sentence,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "a2b57a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(model, n):\n",
    "    \n",
    "    # We use TRank metric : # of candidates chosen with rank <= n\n",
    "    \n",
    "    candidates_matrix = []\n",
    "    sentence_list = []\n",
    "    word_candCount_dict = {}\n",
    "    word_phrase_dict = {}\n",
    "    scores_matrix = []\n",
    "    complex_word_list = []\n",
    "    num_candidates = 0\n",
    "    num_features = 7\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    \n",
    "    for line in test_data:\n",
    "        candidates = []\n",
    "        scores = []\n",
    "        scores_candidates = line[3:]\n",
    "        sentence = line[0]\n",
    "        sentence_list.append(sentence)\n",
    "        target_word = line[1]\n",
    "        complex_word_list.append(target_word)\n",
    "        num_candidates += len(scores_candidates)\n",
    "        word_candCount_dict[target_word] = len(scores_candidates)\n",
    "        for score_candidate in scores_candidates:\n",
    "            candidates.append(score_candidate[2:])\n",
    "            scores.append(int(score_candidate[0]))\n",
    "        \n",
    "        candidates_matrix.append(candidates)\n",
    "        scores_matrix.append(scores)\n",
    "        \n",
    "    indx = 0\n",
    "    for line in candidates_matrix:\n",
    "        word_phrase_dict = {}\n",
    "        feature_matrix = []\n",
    "        cosine_similarities = []\n",
    "        similarity_ratios = []\n",
    "        sem_similarity_ratios = []\n",
    "        sentence = sentence_list[indx]\n",
    "        complex_word = complex_word_list[indx]\n",
    "        scores_list = scores_matrix[indx]\n",
    "        for candidate in line:\n",
    "            prev_word = word_preceding(sentence.lower(), complex_word)\n",
    "            next_word = word_following(sentence.lower(), complex_word)\n",
    "            if(prev_word and next_word):\n",
    "                word_phrase_dict[candidate] = [prev_word, candidate, next_word]\n",
    "            feature_list_ = extractFeaturesFromWord(candidate,word_phrase_dict)\n",
    "            if(complex_word in word_vectors and candidate in word_vectors):     \n",
    "                complex_word_vector = word_vectors[complex_word]\n",
    "                substitution_vector = word_vectors[candidate]\n",
    "                cos_similarity = getCosSim(complex_word_vector, substitution_vector)\n",
    "            else:\n",
    "                cos_similarity = 0\n",
    "            cosine_similarities.append(cos_similarity)\n",
    "            similarity_ratios.append(similarityRatio(complex_word, candidate))\n",
    "            sem_similarity_ratios.append(semanticSimilarityRatio(target_word.lower(), candidate.lower()))\n",
    "            feature_matrix.append(feature_list_)\n",
    "        cosine_similarities = np.array(cosine_similarities).reshape(len(feature_matrix),1)\n",
    "        similarity_ratios = np.array(similarity_ratios).reshape(len(feature_matrix),1)\n",
    "        sem_similarity_ratios = np.array(sem_similarity_ratios).reshape(len(feature_matrix),1)\n",
    "        X = np.hstack((feature_matrix,cosine_similarities,sem_similarity_ratios))\n",
    "        max_in_column = np.max(X,axis=0)\n",
    "        for i in range(num_features):\n",
    "            if(max_in_column[i] != 0):\n",
    "                X[:, i] /= max_in_column[i]\n",
    "                \n",
    "        prediction = predict(model, X)\n",
    "        min_value = min(prediction)\n",
    "        prediction_list = prediction.tolist()\n",
    "        min_index=prediction_list.index(min_value)\n",
    "        reference_score = scores_list[min_index]\n",
    "        \n",
    "        if(reference_score <= n):\n",
    "            num_correct+=1\n",
    "        \n",
    "        num_total +=1\n",
    "        indx+=1\n",
    "    \n",
    "    accuracy = num_correct/num_total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b1814748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ranker at n=1 is  0.6162162162162163\n",
      "Accuracy of ranker at n=2 is  0.918918918918919\n",
      "Accuracy of ranker at n=3 is  1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = testModel(model,1)\n",
    "print(\"Accuracy of ranker at n=1 is \",accuracy)\n",
    "\n",
    "accuracy = testModel(model,2)\n",
    "print(\"Accuracy of ranker at n=2 is \",accuracy)\n",
    "\n",
    "accuracy = testModel(model,3)\n",
    "print(\"Accuracy of ranker at n=3 is \",accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
