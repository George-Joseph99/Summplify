{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c501685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk\n",
    "import io\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from itertools import islice\n",
    "from rouge import Rouge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9fd77f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download()  # uncomment these lines once they are not downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb19c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(article):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "    article_preprocessed = []\n",
    "    sentences = sent_tokenize(article)\n",
    "    for sentence in sentences:\n",
    "        sentence_preprocessed = []\n",
    "        sentence = re.sub(r\"[^a-zA-Z\\s]+\", \"\", sentence)\n",
    "        words = word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            if (word not in stopwords_english and word not in string.punctuation):\n",
    "                word_stemmed = stemmer.stem(word)  \n",
    "                sentence_preprocessed.append(word_stemmed)\n",
    "#         sentence_preprocessed = \" \".join(sentence_preprocessed)\n",
    "        if sentence_preprocessed:\n",
    "            article_preprocessed.append(sentence_preprocessed)\n",
    "            \n",
    "        \n",
    "        \n",
    "#     words = [word_tokenize(sent) for sent in sentences]\n",
    "#     words_without_stopwords = [[word for word in sent if word not in stopwords.words('english')] for sent in words]\n",
    "    return article_preprocessed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d0a784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['quarterli', 'profit', 'us', 'media', 'giant', 'timewarn', 'jump', 'bn', 'three', 'month', 'decemb', 'yearearli'], ['the', 'firm', 'one', 'biggest', 'investor', 'googl', 'benefit', 'sale', 'highspe', 'internet', 'connect', 'higher', 'advert', 'sale'], ['timewarn', 'said', 'fourth', 'quarter', 'sale', 'rose', 'bn', 'bn'], ['it', 'profit', 'buoy', 'oneoff', 'gain', 'offset', 'profit', 'dip', 'warner', 'bro', 'less', 'user', 'aol'], ['time', 'warner', 'said', 'friday', 'own', 'searchengin', 'googl'], ['but', 'internet', 'busi', 'aol', 'mix', 'fortun'], ['it', 'lost', 'subscrib', 'fourth', 'quarter', 'profit', 'lower', 'preced', 'three', 'quarter'], ['howev', 'compani', 'said', 'aol', 'underli', 'profit', 'except', 'item', 'rose', 'back', 'stronger', 'internet', 'advertis', 'revenu'], ['it', 'hope', 'increas', 'subscrib', 'offer', 'onlin', 'servic', 'free', 'timewarn', 'internet', 'custom', 'tri', 'sign', 'aol', 'exist', 'custom', 'highspe', 'broadband'], ['timewarn', 'also', 'restat', 'result', 'follow', 'probe', 'us', 'secur', 'exchang', 'commiss', 'sec', 'close', 'conclud'], ['time', 'warner', 'fourth', 'quarter', 'profit', 'slightli', 'better', 'analyst', 'expect'], ['but', 'film', 'divis', 'saw', 'profit', 'slump', 'help', 'boxoffic', 'flop', 'alexand', 'catwoman', 'sharp', 'contrast', 'yearearli', 'third', 'final', 'film', 'lord', 'ring', 'trilog', 'boost', 'result'], ['for', 'fullyear', 'timewarn', 'post', 'profit', 'bn', 'perform', 'revenu', 'grew', 'bn'], ['our', 'financi', 'perform', 'strong', 'meet', 'exceed', 'fullyear', 'object', 'greatli', 'enhanc', 'flexibl', 'chairman', 'chief', 'execut', 'richard', 'parson', 'said'], ['for', 'timewarn', 'project', 'oper', 'earn', 'growth', 'around', 'also', 'expect', 'higher', 'revenu', 'wider', 'profit', 'margin'], ['timewarn', 'restat', 'account', 'part', 'effort', 'resolv', 'inquiri', 'aol', 'us', 'market', 'regul'], ['it', 'alreadi', 'offer', 'pay', 'settl', 'charg', 'deal', 'review', 'sec'], ['the', 'compani', 'said', 'unabl', 'estim', 'amount', 'need', 'set', 'asid', 'legal', 'reserv', 'previous', 'set'], ['it', 'intend', 'adjust', 'way', 'account', 'deal', 'german', 'music', 'publish', 'bertelsmann', 'purchas', 'stake', 'aol', 'europ', 'report', 'advertis', 'revenu'], ['it', 'book', 'sale', 'stake', 'aol', 'europ', 'loss', 'valu', 'stake']]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "article_file = io.open(\"articles/original (\" + str(1) +\").txt\", \"r\", encoding='utf-8-sig')\n",
    "article_file.readline()\n",
    "article = article_file.read()\n",
    "article_file.close()\n",
    "\n",
    "# print(article)\n",
    "article_preprocessed = preprocessing(article)\n",
    "print(article_preprocessed)\n",
    "print(len(article_preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f41b89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_string(sentences):  # converts list of lists to list of strings\n",
    "    sentences_modified = []   # list of strings\n",
    "    for sentence in sentences:\n",
    "        sentence_modified = ' '.join(sentence)\n",
    "        sentences_modified.append(sentence_modified)\n",
    "    return sentences_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fdb016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature 1\n",
    "\n",
    "def calculate_TF_IDF(content):\n",
    "    flat_words = [word for sent in content for word in sent]\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(flat_words)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    word_scores = {}\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        word_scores[feature] = tfidf_matrix[:, i].sum()\n",
    "    return word_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dd44d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_each_sentence_score(article_preprocessed, word_scores):\n",
    "    sentence_scores = []\n",
    "    for sent in article_preprocessed:\n",
    "        score = 0\n",
    "        for word in sent:\n",
    "            score += word_scores.get(word, 0)\n",
    "        sentence_scores.append(score)\n",
    "        \n",
    "    sentence_scores = sentence_scores / max(sentence_scores)\n",
    "    return sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed29965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(sentences, sentence_scores, num_sentences):\n",
    "    top_sentences_idx = sorted(range(len(sentence_scores)), key=lambda i: sentence_scores[i], reverse=True)[:num_sentences]\n",
    "    summary = [sentences[i] for i in top_sentences_idx]\n",
    "    return ' '.join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44f0eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tf_idf(file_name): # using tf-idf\n",
    "    \n",
    "    article_file = io.open(\"articles/original (\" + str(file_name) +\").txt\", \"r\", encoding='utf-8-sig')\n",
    "    article_file.readline()\n",
    "    article = article_file.read()\n",
    "    article_file.close()\n",
    "    \n",
    "    sentences = sent_tokenize(article)\n",
    "    sentences[0] = sentences[0][1:]\n",
    "    \n",
    "    summarized_file = io.open(\"articles/summarized (\" + str(file_name) +\").txt\", \"r\", encoding='utf-8-sig')\n",
    "    summarized_original = summarized_file.read()\n",
    "    summarized_file.close()\n",
    "\n",
    "    article_preprocessed = preprocessing(article)\n",
    "    word_scores = calculate_TF_IDF(article_preprocessed)\n",
    "    sentence_scores = calculate_each_sentence_score(article_preprocessed, word_scores)\n",
    "    summary = generate_summary(sentences, sentence_scores, 7)\n",
    "    \n",
    "    print(sentence_scores)\n",
    "    rouge = Rouge(metrics=['rouge-n', 'rouge-l'], max_n=2)\n",
    "    scores_tf_idf = rouge.get_scores(summary, summarized_original)\n",
    "    \n",
    "    rouge_1_tf_idf = scores_tf_idf['rouge-1']['f']\n",
    "    rouge_2_tf_idf = scores_tf_idf['rouge-2']['f']\n",
    "    rouge_l_tf_idf = scores_tf_idf['rouge-l']['f']\n",
    "    print('t-idf accuracy')\n",
    "    print('Rouge 1 score is: %f' % (rouge_1_tf_idf))\n",
    "    print('Rouge 2 score is: %f' % (rouge_2_tf_idf))\n",
    "    print('Rouge l score is: %f' % (rouge_l_tf_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e519623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75       0.28571429 0.64285714 0.96428571 0.85714286 0.60714286\n",
      " 0.75       0.82142857 0.64285714 1.         0.64285714 0.32142857\n",
      " 0.35714286 0.96428571]\n",
      "t-idf accuracy\n",
      "Rouge 1 score is: 0.709402\n",
      "Rouge 2 score is: 0.629310\n",
      "Rouge l score is: 0.529915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "test_tf_idf(301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17129375",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature 2\n",
    "\n",
    "def sentence_length(article_preprocessed):\n",
    "    article_preprocessed = convert_list_to_string(article_preprocessed)\n",
    "    max_length = 0\n",
    "    for sentence in article_preprocessed:\n",
    "        # print(sentence)\n",
    "        if len(sentence.split()) > max_length:\n",
    "            max_length = len(sentence.split())\n",
    "            \n",
    "    sentence_length_feature = []\n",
    "    for sentence in article_preprocessed:\n",
    "        sentence_length_feature.append(len(sentence.split()) / max_length)\n",
    "\n",
    "#     sentence_length_feature = []\n",
    "#     for sentence in article_preprocessed:\n",
    "#         sentence_length_feature.append(1 / len(sentence.split()))\n",
    "\n",
    "    return sentence_length_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a4b22f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5454545454545454,\n",
       " 0.6363636363636364,\n",
       " 0.36363636363636365,\n",
       " 0.5909090909090909,\n",
       " 0.3181818181818182,\n",
       " 0.2727272727272727,\n",
       " 0.45454545454545453,\n",
       " 0.6363636363636364,\n",
       " 0.8181818181818182,\n",
       " 0.5909090909090909,\n",
       " 0.4090909090909091,\n",
       " 1.0,\n",
       " 0.45454545454545453,\n",
       " 0.7727272727272727,\n",
       " 0.6363636363636364,\n",
       " 0.5,\n",
       " 0.4090909090909091,\n",
       " 0.5909090909090909,\n",
       " 0.7727272727272727,\n",
       " 0.4090909090909091]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_file = io.open(\"articles/original (\" + str(1) +\").txt\", \"r\", encoding='utf-8-sig')\n",
    "article_file.readline()\n",
    "art = article_file.read()\n",
    "article_file.close()\n",
    "\n",
    "art = preprocessing(art)\n",
    "art = convert_list_to_string(art)\n",
    "sentence_length(article_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fc1f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_X_labels(article_preprocessed):\n",
    "    # feature 1 (tf_idf)\n",
    "    word_scores = calculate_TF_IDF(article_preprocessed)\n",
    "    tf_idf_score_feature = calculate_each_sentence_score(article_preprocessed, word_scores)\n",
    "    \n",
    "    # feature 2 (sentence_length)\n",
    "    sentence_length_feature = sentence_length(article_preprocessed)\n",
    "    \n",
    "    matrix = np.column_stack((tf_idf_score_feature, sentence_length_feature))\n",
    "#     matrix = np.array(tf_idf_score_feature).reshape(len(tf_idf_score_feature), 1)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c41f3ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.79069767 0.54545455]\n",
      " [0.62790698 0.63636364]\n",
      " [0.81395349 0.36363636]\n",
      " [0.97674419 0.59090909]\n",
      " [0.34883721 0.31818182]\n",
      " [0.37209302 0.27272727]\n",
      " [0.76744186 0.45454545]\n",
      " [0.95348837 0.63636364]\n",
      " [1.         0.81818182]\n",
      " [0.58139535 0.59090909]\n",
      " [0.60465116 0.40909091]\n",
      " [0.81395349 1.        ]\n",
      " [0.88372093 0.45454545]\n",
      " [0.53488372 0.77272727]\n",
      " [0.81395349 0.63636364]\n",
      " [0.62790698 0.5       ]\n",
      " [0.39534884 0.40909091]\n",
      " [0.48837209 0.59090909]\n",
      " [0.86046512 0.77272727]\n",
      " [0.65116279 0.40909091]]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "article_file = io.open(\"articles/original (\" + str(1) +\").txt\", \"r\", encoding='utf-8-sig')\n",
    "article_file.readline()\n",
    "article = article_file.read()\n",
    "article_file.close()\n",
    "\n",
    "art = preprocessing(article)\n",
    "art = convert_list_to_string(art)\n",
    "print(generate_X_labels(article_preprocessed))\n",
    "print(len(generate_X_labels(article_preprocessed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1be1abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Y_labels(original, summarized):\n",
    "    Y_list = []\n",
    "    original_sentences = sent_tokenize(original)\n",
    "    original_sentences[0] = original_sentences[0][1:] # to remove the \\n\n",
    "    summarized_sentences = sent_tokenize(summarized)\n",
    "    \n",
    "    for original_sentence in original_sentences:\n",
    "        added = 0\n",
    "        for summarized_sentence in summarized_sentences:\n",
    "            if original_sentence in summarized_sentence:\n",
    "                Y_list.append(1)\n",
    "                added = 1\n",
    "                break\n",
    "        if added == 0:\n",
    "            Y_list.append(0)\n",
    "    \n",
    "    return Y_list, original_sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad975c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0]\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "article_file = io.open(\"articles/original (\" + str(301) +\").txt\", \"r\", encoding='utf-8-sig')\n",
    "article_file.readline()\n",
    "article = article_file.read()\n",
    "article_file.close()\n",
    "\n",
    "summarized_file = io.open(\"articles/summarized (\" + str(301) +\").txt\", \"r\", encoding='utf-8-sig')\n",
    "summarized = summarized_file.read()\n",
    "summarized_file.close()\n",
    "\n",
    "Y,_ = generate_Y_labels(article, summarized)\n",
    "print(Y)\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5378413a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4179\n",
      "4179\n"
     ]
    }
   ],
   "source": [
    "X_matrix = []\n",
    "X = []\n",
    "Y = []\n",
    "sentences = []\n",
    "\n",
    "article_types = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "\n",
    "for article_type in article_types:\n",
    "    for i in range (1, 51):   # loading business articles\n",
    "        article_file = io.open(\"train_original/\" + article_type + \"/article (\" + str(i) +\").txt\", \"r\", encoding='utf-8-sig')\n",
    "        article_file.readline()\n",
    "        article = article_file.read()\n",
    "        article_file.close()\n",
    "\n",
    "        summarized_file = io.open(\"train_summary/\" + article_type + \"/summary (\" + str(i) +\").txt\", \"r\", encoding='utf-8-sig')\n",
    "        summarized = summarized_file.read()\n",
    "        summarized_file.close()\n",
    "\n",
    "        article_preprocessed = preprocessing(article)\n",
    "    #     article_preprocessed = convert_list_to_string(article_preprocessed)\n",
    "        X_i = generate_X_labels(article_preprocessed)\n",
    "        Y_i, original_list_no_first_space = generate_Y_labels(article, summarized)\n",
    "\n",
    "        if(len(X_i) != len(Y_i)):\n",
    "            print('Error! features and labels are not equal in length')\n",
    "\n",
    "        Y.extend(Y_i)\n",
    "        X_matrix.extend(X_i)\n",
    "        sentences.extend(original_list_no_first_space)\n",
    "    \n",
    "\n",
    "for x in X_matrix:\n",
    "    X.append(x.tolist())\n",
    "    \n",
    "X = np.matrix(X)\n",
    "\n",
    "m = len(X)\n",
    "# m = 500\n",
    "print(len(X))\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d42a131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_input_dim = 2 # input layer size (we have two input features)\n",
    "nn_output_dim = 1  # output layer size (we have one output)\n",
    "\n",
    "# Gradient descent parameters\n",
    "alpha = 0.1  # learning rate for gradient descent\n",
    "# print(Y)\n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be4a9b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # TODO 1: Compute the sigmoid function at the given x (~1 line)\n",
    "    # For example: sigmoid(2) should compute the value of sigmoid function at x = 2.\n",
    "    # Hint: Use np.exp instead of math.exp to allow for vectorization.\n",
    "    #----------------------------------------------------------------------------------------------\n",
    "    sig = (1/(1+np.exp(-x)))\n",
    "    #----------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1468bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(nn_hdim, num_passes=20000, print_loss=False):\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(nn_hdim, nn_input_dim) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((nn_hdim, 1))\n",
    "    W2 = np.random.randn(nn_output_dim, nn_hdim) / np.sqrt(nn_hdim)\n",
    "    b2 = np.zeros((nn_output_dim, 1))\n",
    "\n",
    "    model = {}\n",
    "\n",
    "    for i in range(0, num_passes):\n",
    "        DW1 = 0\n",
    "        DW2 = 0\n",
    "        Db1 = 0\n",
    "        Db2 = 0\n",
    "        cost = 0\n",
    "\n",
    "        for j in range(0, m):\n",
    "            a0 = X[j, :].reshape(-1, 1)  # Every training example is a column vector.\n",
    "            y = Y[j]\n",
    "            \n",
    "            z1 = np.dot(W1 , a0 )+ b1\n",
    "            a1 = np.tanh(z1)\n",
    "            z2 = np.dot(W2 , a1) + b2\n",
    "            a2 = sigmoid(z2)\n",
    "            \n",
    "#             if (i == num_passes -1 ):\n",
    "#                 print('True value: %f, got: %f'% (y, a2))\n",
    "\n",
    "            cost_j = -1 * ((np.log(a2) * y + (1-y)* np.log(1-a2)))\n",
    "\n",
    "            da2 =  ( -y/a2  + (1-y)/(1-a2) )\n",
    "            dz2 =  da2 * a2 * ( 1 - a2)\n",
    "            dW2 = np.dot(dz2 , a1.T)\n",
    "            db2 = dz2\n",
    "\n",
    "            da1 =  np.dot(dz2,W2).T\n",
    "            dz1 = np.multiply(da1 , 1 - np.square(a1) )\n",
    "            dW1 = np.dot(dz1 , a0.T )\n",
    "            db1 = dz1\n",
    "\n",
    "            DW1 += dW1\n",
    "            DW2 += dW2\n",
    "            Db2 += db2\n",
    "            Db1 += db1\n",
    "            cost += cost_j\n",
    "        \n",
    "        DW1 /= m\n",
    "        DW2 /= m\n",
    "        Db1 /= m\n",
    "        Db2 /= m\n",
    "        cost /= m\n",
    "\n",
    "        W1 -= alpha * DW1\n",
    "        b1 -= alpha * Db1\n",
    "        W2 -= alpha * DW2\n",
    "        b2 -= alpha * Db2\n",
    "\n",
    "        model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\n",
    "        if print_loss and i % 1000 == 0:\n",
    "            print(\"Loss after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bd38e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to predict an output (0 or 1)\n",
    "def predict(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    a0 = x.T\n",
    "    \n",
    "    # TODO 6 (aka TODO 2): Apply forward propagation on every test example a0 (a column vector 2x1) with its\n",
    "    #  corresponding label y. It is required to compute z1, a1, z2, and a2  (SAME AS TODO2).\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    z1 = np.dot(W1 , a0) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = np.dot(W2 , a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    # ------------------------------------------------------------------------------------------------\n",
    "    # Applying a threshold of 0.5 (i.e. predictions greater than 0.5 are mapped to 1, and 0 otherwise)\n",
    "#     prediction = np.round(a2)\n",
    "    prediction = a2\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9854a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.703959\n",
      "Loss after iteration 1000: 0.480920\n",
      "Loss after iteration 2000: 0.477243\n",
      "Loss after iteration 3000: 0.476367\n",
      "Loss after iteration 4000: 0.476015\n",
      "Loss after iteration 5000: 0.475833\n",
      "Loss after iteration 6000: 0.475719\n",
      "Loss after iteration 7000: 0.475636\n",
      "Loss after iteration 8000: 0.475568\n",
      "Loss after iteration 9000: 0.475507\n",
      "Loss after iteration 10000: 0.475452\n"
     ]
    }
   ],
   "source": [
    "model = build_model(nn_hdim= 8, num_passes = 10001, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3be39ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 2.65985065, -0.5365754 ],\n",
      "       [ 0.6183042 ,  1.65977413],\n",
      "       [ 1.9195185 , -1.87884063],\n",
      "       [ 0.69353291, -0.08007882],\n",
      "       [-1.08322081,  1.40649314],\n",
      "       [ 0.20895092,  1.01007634],\n",
      "       [ 1.3257662 , -0.30570778],\n",
      "       [ 0.11674894,  0.35820707]]), 'b1': array([[-0.56912543],\n",
      "       [ 0.02103428],\n",
      "       [-0.24905846],\n",
      "       [ 0.1409427 ],\n",
      "       [ 0.2860866 ],\n",
      "       [-0.05592954],\n",
      "       [-0.06771996],\n",
      "       [ 0.113594  ]]), 'W2': array([[ 2.3577487 , -0.3403027 ,  2.22138009,  0.28017802, -1.93652971,\n",
      "        -0.08309288,  1.22371814, -0.24322531]]), 'b2': array([[-0.39537232]])}\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec7bb518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(article_preprocessed_test, original_test, summarized_text, compression_ratio, file_number = 0):\n",
    "    X_test = generate_X_labels(article_preprocessed_test)\n",
    "    predicton = predict(model, X_test)\n",
    "    Y_test, original_sentences = generate_Y_labels(original_test, summarized_text)\n",
    "    num_sentences_summarized = math.ceil(compression_ratio * len(original_sentences))\n",
    "    \n",
    "    \n",
    "    highest = np.argsort(predicton[0]) [::-1]\n",
    "    highest = highest[: num_sentences_summarized]\n",
    "#     highest = sorted(highest) # uncomment to arrange the article\n",
    "    output_sentences = []\n",
    "    output_indices = []\n",
    "    \n",
    "\n",
    "    \n",
    "    for i in range (0, num_sentences_summarized):\n",
    "        output_sentences.append(original_sentences[highest[i]])\n",
    "        output_indices.append(highest[i])\n",
    "        \n",
    "    output_sentences = ''.join(output_sentences)\n",
    "    \n",
    "    rouge = Rouge(metrics=['rouge-n', 'rouge-l'], max_n=2)\n",
    "    scores_nn = rouge.get_scores(output_sentences, summarized_text)\n",
    "    \n",
    "    rouge_1_nn = scores_nn['rouge-1']['f']\n",
    "    rouge_2_nn = scores_nn['rouge-2']['f']\n",
    "    rouge_l_nn = scores_nn['rouge-l']['f']\n",
    "    \n",
    "#     print('article number: %d' % (file_number))\n",
    "#     print('nn accuracy')\n",
    "#     print('Rouge 1 score is: %f' % (rouge_1_nn))\n",
    "#     print('Rouge 2 score is: %f' % (rouge_2_nn))\n",
    "#     print('Rouge l score is: %f' % (rouge_l_nn))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### classified using TF_idf score\n",
    "\n",
    "    output_tf_idf = extractive_summary(original_test, num_sentences_summarized)\n",
    "    \n",
    "    scores_tf_idf = rouge.get_scores(output_tf_idf, summarized_text)\n",
    "    \n",
    "    rouge_1_tf_idf = scores_tf_idf['rouge-1']['f']\n",
    "    rouge_2_tf_idf = scores_tf_idf['rouge-2']['f']\n",
    "    rouge_l_tf_idf = scores_tf_idf['rouge-l']['f']\n",
    "    \n",
    "#     print('t-idf accuracy')\n",
    "#     print('Rouge 1 score is: %f' % (rouge_1_tf_idf))\n",
    "#     print('Rouge 2 score is: %f' % (rouge_2_tf_idf))\n",
    "#     print('Rouge l score is: %f' % (rouge_l_tf_idf))\n",
    "    \n",
    "    return rouge_1_nn, rouge_2_nn, rouge_l_nn, rouge_1_tf_idf, rouge_2_tf_idf, rouge_l_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c087f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractive_summary(text, num_sentences):\n",
    "    # Preprocess the text\n",
    "    text = text.lower()\n",
    "    text = ''.join(c for c in text if c not in '1234567890')\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = [word_tokenize(sent) for sent in sentences]\n",
    "    words_without_stopwords = [[word for word in sent if word not in stopwords.words('english')] for sent in words]\n",
    "    \n",
    "    # Calculate TF-IDF scores\n",
    "    flat_words = [word for sent in words_without_stopwords for word in sent]\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(flat_words)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    word_scores = {}\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        word_scores[feature] = tfidf_matrix[:, i].sum()\n",
    "    \n",
    "    # Calculate sentence scores\n",
    "    sentence_scores = []\n",
    "    for sent in words_without_stopwords:\n",
    "        score = 0\n",
    "        for word in sent:\n",
    "            score += word_scores.get(word, 0)\n",
    "        sentence_scores.append(score)\n",
    "    \n",
    "    # Select top N sentences with highest scores\n",
    "    top_sentences_idx = sorted(range(len(sentence_scores)), key=lambda i: sentence_scores[i], reverse=True)[:num_sentences]\n",
    "    summary = [sentences[i] for i in top_sentences_idx]\n",
    "    return ' '.join(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41f7af17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using nn\n",
      "Average Rouge 1 score is: 0.786239\n",
      "Average Rouge 2 score is: 0.716092\n",
      "Average Rouge l score is: 0.574576\n",
      "Using tf_idf only\n",
      "Average Rouge 1 score is: 0.686983\n",
      "Average Rouge 2 score is: 0.575671\n",
      "Average Rouge l score is: 0.503824\n"
     ]
    }
   ],
   "source": [
    "# precision_nn = []\n",
    "# recall_nn = []\n",
    "# precision_tf_idf = []\n",
    "# recall_tf_idf = []\n",
    "\n",
    "rouge_1_list_nn = []\n",
    "rouge_2_list_nn = []\n",
    "rouge_l_list_nn = []\n",
    "\n",
    "rouge_1_list_tf_idf = []\n",
    "rouge_2_list_tf_idf = []\n",
    "rouge_l_list_tf_idf = []\n",
    "\n",
    "article_types = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "\n",
    "for article_type in article_types:\n",
    "    for i in range(101, 131):\n",
    "\n",
    "        article_file = io.open(\"train_original/\" + article_type + \"/article (\" + str(i) +\").txt\", \"r\", encoding='utf-8-sig')\n",
    "        article_file.readline()\n",
    "        article = article_file.read()\n",
    "        article_preprocessed = preprocessing(article)\n",
    "        article_file.close()\n",
    "\n",
    "        summarized_file = io.open(\"train_summary/\" + article_type + \"/summary (\" + str(i) +\").txt\", \"r\", encoding='utf-8-sig')\n",
    "        summarized = summarized_file.read()\n",
    "        summarized_file.close()\n",
    "\n",
    "        rouge_1_nn, rouge_2_nn, rouge_l_nn, rouge_1_tf_idf, rouge_2_tf_idf, rouge_l_tf_idf = test(\n",
    "            article_preprocessed, article, summarized, 0.35, i)\n",
    "\n",
    "        rouge_1_list_nn.append(rouge_1_nn)\n",
    "        rouge_2_list_nn.append(rouge_2_nn)\n",
    "        rouge_l_list_nn.append(rouge_l_nn)\n",
    "\n",
    "        rouge_1_list_tf_idf.append(rouge_1_tf_idf)\n",
    "        rouge_2_list_tf_idf.append(rouge_2_tf_idf)\n",
    "        rouge_l_list_tf_idf.append(rouge_l_tf_idf)\n",
    "\n",
    "\n",
    "print('Using nn')\n",
    "print('Average Rouge 1 score is: %f' % (np.average(rouge_1_list_nn)))\n",
    "print('Average Rouge 2 score is: %f' % (np.average(rouge_2_list_nn)))\n",
    "print('Average Rouge l score is: %f' % (np.average(rouge_l_list_nn)))\n",
    "\n",
    "print('Using tf_idf only')\n",
    "print('Average Rouge 1 score is: %f' % (np.average(rouge_1_list_tf_idf)))\n",
    "print('Average Rouge 2 score is: %f' % (np.average(rouge_2_list_tf_idf)))\n",
    "print('Average Rouge l score is: %f' % (np.average(rouge_l_list_tf_idf)))\n",
    "\n",
    "# print('Neural network accuracy: ')\n",
    "# print('Average precision score is: %f' % (np.average(precision_nn)))\n",
    "# print('Average recall score is: %f' % (np.average(recall_nn)))\n",
    "      \n",
    "# print('Classical approach accuracy using tf-idf: ')\n",
    "# print('Average precision score is: %f' % (np.average(precision_tf_idf)))\n",
    "# print('Average recall score is: %f' % (np.average(recall_tf_idf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f249d2db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using nn\n",
      "Average Rouge 1 score is: 0.326038\n",
      "Average Rouge 2 score is: 0.130973\n",
      "Average Rouge l score is: 0.268904\n",
      "Using tf_idf only\n",
      "Average Rouge 1 score is: 0.309745\n",
      "Average Rouge 2 score is: 0.109878\n",
      "Average Rouge l score is: 0.251321\n"
     ]
    }
   ],
   "source": [
    "rouge_1_list_nn = []\n",
    "rouge_2_list_nn = []\n",
    "rouge_l_list_nn = []\n",
    "\n",
    "rouge_1_list_tf_idf = []\n",
    "rouge_2_list_tf_idf = []\n",
    "rouge_l_list_tf_idf = []\n",
    "\n",
    "df = pd.read_csv('test.csv')\n",
    "articles_cnn = df['article']\n",
    "summaries_cnn = df['highlights']\n",
    "\n",
    "i = 0\n",
    "for article_cnn, summary_cnn in zip(articles_cnn, summaries_cnn):\n",
    "    article_cnn_preprocessed = preprocessing(article_cnn)\n",
    "    rouge_1_nn, rouge_2_nn, rouge_l_nn, rouge_1_tf_idf, rouge_2_tf_idf, rouge_l_tf_idf = test(\n",
    "        article_cnn_preprocessed, article_cnn, summary_cnn, 0.35, i)\n",
    "    i += 1\n",
    "    if (i == 300):\n",
    "        break\n",
    "    \n",
    "    rouge_1_list_nn.append(rouge_1_nn)\n",
    "    rouge_2_list_nn.append(rouge_2_nn)\n",
    "    rouge_l_list_nn.append(rouge_l_nn)\n",
    "\n",
    "    rouge_1_list_tf_idf.append(rouge_1_tf_idf)\n",
    "    rouge_2_list_tf_idf.append(rouge_2_tf_idf)\n",
    "    rouge_l_list_tf_idf.append(rouge_l_tf_idf)\n",
    "\n",
    "print('Using nn')\n",
    "print('Average Rouge 1 score is: %f' % (np.average(rouge_1_list_nn)))\n",
    "print('Average Rouge 2 score is: %f' % (np.average(rouge_2_list_nn)))\n",
    "print('Average Rouge l score is: %f' % (np.average(rouge_l_list_nn)))\n",
    "p\n",
    "\n",
    "print('Using tf_idf only')\n",
    "print('Average Rouge 1 score is: %f' % (np.average(rouge_1_list_tf_idf)))\n",
    "print('Average Rouge 2 score is: %f' % (np.average(rouge_2_list_tf_idf)))\n",
    "print('Average Rouge l score is: %f' % (np.average(rouge_l_list_tf_idf)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf2bc8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextToSummarize(article, compression_ratio):\n",
    "    original_sentences = sent_tokenize(article)\n",
    "    article_preprocessed_entered = preprocessing(article)\n",
    "    X_test_entered = generate_X_labels(article_preprocessed_entered)\n",
    "    summary_predicted = predict(model, X_test_entered)\n",
    "    num_sentences_summarized = math.ceil(compression_ratio * len(original_sentences))\n",
    "    \n",
    "    \n",
    "    highest = np.argsort(summary_predicted[0]) [::-1]\n",
    "    highest = highest[: num_sentences_summarized]\n",
    "    highest = sorted(highest) # uncomment to arrange the article\n",
    "    output_sentences = []\n",
    "    \n",
    "\n",
    "    \n",
    "    for i in range (0, num_sentences_summarized):\n",
    "        output_sentences.append(original_sentences[highest[i]])\n",
    "        \n",
    "    output_sentences = ''.join(output_sentences)\n",
    "    \n",
    "    return output_sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a19c6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Madrid forward Cristiano Ronaldo has said that he is the \"best player in history\" after winning his fifth Ballon d'Or on Thursday.Ronaldo picked up the award for the second year in a row to equal the record of Barcelona star Lionel Messi, and he said he does not believe any player is better than him.He told France Football (h/t Goal's Robin Bairner): \"I've never seen anyone better than me.\"There’s no player more complete than me.\"No one has won as many individual trophies as me.It’s the sum of many things.\n"
     ]
    }
   ],
   "source": [
    "article_file = io.open(\"cr7.txt\", \"r\", encoding='utf-8-sig')\n",
    "article = article_file.read()\n",
    "article_file.close\n",
    "# print(article)\n",
    "summary = TextToSummarize(article, 0.35)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d649d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
