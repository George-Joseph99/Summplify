{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc3b0dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/Users/apple/opt/anaconda3/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4)/charset_normalizer (2.1.1) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk\n",
    "import io\n",
    "import csv\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import nltk\n",
    "import wikiwords\n",
    "import argparse\n",
    "import re\n",
    "import requests\n",
    "import pattern\n",
    "import syllables\n",
    "import json\n",
    "import enchant\n",
    "import gensim \n",
    "import random\n",
    "import sys\n",
    "import gensim.downloader as api\n",
    "import wiki_dump_parser as parser\n",
    "import xml.etree.ElementTree as Xet\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from pattern.en import parse, tag, lexeme, lemma\n",
    "from textblob import Word\n",
    "from nltk.corpus import brown\n",
    "from difflib import SequenceMatcher\n",
    "from textblob import TextBlob\n",
    "from autocorrect import Speller\n",
    "from enchant.checker import SpellChecker\n",
    "from array import array\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from wordfreq import zipf_frequency\n",
    "from wikiwords import occdb\n",
    "from nltk.corpus import wordnet\n",
    "from py_thesaurus import Thesaurus\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from itertools import islice\n",
    "from pattern.text.en import pluralize, singularize, comparative, superlative, conjugate\n",
    "from pattern.text.en import tenses, INFINITIVE, PRESENT, PAST, FUTURE\n",
    "from google_ngram_api.Downloader import Downloader\n",
    "from gensim.corpora import WikiCorpus\n",
    "from wiki_dump_reader import Cleaner, iterate\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from colorama import Fore, Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1462fd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca442921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d28809e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_corpus = brown.words()\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "fourgram_measures = nltk.collocations.QuadgramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1386bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_freq_dict = {}\n",
    "lexicon_dict = {}\n",
    "complex_words = []\n",
    "BIGHUGE_KEY = \"105a58f9d880af14af1ca1abf6b1f996\"\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-300\")\n",
    "ppdb_filepath = \"ppdb-2.0-m-lexical\"\n",
    "three_gram_filepath = \"wp_3gram.txt\"\n",
    "dump_xml_path = 'enwiki-latest-abstract.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcf4541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eff992e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_similar(word1, word2):\n",
    "    similarity_ratio = SequenceMatcher(None, word1, word2).ratio()\n",
    "    return similarity_ratio >= 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04d974af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used for flattening list of lists to a single list\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2de4db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateWikiFreqDict(text_file):\n",
    "    wiki_freq_dict = {}\n",
    "    with open(text_file, encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            (word, freq) = line.split()\n",
    "            wiki_freq_dict[word.lower()] = freq\n",
    "    return wiki_freq_dict\n",
    "\n",
    "wiki_freq_dict = generateWikiFreqDict(\"wiki_frequencies.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2e5031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateEnglishFreqDict(text_file):\n",
    "    english_freq_dict = {}\n",
    "    with open(text_file) as f:\n",
    "        file = csv.reader(f, delimiter=\"\\t\")\n",
    "        for line in file:\n",
    "            word_freq_list = line[0].split(',') \n",
    "            if(word_freq_list[1]=='count'): # Skip first line\n",
    "                continue\n",
    "            word = word_freq_list[0]\n",
    "            freq = word_freq_list[1]\n",
    "            english_freq_dict[word] = int(freq)\n",
    "    return english_freq_dict\n",
    "\n",
    "english_freq_dict = generateEnglishFreqDict('ngram_freq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ebb1650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateLexiconDict(tsv_file):\n",
    "    lexicon_dict = {}\n",
    "    with open(tsv_file) as f:\n",
    "        file = csv.reader(f, delimiter=\"\\t\")\n",
    "        for line in file:\n",
    "            word = line[0]\n",
    "            score = line[1]\n",
    "            lexicon_dict[word.lower()] = score\n",
    "    return lexicon_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "078c396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_dict = generateLexiconDict('lexicon.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f8072d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_special_characters(string):\n",
    "    pattern = r'[^\\w\\s]'\n",
    "    if re.search(pattern, string):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7204c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_number(string):\n",
    "    pattern = r'\\d'\n",
    "    if re.search(pattern, string):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec4e410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LS step1: Complex Word Identification\n",
    "\n",
    "def word_preceding(text, word):\n",
    "    words = []\n",
    "    word_list = word_tokenize(text)\n",
    "    words.extend(word_list)\n",
    "    index = word_list.index(word)\n",
    "    return word_list[index-1] if index > 0 else None\n",
    "\n",
    "def word_following(text, word):\n",
    "    words = []\n",
    "    word_list = word_tokenize(text)\n",
    "    words.extend(word_list)\n",
    "    index = word_list.index(word)\n",
    "    return word_list[index+1] if index+1 < len(word_list) else None\n",
    "\n",
    "def complexWordIdentif(article):\n",
    "    threshold_scores_dict = {}\n",
    "    word_sentence_dict = {}\n",
    "    for sentence in sent_tokenize(article):\n",
    "        for word_ in word_tokenize(sentence):\n",
    "            word = word_.lower()\n",
    "            index = word.find(\"'\")\n",
    "            word_split_hyphen = word_.split(\"-\")\n",
    "            word_split_underscore = word_.split(\"_\")\n",
    "            if(len(word_split_hyphen)>1):\n",
    "                total_freq = 0\n",
    "                total_lexicon = 0\n",
    "                for word_hyph in word_split_hyphen:\n",
    "                    total_freq+=int(wiki_freq_dict.get(word_hyph,0))\n",
    "                    total_lexicon+=float(lexicon_dict.get(word_hyph,0))\n",
    "                wiki_freq = int(total_freq/len(word_split_hyphen))\n",
    "                lexicon_score = float(total_lexicon/len(word_split_hyphen))\n",
    "                if(wiki_freq<12000 or lexicon_score>3.0):\n",
    "                    threshold_scores_dict[word_]=1\n",
    "                    word_sentence_dict[word_] = sentence\n",
    "                    \n",
    "            elif(len(word_split_underscore)>1):\n",
    "                total_freq = 0\n",
    "                total_lexicon = 0\n",
    "                for word_un in word_split_underscore:\n",
    "                    total_freq+=int(wiki_freq_dict.get(word_un,0))\n",
    "                    total_lexicon+=float(lexicon_dict.get(word_un,0))\n",
    "                wiki_freq = int(total_freq/len(word_split_underscore))\n",
    "                lexicon_score = float(total_lexicon/len(word_split_underscore))\n",
    "                if(wiki_freq<12000 or lexicon_score>3.0):\n",
    "                    threshold_scores_dict[word_]=1\n",
    "                    word_sentence_dict[word_] = sentence\n",
    "            elif(has_special_characters(word) or has_number(word)):\n",
    "                continue\n",
    "            else:\n",
    "                threshold_scores_dict[word]=0\n",
    "                if(word in wiki_freq_dict and word in lexicon_dict):\n",
    "                    wiki_freq = int(wiki_freq_dict[word])\n",
    "                    lexicon_score = float(lexicon_dict[word])\n",
    "                    if(wiki_freq<12000 or lexicon_score>3.0):\n",
    "                        threshold_scores_dict[word_]=1\n",
    "                        word_sentence_dict[word_] = sentence\n",
    "                elif(word in wiki_freq_dict):\n",
    "                    wiki_freq = int(wiki_freq_dict[word])\n",
    "                    if(wiki_freq<12000):\n",
    "                        threshold_scores_dict[word_]=1\n",
    "                        word_sentence_dict[word_] = sentence\n",
    "                elif(word in lexicon_dict):\n",
    "                    lexicon_score = float(lexicon_dict[word])\n",
    "                    if(lexicon_score>3.0):\n",
    "                        threshold_scores_dict[word_]=1\n",
    "                        word_sentence_dict[word_] = sentence\n",
    "                else:\n",
    "                    threshold_scores_dict[word_]=1\n",
    "                    word_sentence_dict[word_] = sentence\n",
    "    return threshold_scores_dict, word_sentence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9329f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pos tag of a certain word \n",
    "\n",
    "def getPosTag(word):\n",
    "    tag = nltk.pos_tag([word])\n",
    "    return tag[0][1]\n",
    "\n",
    "def getPosTagFromSentence(string, target_word):\n",
    "    pos_tag=\"\"\n",
    "    tokens = nltk.word_tokenize(string.lower())\n",
    "    tag = nltk.pos_tag(tokens)\n",
    "    for pair in tag:\n",
    "        if(pair[0]==target_word.lower()):\n",
    "            pos_tag = pair[1]\n",
    "    return pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56f78c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used with word net dictionary\n",
    "\n",
    "def getTypeFromTag(tag):\n",
    "    # Convert all forms of noun tags to \"n\" noun type\n",
    "    if(tag==\"NN\" or tag==\"NNS\" or tag==\"NNP\" or tag==\"NNPS\"):\n",
    "        return 'n'\n",
    "    # Convert all forms of adjective tags to \"a\" adjective type\n",
    "    elif(tag==\"JJ\" or tag==\"JJR\" or tag==\"JJS\"):\n",
    "        return 'a'\n",
    "    # Convert all forms of verb tags to \"v\" verb type\n",
    "    elif(tag==\"VBZ\" or tag==\"VB\" or tag==\"VBP\" or tag==\"VBN\" or tag==\"VBG\" or tag==\"VBD\"):\n",
    "        return 'v'\n",
    "    # Convert all forms of adverb tags to \"r\" adverb type\n",
    "    elif(tag==\"RBS\" or tag==\"RB\" or tag==\"RBR\"):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5e1d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Net Synonyms\n",
    "\n",
    "def getSynWordNet(complex_word):\n",
    "    synonyms = []\n",
    "    for synset in wordnet.synsets(complex_word):\n",
    "            for l in synset.lemmas():\n",
    "                synonyms.append(l.name())\n",
    "    return list(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "160e25ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Net Synonyms with specific pos tag\n",
    "\n",
    "def getSynWordNetSpec(complex_word, pos_tag):\n",
    "    synonyms = []\n",
    "    for synset in wordnet.synsets(complex_word):\n",
    "        if(synset.pos()==pos_tag):\n",
    "            for l in synset.lemmas():\n",
    "                synonyms.append(l.name())\n",
    "    return list(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5408db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigHuge thesaurus\n",
    "\n",
    "def getSynBigHuge(complex_word):\n",
    "    bighuge_synonyms = []\n",
    "    r = requests.get(url='http://words.bighugelabs.com/api/2/'+BIGHUGE_KEY+'/'+complex_word+'/json')  \n",
    "    if(r.status_code!=404 and r.status_code!=500):\n",
    "#         print(type(r.json()),\"\\n\",r.json())\n",
    "        if(type(r.json()) is dict):\n",
    "            synonym_dict = r.json()\n",
    "            for key in synonym_dict: # key may be: noun/verb/adjective/adverb\n",
    "                synonym_list = synonym_dict[key].get(\"syn\")\n",
    "                if(synonym_list):\n",
    "                    bighuge_synonyms.append(synonym_list)\n",
    "            flatList = [element for innerList in bighuge_synonyms for element in innerList] # Convert it to a single list\n",
    "            return flatList,synonym_dict \n",
    "        else:\n",
    "            return r.json(),{}\n",
    "    else:\n",
    "        return [],{}\n",
    "            \n",
    "\n",
    "# May not use this function to avoid doing multiple requests for the same word\n",
    "# Instead, use the aboive one and make one request per word, then search for the specific pos tag inside the returned dict\n",
    "def getSynBigHugeSpec(complex_word, pos_tag):\n",
    "    bighuge_synonyms = []\n",
    "    r = requests.get(url='http://words.bighugelabs.com/api/2/'+BIGHUGE_KEY+'/'+complex_word+'/json') \n",
    "    if(r.status_code!=404):\n",
    "        if(type(r.json()) is dict):\n",
    "            synonym_dict = r.json()\n",
    "            if(pos_tag in synonym_dict):\n",
    "                bighuge_synonyms = synonym_dict[pos_tag].get(\"syn\")\n",
    "        else:\n",
    "            bighuge_synonyms = r.json()\n",
    "    return bighuge_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41406408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenOffice thesaurus\n",
    "\n",
    "# This generates a dictionary \"thesaurus\" with (word,pos) as the key and a set of synonyms as the value\n",
    "# For example:\n",
    "# thesaurus[(\"happy\",\"adj\")] = {'glad', 'pleased', 'prosperous', 'cheerful', ... }\n",
    "# possible pos: noun,verb,adj,adv\n",
    "def generateTheSaurusDict():\n",
    "    thesaurus = {}\n",
    "    with open(\"th_en_US_new.dat\") as f:\n",
    "        code = f.readline()    # Skip the file encoding\n",
    "        while(True):\n",
    "            word_count_line = f.readline()\n",
    "            if(word_count_line == \"\"):\n",
    "                break\n",
    "            (word,count) = word_count_line.split('|')\n",
    "            for i in range(0,int(count)):\n",
    "                pos_synonyms = f.readline().split(\"|\")\n",
    "                synonyms_list = pos_synonyms[1:]\n",
    "                pos = re.sub(r\"[\\([{})\\]]\", \"\",pos_synonyms[0]) # Remove te brackest surronding the pos (noun - verb - adv - adj)\n",
    "                synonyms_list = [synonym.strip() for synonym in synonyms_list] # Remove unnecessary spaces\n",
    "                if((word,pos) not in thesaurus):\n",
    "                    thesaurus[(word,pos)]=set()\n",
    "                for synonym in synonyms_list:\n",
    "                    if(synonym not in thesaurus[(word,pos)] and synonym!=word):\n",
    "                        thesaurus[(word,pos)].add(synonym)\n",
    "    return thesaurus\n",
    "\n",
    "# thesaurus = generateTheSaurusDict()\n",
    "# print(thesaurus[\"happy\",\"adj\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8596d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPDB dictionary\n",
    "\n",
    "def genPPDBdict(file_path):\n",
    "    ppdb_dict = {}\n",
    "    with open(file_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            line = row[0]\n",
    "            word_list = line.split(\"|||\")\n",
    "            source = word_list[1].strip()\n",
    "            target = word_list[2].strip()\n",
    "            score_list = word_list[3].split()\n",
    "            if source in ppdb_dict:\n",
    "                ppdb_dict[source].append(target)\n",
    "            else:\n",
    "                ppdb_dict[source] = [target]\n",
    "        return ppdb_dict\n",
    "# with open(\"ppdb-2.0-tldr\") as f:\n",
    "#         dic = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "289d90af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LS step2: Substitution Generation\n",
    "\n",
    "thesaurus = generateTheSaurusDict()\n",
    "ppdb = genPPDBdict(ppdb_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "385147cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genSubstitution(complex_word):\n",
    "    thesaurus_candidates = []\n",
    "    wordnet_candidates = []\n",
    "    wordnet_candidates = getSynWordNet(complex_word)\n",
    "    for key in thesaurus:\n",
    "        if(key[0] == complex_word):\n",
    "            thesaurus_candidates.append(thesaurus.get((key[0], key[1])))\n",
    "    return wordnet_candidates, list(thesaurus_candidates)\n",
    "    \n",
    "def genSubstitutionSpec(complex_word, sentence):\n",
    "    thesaurus_candidates = []\n",
    "    bighuge_candidates = []\n",
    "    wordnet_candidates = []\n",
    "    ppdb_candidates = []\n",
    "    pos_tag = getPosTagFromSentence(sentence, complex_word)\n",
    "    word_type = getTypeFromTag(pos_tag)\n",
    "    wordnet_candidates = getSynWordNetSpec(complex_word.lower(), word_type)\n",
    "    flat_syn_list, bighuge_dict = [],{}\n",
    "#     flat_syn_list, bighuge_dict = getSynBigHuge(complex_word)\n",
    "    if(complex_word.lower() in ppdb):\n",
    "        ppdb_candidates = ppdb[complex_word.lower()]\n",
    "    if(flat_syn_list and not bighuge_dict):\n",
    "        bighuge_candidates = flat_syn_list\n",
    "    if(word_type == 'n'):\n",
    "        thesaurus_candidates = thesaurus.get((complex_word.lower(),\"noun\"))\n",
    "        if(\"noun\" in bighuge_candidates):\n",
    "            bighuge_candidates = bighuge_dict[\"noun\"].get(\"syn\")\n",
    "    elif(word_type == 'r'):\n",
    "        thesaurus_candidates = thesaurus.get((complex_word.lower(),\"adv\"))\n",
    "        if(\"adverb\" in bighuge_candidates):\n",
    "            bighuge_candidates = bighuge_dict[\"adverb\"].get(\"syn\")\n",
    "    elif(word_type == 'v'):\n",
    "        thesaurus_candidates = thesaurus.get((complex_word.lower(),\"verb\"))\n",
    "        if(\"verb\" in bighuge_candidates):\n",
    "            bighuge_candidates = bighuge_dict[\"verb\"].get(\"syn\") \n",
    "    elif(word_type == 'a'):\n",
    "        thesaurus_candidates = thesaurus.get((complex_word.lower(),\"adj\"))\n",
    "        if(\"adjective\" in bighuge_candidates):\n",
    "            bighuge_candidates = bighuge_dict[\"adjective\"].get(\"syn\")\n",
    "\n",
    "    return wordnet_candidates, thesaurus_candidates, bighuge_candidates, ppdb_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "207e9d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterSubstitutions(word_subs_dict):\n",
    "    word_subs_dict_filtered = {}\n",
    "    wnl = WordNetLemmatizer()\n",
    "    ps = PorterStemmer()\n",
    "    for word in word_subs_dict:\n",
    "        cosine_sim = 0\n",
    "        word_lemm = wnl.lemmatize(word.lower())\n",
    "        filtered_subs_list = []\n",
    "        filtered_subs_score = []\n",
    "        subs_set = word_subs_dict[word]\n",
    "        if(len(subs_set)>0):\n",
    "            for subs in subs_set:\n",
    "                if(subs.lower() == word.lower()):\n",
    "                    continue\n",
    "                if(word.lower() in word_vectors and subs.lower() in word_vectors):     \n",
    "                    target_word_vector = word_vectors[word.lower()]\n",
    "                    substitution_vector = word_vectors[subs.lower()]\n",
    "                    cosine_sim = getCosSim(target_word_vector,substitution_vector)\n",
    "                word_list = nltk.word_tokenize(subs)\n",
    "                word_split_hyphen = subs.split(\"-\")\n",
    "                word_split_underscore = subs.split(\"_\")\n",
    "                if(len(word_list) > 1):\n",
    "                    add_word = True\n",
    "                    for word_ in word_list:\n",
    "                        word_lemm = wnl.lemmatize(word.lower())\n",
    "                        subs_lemm = wnl.lemmatize(word_.lower())\n",
    "                        if(ps.stem(word_.lower())==ps.stem(word.lower()) or subs_lemm==word_lemm or is_similar(word_.lower(), word.lower()) or is_similar(word_.lower(), word_lemm) \n",
    "                          or is_similar(subs_lemm, word_lemm) or is_similar(subs_lemm, word.lower())):\n",
    "                            add_word = False\n",
    "                            break\n",
    "                    if(add_word):\n",
    "                        filtered_subs_list.append(subs)\n",
    "                        filtered_subs_score.append(cosine_sim)\n",
    "                        \n",
    "                elif(len(word_split_hyphen)>1):\n",
    "                    add_word = True\n",
    "                    for word_ in word_split_hyphen:\n",
    "                        word_lemm = wnl.lemmatize(word.lower())\n",
    "                        subs_lemm = wnl.lemmatize(word_.lower())\n",
    "                        if(ps.stem(word_.lower())==ps.stem(word.lower()) or subs_lemm==word_lemm or is_similar(word_.lower(), word.lower()) or is_similar(word_.lower(), word_lemm) \n",
    "                          or is_similar(subs_lemm, word_lemm) or is_similar(subs_lemm, word.lower())):\n",
    "                            add_word = False\n",
    "                            break\n",
    "                    if(add_word):\n",
    "                        filtered_subs_list.append(subs)\n",
    "                        filtered_subs_score.append(cosine_sim)\n",
    "                elif(len(word_split_underscore)>1):\n",
    "                    add_word = True\n",
    "                    for word_ in word_split_underscore:\n",
    "                        word_lemm = wnl.lemmatize(word.lower())\n",
    "                        subs_lemm = wnl.lemmatize(word_.lower())\n",
    "                        if(ps.stem(word_.lower())==ps.stem(word.lower()) or subs_lemm==word_lemm or is_similar(word_.lower(), word.lower()) or is_similar(word_.lower(), word_lemm)\n",
    "                          or is_similar(subs_lemm, word_lemm) or is_similar(subs_lemm, word.lower())):\n",
    "                            add_word = False\n",
    "                            break\n",
    "                    if(add_word):\n",
    "                        filtered_subs_list.append(subs)\n",
    "                        filtered_subs_score.append(cosine_sim)\n",
    "                else: \n",
    "                    subs_lemm = wnl.lemmatize(subs.lower())\n",
    "                    if(ps.stem(subs.lower())!=ps.stem(word.lower()) and subs_lemm!=word_lemm and not is_similar(subs.lower(), word.lower()) and not is_similar(subs.lower(), word_lemm)\n",
    "                      and not is_similar(subs_lemm, word_lemm) and not is_similar(subs_lemm, word.lower())):\n",
    "                        filtered_subs_list.append(subs)\n",
    "                        filtered_subs_score.append(cosine_sim)\n",
    "        \n",
    "        if(len(filtered_subs_score)<10):\n",
    "            new_list = []\n",
    "            list_len = len(filtered_subs_score)\n",
    "            for i in range(list_len):\n",
    "                max_index = filtered_subs_score.index(max(filtered_subs_score))\n",
    "                new_list.append(filtered_subs_list[max_index])\n",
    "                filtered_subs_score[max_index] = -1\n",
    "        else:\n",
    "            # Get the 10 highest values\n",
    "            new_list = []\n",
    "            for i in range(10):\n",
    "                max_index = filtered_subs_score.index(max(filtered_subs_score))\n",
    "                new_list.append(filtered_subs_list[max_index])\n",
    "                # Remove the maximum value from the list\n",
    "                filtered_subs_score[max_index] = -1\n",
    "        word_subs_dict_filtered[word] = new_list\n",
    "    return word_subs_dict_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7a3b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertGrammStructure(word_subs_dict, word_sentence_dict):\n",
    "    modified_word_subs_dict = word_subs_dict.copy()\n",
    "#     for word in word_subs_dict:\n",
    "#         subs_list = word_subs_dict[word]\n",
    "#         word_tag = getPosTagFromSentence(word_sentence_dict[word],word)\n",
    "#         word_type = getTypeFromTag(word_tag)\n",
    "#         if(word_type == 'n'): #NOUNS\n",
    "#             modified_subs = []\n",
    "#             for subs in subs_list:\n",
    "#                 subs_tag = getPosTag(subs)\n",
    "#                 subs_type = getTypeFromTag(subs_tag)\n",
    "#                 if(word_tag==\"NN\" and subs_tag==\"NNS\"):\n",
    "#                     new_subs = singularize(subs)\n",
    "#                     modified_subs.append(new_subs)\n",
    "#                 elif(word_tag==\"NNS\" and subs_tag==\"NN\"):\n",
    "#                     new_subs = pluralize(subs)\n",
    "#                     modified_subs.append(new_subs)\n",
    "#                 elif((word_tag==\"NNS\" and subs_tag==\"NNS\") or (word_tag==\"NN\" and subs_tag==\"NN\")):\n",
    "#                     modified_subs.append(subs)\n",
    "#             modified_word_subs_dict[word] = modified_subs\n",
    "#         elif(word_type == 'a'): #ADJECTIVES\n",
    "#             modified_subs = []\n",
    "#             for subs in subs_list:\n",
    "#                 subs_tag = getPosTag(subs)\n",
    "#                 subs_type = getTypeFromTag(subs_tag)\n",
    "#                 if(word_tag==\"JJR\" and (subs_tag==\"JJS\" or subs_tag==\"JJ\")):\n",
    "#                     new_subs = comparative(subs)\n",
    "#                     modified_subs.append(new_subs)\n",
    "#                 elif(word_tag==\"JJS\" and (subs_tag==\"JJR\" or subs_tag==\"JJ\")):\n",
    "#                     new_subs = superlative(subs)\n",
    "#                     modified_subs.append(new_subs)\n",
    "#                 elif((word_tag==\"JJR\" and subs_tag==\"JJR\") or (word_tag==\"JJS\" and subs_tag==\"JJS\") or (word_tag==\"JJ\" and subs_tag==\"JJ\")):\n",
    "#                     modified_subs.append(subs)\n",
    "#             modified_word_subs_dict[word] = modified_subs\n",
    "#         elif(word_type == 'v'): #VERBS\n",
    "#             v_tense = None\n",
    "#             modified_subs = []\n",
    "#             if(word):\n",
    "#                 parsed = parse(word)\n",
    "#                 if(len(parsed.split()[0])>0):\n",
    "#                     for p in parsed.split()[0]:\n",
    "#                         if(len(p)>0):\n",
    "#                             if word_tag == 'VBD':\n",
    "#                                 v_tense = 'past'\n",
    "#                             elif word_tag == 'VBN':\n",
    "#                                 v_tense = 'past'\n",
    "#                             elif word_tag == 'VBG':\n",
    "#                                 v_tense = 'present participle'\n",
    "#                             elif word_tag == 'VBZ':\n",
    "#                                 v_tense = 'present'\n",
    "#                             elif word_tag == 'VBP':\n",
    "#                                 v_tense = 'present'\n",
    "#                             elif word_tag == 'MD':\n",
    "#                                 v_tense = 'future'\n",
    "#                             elif word_tag == 'VB':\n",
    "#                                 if 'VBG' in [t[1] for t in tag(p[0])]:\n",
    "#                                     v_tense = 'present participle'\n",
    "#                                 elif 'VBN'in [t[1] for t in tag(p[0])]:\n",
    "#                                     v_tense = 'past participle'\n",
    "#                                 else:\n",
    "#                                     v_tense = 'infinitive'\n",
    "#                 for subs in subs_list: \n",
    "#                         subs_tag = getPosTag(subs)\n",
    "#                         if(word_tag == 'VBG'):\n",
    "#                             new_subs = lemma(subs) + 'ing'\n",
    "#                             modified_subs.append(new_subs) \n",
    "#                         elif(v_tense and len(tenses(word))>0):\n",
    "#                             new_subs = conjugate(subs, v_tense)\n",
    "# #                             print(word, word_tag, subs, v_tense, new_subs)\n",
    "#                             if(new_subs):\n",
    "#                                 modified_subs.append(new_subs)   \n",
    "#                             else:\n",
    "#                                 modified_subs.append(subs)  \n",
    "#                         else:\n",
    "#                             modified_subs.append(subs) \n",
    "#                 modified_word_subs_dict[word] = modified_subs   \n",
    "    return modified_word_subs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d71bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LS Step3: Substitution Ranking\n",
    "\n",
    "def getThreeGramFreq(phrase):\n",
    "    freq = 0\n",
    "    with open(\"three_gram.txt\", \"r\") as infile:\n",
    "        reader = csv.reader(infile, delimiter=\"\\t\")\n",
    "        for i, row in enumerate(reader):\n",
    "            if row[0] == phrase:\n",
    "                freq = int(row[1])\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d291becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate3GramDict(filepath):\n",
    "    three_gram_dict = {}\n",
    "    max_val = 0\n",
    "    \n",
    "    with open(filepath, newline='', encoding='utf-8') as f:\n",
    "        file = csv.reader(f, delimiter=\"\\t\")\n",
    "        for line in file:\n",
    "            if(len(line)>=2):\n",
    "                phrase = line[0]\n",
    "                freq = line[2]\n",
    "                if(phrase.lower() not in three_gram_dict):\n",
    "                    three_gram_dict[phrase.lower()] = int(freq)\n",
    "                else:\n",
    "                    three_gram_dict[phrase.lower()] += int(freq)\n",
    "    \n",
    "    # Filter the dict\n",
    "    keys_tobe_removed = []\n",
    "    for key in three_gram_dict:\n",
    "        word_list = key.split()\n",
    "        freq = three_gram_dict[key]\n",
    "        if(freq>max_val):\n",
    "            max_val = freq\n",
    "        if(freq<=50 or len(word_list)>3):\n",
    "            keys_tobe_removed.append(key)\n",
    "            \n",
    "    for key in keys_tobe_removed:\n",
    "        three_gram_dict.pop(key, None)\n",
    "        \n",
    "    return three_gram_dict\n",
    "                \n",
    "three_gram_dict_google = generate3GramDict('googlebooks-eng-all-3gram-20090715-0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10b1d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNgramScore(phrase, start_year=2018, end_year=2019, corpus=26, smoothing=3):\n",
    "    avg_score = 0\n",
    "    google_ngram_url = \"https://books.google.com/ngrams/json?content=\"+phrase+'&year_start=' + str(start_year) + '&year_end=' + str(end_year) + '&corpus=' + str(corpus) + '&smoothing=' + str(smoothing)\n",
    "    response = requests.get(google_ngram_url) # Commenting this line just for now\n",
    "    if(response):\n",
    "        output = response.json()\n",
    "        scores_list = []\n",
    "        if(len(output) > 0):\n",
    "            scores_list = output[0]['timeseries']\n",
    "            if(len(scores_list) > 1):\n",
    "                avg_score = (scores_list[0] + scores_list[1])/2\n",
    "    return avg_score\n",
    "            \n",
    "def extractFeaturesFromWord(target_word, word_phrase_dict):\n",
    "    \n",
    "    # Features we have are:\n",
    "    # lex_exist_flag, complexity_score, word_length, syllable_count, wiki_freq, ngram_score\n",
    "    lex_exist_flag = -1\n",
    "    complexity_score = -1\n",
    "    word_length = -1\n",
    "    syllable_count = -1\n",
    "    wiki_freq = -1\n",
    "    ngram_score = -1\n",
    "    brown_freq = -1\n",
    "    eng_word_freq = -1\n",
    "    sem_ratio = -1\n",
    "\n",
    "    \n",
    "    # Before extracting features, check if it's a multi-word phrase, if so, we work on the longest word\n",
    "    longest_word = ''\n",
    "    word_list = nltk.word_tokenize(target_word)\n",
    "    word_split_hyphen = target_word.split(\"-\")\n",
    "    word_split_underscore = target_word.split(\"_\")\n",
    "    if(len(word_list) > 1):\n",
    "        longest_word = ''\n",
    "        max_length = 0\n",
    "        for word in word_list:\n",
    "            if(len(word)>max_length):\n",
    "                max_length=len(word)\n",
    "                longest_word = word\n",
    "    elif(len(word_split_hyphen) > 1):\n",
    "        longest_word = ''\n",
    "        max_length = 0\n",
    "        for word in word_split_hyphen:\n",
    "            if(len(word)>max_length):\n",
    "                max_length=len(word)\n",
    "                longest_word = word\n",
    "    elif(len(word_split_underscore) > 1):\n",
    "        longest_word = ''\n",
    "        max_length = 0\n",
    "        for word in word_split_underscore:\n",
    "            if(len(word)>max_length):\n",
    "                max_length=len(word)\n",
    "                longest_word = word\n",
    "        \n",
    "    # EXTRACTING FEATURES\n",
    "    \n",
    "    # Feature 1: Binary number representing word's presence in lexicon (1:existent 0:non existent)\n",
    "#     if(target_word in lexicon_dict):\n",
    "#         lex_exist_flag = 1\n",
    "#     else:\n",
    "#         lex_exist_flag = 0\n",
    "            \n",
    "    # Feature 2: Complexity score of the word in the lexicon\n",
    "    if(target_word in lexicon_dict):\n",
    "        complexity_score = float(lexicon_dict[target_word])\n",
    "    else:\n",
    "        complexity_score = 0 # If word is not found in lexicon, set its complexity score with 0 also\n",
    "      \n",
    "    # Feature 3: word length (character count)\n",
    "    word_length = len(target_word)\n",
    "    \n",
    "    # Feature 4: Syllable count\n",
    "    syllable_count = syllables.estimate(target_word)     \n",
    "    \n",
    "    # Feature 5: Frequency with respect to Wiki-Frequency\n",
    "    if(target_word.lower() in wiki_freq_dict):\n",
    "        wiki_freq = int(wiki_freq_dict[target_word.lower()])\n",
    "    else:\n",
    "        wiki_freq = 0\n",
    "        \n",
    "    # Feature 6: Google Ngram average score\n",
    "    if(target_word in word_phrase_dict):\n",
    "        three_words = word_phrase_dict[target_word]\n",
    "        phrase = three_words[0] + \" \" + three_words[1] + \" \" + three_words[2]\n",
    "#         ngram_score = three_gram_dict_google.get(phrase,0)\n",
    "#         ngram_score = getNgramScore(phrase)*pow(10,9)\n",
    "#         ngram_score = getThreeGramFreq(phrase)\n",
    "    else:\n",
    "        ngram_score = 0\n",
    "        \n",
    "    # Feature 7: Frequency with respect to English Corpus\n",
    "    eng_word_freq = english_freq_dict.get(target_word, 0)\n",
    "#     brown_freq = brown_corpus.count(target_word)\n",
    "    \n",
    "    return [complexity_score, word_length, syllable_count, wiki_freq, eng_word_freq] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff2ad373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair wise features\n",
    "\n",
    "def getCosSim(vec_1, vec_2):\n",
    "    cos_sim = cosine_similarity([vec_1], [vec_2])\n",
    "    return cos_sim[0][0]\n",
    "\n",
    "def getJaccSim(word1, word2): \n",
    "    vectorizer = CountVectorizer()\n",
    "    corpus = [word1, word2]\n",
    "    bag_of_words = vectorizer.fit_transform(corpus)\n",
    "    word1_vector = bag_of_words[0].toarray()[0]\n",
    "    word2_vector = bag_of_words[1].toarray()[0]\n",
    "    intersection = sum([1 for i, j in zip(word1_vector, word2_vector) if i == j and i == 1])\n",
    "    union = sum([1 for i, j in zip(word1_vector, word2_vector) if i == 1 or j == 1])\n",
    "    similarity = intersection / union\n",
    "    return similarity\n",
    "    \n",
    "def similarityRatio(word1, word2):\n",
    "    similarity_ratio = SequenceMatcher(None, word1, word2).ratio()\n",
    "    return similarity_ratio \n",
    "\n",
    "def semanticSimilarityRatio(word1, word2):\n",
    "    word1_synsets = wordnet.synsets(word1)\n",
    "    word2_synsets = wordnet.synsets(word2)\n",
    "    max_similarity = 0\n",
    "    for word1_synset in word1_synsets:\n",
    "        for word2_synset in word2_synsets:\n",
    "            similarity = word1_synset.wup_similarity(word2_synset)\n",
    "            if similarity is not None and similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "    return max_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe89faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # TODO 1: Compute the sigmoid function at the given x (~1 line)\n",
    "    # For example: sigmoid(2) should compute the value of sigmoid function at x = 2.\n",
    "    # Hint: Use np.exp instead of math.exp to allow for vectorization.\n",
    "    #----------------------------------------------------------------------------------------------\n",
    "    sig = (1/(1+np.exp(-x)))\n",
    "    #----------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return sig\n",
    "\n",
    "def batch_normalize(X, eps=1e-5):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    var = np.var(X, axis=0)\n",
    "    X_norm = (X - mean) / np.sqrt(var + eps)\n",
    "    return X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c7dd580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "feature_matrix_ = []\n",
    "cosine_similarities = []\n",
    "similarity_ratios = []\n",
    "sem_similarity_ratios = []\n",
    "candidates_matrix = []\n",
    "sentence_list = []\n",
    "word_candCount_dict = {}\n",
    "word_phrase_dict = {}\n",
    "scores_matrix = []\n",
    "complex_word_list = []\n",
    "target_word = ''\n",
    "X = []\n",
    "num_features = 6\n",
    "num_single_features = 5\n",
    "with open('BenchLS.txt') as f:\n",
    "        data_reader = csv.reader(f, delimiter='\\t')\n",
    "        data = list(data_reader)\n",
    "data = list(data)\n",
    "# print(data)\n",
    "train_count = math.ceil(0.8*len(data))\n",
    "test_count = len(data) - train_count\n",
    "train_data = data[:train_count]\n",
    "test_data = data[train_count:]\n",
    "\n",
    "num_candidates = 0\n",
    "for line in train_data:\n",
    "    candidates = []\n",
    "    scores = []\n",
    "    scores_candidates = line[3:]\n",
    "    sentence = line[0]\n",
    "    sentence_list.append(sentence)\n",
    "    target_word = line[1]\n",
    "    complex_word_list.append(target_word)\n",
    "    num_candidates += len(scores_candidates)\n",
    "    word_candCount_dict[target_word] = len(scores_candidates)\n",
    "    for score_candidate in scores_candidates:\n",
    "        candidates.append(score_candidate[2:])\n",
    "        scores.append(int(score_candidate[0]))\n",
    "    candidates_matrix.append(candidates)\n",
    "    scores_matrix.append(scores)\n",
    "\n",
    "indx = 0\n",
    "for line in candidates_matrix:\n",
    "    sentence = sentence_list[indx]\n",
    "    complex_word = complex_word_list[indx]\n",
    "    current_candidates_features = []\n",
    "    for candidate in line:\n",
    "        prev_word = word_preceding(sentence.lower(), complex_word)\n",
    "        next_word = word_following(sentence.lower(), complex_word)\n",
    "        if(prev_word and next_word):\n",
    "            word_phrase_dict[candidate] = [prev_word, candidate, next_word]\n",
    "        feature_list_ = extractFeaturesFromWord(candidate,word_phrase_dict)\n",
    "        current_candidates_features.append(feature_list_)\n",
    "        if(target_word in word_vectors and candidate in word_vectors):     \n",
    "            target_word_vector = word_vectors[target_word]\n",
    "            substitution_vector = word_vectors[candidate]\n",
    "            cos_similarity = getCosSim(target_word_vector, substitution_vector)\n",
    "        else:\n",
    "            cos_similarity = 0\n",
    "        cosine_similarities.append(cos_similarity)\n",
    "#         similarity_ratios.append(similarityRatio(target_word, candidate))\n",
    "        sem_similarity_ratios.append(semanticSimilarityRatio(target_word.lower(), candidate.lower()))\n",
    "    current_candidates_features = np.array(current_candidates_features).reshape(len(line),num_single_features)\n",
    "    max_in_column = np.max(current_candidates_features,axis=0)\n",
    "    for i in range(num_single_features):\n",
    "        if(max_in_column[i] != 0):\n",
    "            current_candidates_features[:, i] = current_candidates_features[:, i]/max_in_column[i]\n",
    "#     print(current_candidates_features)\n",
    "    feature_matrix_.append(current_candidates_features)\n",
    "#     print(feature_matrix_)\n",
    "    indx+=1\n",
    "\n",
    "feature_matrix_ = np.concatenate(feature_matrix_, axis=0)\n",
    "cosine_similarities = np.array(cosine_similarities).reshape(num_candidates,1)\n",
    "# similarity_ratios = np.array(similarity_ratios).reshape(num_candidates,1)\n",
    "sem_similarity_ratios = np.array(sem_similarity_ratios).reshape(num_candidates,1)\n",
    "\n",
    "\n",
    "X = np.hstack((feature_matrix_,cosine_similarities))\n",
    "\n",
    "Y = np.ones((len(feature_matrix_),1))\n",
    "y_indx = 0\n",
    "max_score = max(flatten(scores_matrix))\n",
    "for line in scores_matrix: \n",
    "    for score in line:\n",
    "        Y[y_indx]=score/max_score\n",
    "        y_indx+=1\n",
    "\n",
    "m = len(X)  # training set size\n",
    "nn_input_dim = 6  # input layer dimensionality (we have seven input features)\n",
    "nn_output_dim = 1  # output layer dimensionality (we have one output :: score)\n",
    "\n",
    "# Gradient descent parameters\n",
    "alpha = 0.1 # learning rate for gradient descent\n",
    "\n",
    "def buildModel(nn_hdim, num_passes=20000, print_loss=False):\n",
    "\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(nn_hdim, nn_input_dim) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((nn_hdim, 1))\n",
    "    W2 = np.random.randn(nn_output_dim, nn_hdim) / np.sqrt(nn_hdim)\n",
    "    b2 = np.zeros((nn_output_dim, 1))\n",
    "    \n",
    "    model = {}\n",
    "\n",
    "    for i in range(0, num_passes):\n",
    "        DW1 = 0\n",
    "        DW2 = 0\n",
    "        Db1 = 0\n",
    "        Db2 = 0\n",
    "        cost = 0\n",
    "\n",
    "        for j in range(0, m):\n",
    "            a0 = X[j, :].reshape(-1, 1) \n",
    "            y = Y[j]\n",
    "            \n",
    "            # Forward propagation\n",
    "            z1 = np.dot(W1 , a0 )+ b1\n",
    "            a1 = np.tanh(z1)\n",
    "            z2 = np.dot(W2 , a1) + b2\n",
    "            a2 = z2\n",
    "\n",
    "            cost_j = abs(y-a2)\n",
    "       \n",
    "            da2 =  ( -y/a2  + (1-y)/(1-a2) )\n",
    "            dz2 =  da2 * a2 * ( 1 - a2)\n",
    "            dW2 = np.dot(dz2 , a1.T)\n",
    "            db2 = dz2\n",
    "\n",
    "            da1 =  np.dot(dz2,W2).T\n",
    "            dz1 = np.multiply(da1 , 1 - np.square(a1) )\n",
    "            dW1 = np.dot(dz1 , a0.T )\n",
    "            db1 = dz1\n",
    "\n",
    "            DW1 += dW1\n",
    "            DW2 += dW2\n",
    "            Db2 += db2\n",
    "            Db1 += db1\n",
    "            cost += cost_j\n",
    "            \n",
    "        # Averaging DW1, DW2, Db1, Db2 and cost over the m training examples. \n",
    "        DW1 /= m\n",
    "        DW2 /= m\n",
    "        Db1 /= m\n",
    "        Db2 /= m\n",
    "        cost /= m\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        W1 -= alpha * DW1\n",
    "        b1 -= alpha * Db1\n",
    "        W2 -= alpha * DW2\n",
    "        b2 -= alpha * Db2\n",
    "\n",
    "        # Assign new parameters to the model\n",
    "        model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print(\"Loss after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f4babd06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.482368\n",
      "Loss after iteration 100: 0.179039\n",
      "Loss after iteration 200: 0.172427\n",
      "Loss after iteration 300: 0.169946\n",
      "Loss after iteration 400: 0.168673\n",
      "Loss after iteration 500: 0.167900\n",
      "Loss after iteration 600: 0.167393\n",
      "Loss after iteration 700: 0.167024\n",
      "Loss after iteration 800: 0.166744\n",
      "Loss after iteration 900: 0.166528\n",
      "Loss after iteration 1000: 0.166352\n",
      "Loss after iteration 1100: 0.166208\n",
      "Loss after iteration 1200: 0.166086\n",
      "Loss after iteration 1300: 0.165979\n",
      "Loss after iteration 1400: 0.165885\n",
      "Loss after iteration 1500: 0.165802\n",
      "Loss after iteration 1600: 0.165728\n",
      "Loss after iteration 1700: 0.165660\n",
      "Loss after iteration 1800: 0.165596\n",
      "Loss after iteration 1900: 0.165537\n",
      "Loss after iteration 2000: 0.165481\n",
      "Loss after iteration 2100: 0.165429\n",
      "Loss after iteration 2200: 0.165379\n",
      "Loss after iteration 2300: 0.165331\n",
      "Loss after iteration 2400: 0.165285\n",
      "Loss after iteration 2500: 0.165241\n",
      "Loss after iteration 2600: 0.165198\n",
      "Loss after iteration 2700: 0.165157\n",
      "Loss after iteration 2800: 0.165117\n",
      "Loss after iteration 2900: 0.165079\n",
      "Loss after iteration 3000: 0.165042\n",
      "Loss after iteration 3100: 0.165006\n",
      "Loss after iteration 3200: 0.164971\n",
      "Loss after iteration 3300: 0.164937\n",
      "Loss after iteration 3400: 0.164903\n",
      "Loss after iteration 3500: 0.164871\n",
      "Loss after iteration 3600: 0.164839\n",
      "Loss after iteration 3700: 0.164809\n",
      "Loss after iteration 3800: 0.164779\n",
      "Loss after iteration 3900: 0.164750\n",
      "Loss after iteration 4000: 0.164722\n",
      "Loss after iteration 4100: 0.164695\n",
      "Loss after iteration 4200: 0.164668\n",
      "Loss after iteration 4300: 0.164642\n",
      "Loss after iteration 4400: 0.164616\n",
      "Loss after iteration 4500: 0.164592\n",
      "Loss after iteration 4600: 0.164568\n",
      "Loss after iteration 4700: 0.164544\n",
      "Loss after iteration 4800: 0.164521\n",
      "Loss after iteration 4900: 0.164499\n",
      "Loss after iteration 5000: 0.164477\n",
      "{'W1': array([[ 0.72233808,  0.1095257 ,  0.40227652,  1.00183583,  0.78329114],\n",
      "       [-0.3504042 ,  0.41093856,  0.02059112,  0.16036394,  0.06415658],\n",
      "       [ 0.10420877,  0.54806579,  0.35489312,  0.00169983,  0.02334841],\n",
      "       [ 0.14005425,  0.78571255, -0.07616626,  0.19805929, -0.22920131],\n",
      "       [-1.07091597,  0.34851166,  0.41343673, -0.30625377,  1.04965128],\n",
      "       [-0.56894192,  0.06219274, -0.01148874,  0.79112089,  0.63443387],\n",
      "       [-0.02937866, -0.01052242, -0.51071799, -0.84662713, -0.23957263],\n",
      "       [ 0.03604664,  0.57288646,  0.52433271, -0.14254879, -0.0666391 ]]), 'b1': array([[-0.05656992],\n",
      "       [-0.12120977],\n",
      "       [-0.19542273],\n",
      "       [ 0.22629932],\n",
      "       [ 0.07302412],\n",
      "       [ 0.0373275 ],\n",
      "       [-0.18492197],\n",
      "       [ 0.09156071]]), 'W2': array([[-0.1541186 , -0.42321678, -0.65067735,  0.63435709,  0.2450552 ,\n",
      "        -0.21000117, -0.25162714,  0.1723251 ]]), 'b2': array([[0.16835481]])}\n"
     ]
    }
   ],
   "source": [
    "model = buildModel(8,5001,True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84fe645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    a0 = x.T\n",
    "\n",
    "    # Forward propagation\n",
    "    z1 = np.dot(W1 , a0) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = np.dot(W2 , a1) + b2\n",
    "    a2 = z2\n",
    "\n",
    "    prediction = a2\n",
    "    \n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7341fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDefinitions(word_cand_list,word_sentence_dict):\n",
    "    \n",
    "    equiv_tag_dict = {'r':'adv', 'n':'n', 'v':'v', 'a':'adj'}\n",
    "    \n",
    "    definitions = []\n",
    "    \n",
    "    for pair in word_cand_list:\n",
    "        text = ''\n",
    "        definition = ''\n",
    "        optional_def = ''\n",
    "        target_word = pair[0]\n",
    "        word_sentence = word_sentence_dict[target_word]\n",
    "        word_tag = getPosTagFromSentence(word_sentence, target_word)\n",
    "        word_type = getTypeFromTag(word_tag)\n",
    "        check_tag = equiv_tag_dict.get(word_type, word_type)\n",
    "        candidate = pair[1]\n",
    "        url = f\"https://api.datamuse.com/words?sp={target_word}&md=d\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            for definition in data[0]['defs']:\n",
    "                tag_def_list = definition.split('\\t')\n",
    "                if(tag_def_list[0] == check_tag):\n",
    "                    definition = tag_def_list[1]\n",
    "            if(len(data[0]['defs'])>0 and definition==''): # If we had definitions but no one was chosen (select the first one)\n",
    "                definition = data[0]['defs'][0].split('\\t')[1]\n",
    "                \n",
    "        if(len(definition.split('\\t'))>1):\n",
    "            definition = definition.split('\\t')[1]\n",
    "            \n",
    "        if(definition[0]=='('):\n",
    "            definition = re.sub(r'\\([^)]*\\)', '', definition, 1)\n",
    "                \n",
    "        if(target_word and candidate and definition):\n",
    "            text = target_word + \" (replaced with \" + candidate + \") is: \" + definition.lower()\n",
    "        elif(target_word and definition):\n",
    "            text = target_word + \" is: \" + definition.lower()\n",
    "            \n",
    "        definitions.append(text)\n",
    "        \n",
    "    return definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "98aa3e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(text, printText=False):\n",
    "    \n",
    "    model = {'W1': [[ 0.59379275,  0.10573328,  0.3333015 ,  0.92123417,  0.67295641,\n",
    "        -0.35045897],\n",
    "       [ 0.38686119, -0.05986339, -0.06105094,  0.20350073,  0.04233106,\n",
    "         0.59301918],\n",
    "       [ 0.19332279,  0.00160017,  0.08232089,  0.22751841,  0.50550375,\n",
    "        -0.06886701],\n",
    "       [ 0.08795559, -0.36734301, -1.04466844,  0.22720356,  0.37344396,\n",
    "        -0.29992849],\n",
    "       [ 0.92379904, -0.62224386,  0.01826949, -0.1278862 ,  0.63526898,\n",
    "         0.58802669],\n",
    "       [-0.19257993, -0.03247628, -0.5495473 , -0.80284288, -0.31687566,\n",
    "         0.08342405],\n",
    "       [ 0.45730753,  0.41172155, -0.21174458, -0.24380291, -0.44752727,\n",
    "        -0.59411973],\n",
    "       [-0.65819502,  0.82130702, -0.17795647, -0.1987479 , -0.49519112,\n",
    "         0.32212151]], 'b1': [[-0.09245627],\n",
    "       [-0.01986108],\n",
    "       [-0.16607081],\n",
    "       [ 0.00942408],\n",
    "       [-0.01498752],\n",
    "       [-0.31538165],\n",
    "       [-0.01910803],\n",
    "       [ 0.0653324 ]], 'W2': [[-0.20134495, -0.19306152, -0.30827815,  0.09302178,  0.16951155,\n",
    "        -0.3618528 ,  0.06294159,  0.04406287]], 'b2': [[0.40915099]]}\n",
    "    \n",
    "    color_red = \"\\033[31m\"\n",
    "    color_green = \"\\033[32m\"\n",
    "    color_reset = \"\\033[0m\"\n",
    "    \n",
    "    num_features = 6\n",
    "    num_single_features = 5\n",
    "    vowels = \"aeiouAEIOU\"\n",
    "    prediction = []\n",
    "    definitions = []\n",
    "    word_cand_list = []\n",
    "    word_replace_dict = {}\n",
    "    thresh_scores = {} \n",
    "    thresh_scores, word_sentence_dict = complexWordIdentif(text)\n",
    "#     print(word_sentence_dict)\n",
    "    word_subst_dict = {}\n",
    "    new_text = text # initialize the new string with the original one\n",
    "    for word in word_sentence_dict:\n",
    "        word_subst_dict[word] = set()\n",
    "        sentence = word_sentence_dict[word]\n",
    "        word_net_cand, thesaurus_cand, bighuge_cand, ppdb_candidates = genSubstitutionSpec(word, sentence)\n",
    "        if(word_net_cand):\n",
    "            word_subst_dict[word].update(word_net_cand)\n",
    "        if(thesaurus_cand):\n",
    "            word_subst_dict[word].update(thesaurus_cand)\n",
    "        if(bighuge_cand):\n",
    "            word_subst_dict[word].update(bighuge_cand)\n",
    "        if(ppdb_candidates):\n",
    "            word_subst_dict[word].update(ppdb_candidates)\n",
    "    filtered_subs_dict = filterSubstitutions(word_subst_dict)\n",
    "#     print(word_subst_dict)\n",
    "#     print(filtered_subs_dict)\n",
    "    modified_word_subs_dict = convertGrammStructure(filtered_subs_dict, word_sentence_dict) \n",
    "#     print(modified_word_subs_dict)\n",
    "    \n",
    "    # EXTRACT FEATURES\n",
    "    for target_word in modified_word_subs_dict:\n",
    "        word_phrase_dict = {}\n",
    "        feature_matrix = []\n",
    "        cosine_similarities = []\n",
    "        similarity_ratios = []\n",
    "        sem_similarity_ratios = []\n",
    "        candidates = []\n",
    "        sentence = word_sentence_dict[target_word]\n",
    "        candidates = modified_word_subs_dict[target_word]\n",
    "#         print(candidates)\n",
    "#         candidates = [target_word] + modified_word_subs_dict[target_word]\n",
    "        if(len(candidates)>0):\n",
    "            for candidate in candidates:\n",
    "                three_gram_phrase = ''\n",
    "                prev_word = word_preceding(sentence, target_word)\n",
    "                next_word = word_following(sentence, target_word)\n",
    "                if(prev_word and next_word):\n",
    "                    three_gram_phrase = prev_word + \" \" + candidate + \" \" + next_word\n",
    "                    word_phrase_dict[candidate] = [prev_word, candidate, next_word]\n",
    "                features_list = extractFeaturesFromWord(candidate, word_phrase_dict)\n",
    "                if(target_word.lower() in word_vectors and candidate.lower() in word_vectors):     \n",
    "                    target_word_vector = word_vectors[target_word.lower()]\n",
    "                    substitution_vector = word_vectors[candidate.lower()]\n",
    "                    cos_similarity = getCosSim(target_word_vector, substitution_vector)\n",
    "                else:\n",
    "                    cos_similarity = 0\n",
    "                similarity_ratios.append(similarityRatio(target_word.lower(), candidate.lower()))\n",
    "                sem_similarity_ratios.append(semanticSimilarityRatio(target_word.lower(), candidate.lower()))\n",
    "                cosine_similarities.append(cos_similarity)\n",
    "                feature_matrix.append(features_list)\n",
    "            cosine_similarities = np.array(cosine_similarities).reshape(len(feature_matrix),1)\n",
    "            similarity_ratios = np.array(similarity_ratios).reshape(len(feature_matrix),1)\n",
    "            sem_similarity_ratios = np.array(sem_similarity_ratios).reshape(len(feature_matrix),1)\n",
    "            X = np.hstack((feature_matrix,cosine_similarities))\n",
    "            max_in_column = np.max(X,axis=0)\n",
    "            for i in range(num_features):\n",
    "                if(max_in_column[i] != 0):\n",
    "                    X[:, i] = X[:, i]/max_in_column[i]\n",
    "#             print(candidates)\n",
    "#             print(X)\n",
    "            prediction = predict(model, X)\n",
    "#             print(candidates)\n",
    "#             print(prediction)\n",
    "            min_value = min(prediction)\n",
    "            prediction_list = prediction.tolist()\n",
    "            min_index=prediction_list.index(min_value)\n",
    "#             print(prediction_list,\"\\n\",candidates)\n",
    "            chosen_candidate = candidates[min_index]\n",
    "            word_cand_list.append((target_word, chosen_candidate))\n",
    "            if(target_word[0].isupper()):\n",
    "                chosen_candidate = chosen_candidate[0].upper() + chosen_candidate[1:]\n",
    "            if(target_word.isupper()):\n",
    "                chosen_candidate = chosen_candidate.upper()\n",
    "            if(prev_word == 'a' and chosen_candidate[0] in vowels):\n",
    "                new_text = new_text.replace('a '+ target_word, 'an ' + chosen_candidate)\n",
    "            elif(prev_word == 'an' and chosen_candidate[0] not in vowels):\n",
    "                new_text = new_text.replace('an '+ target_word, 'a ' + chosen_candidate)\n",
    "            else:\n",
    "                new_text = new_text.replace(target_word, chosen_candidate)\n",
    "            word_replace_dict[target_word] = chosen_candidate\n",
    "#             print(len(text), len(new_text))\n",
    "\n",
    "    definitions = getDefinitions(word_cand_list,word_sentence_dict)\n",
    "\n",
    "    if(printText):\n",
    "        text_list = word_tokenize(text)\n",
    "        newtext_list = word_tokenize(new_text)\n",
    "        for word in text_list:\n",
    "            if(word in word_sentence_dict):\n",
    "                print(f\"{color_red}{word}{color_reset}\",end=\" \")\n",
    "            else:\n",
    "                print(f\"{word}\",end=\" \")\n",
    "        print(\"\\n\")\n",
    "        for i in range(len(text_list)):  \n",
    "            if(text_list[i] in word_sentence_dict):\n",
    "                print(f\"{color_green}{newtext_list[i]}{color_reset}\",end=\" \")\n",
    "            else:\n",
    "                print(f\"{newtext_list[i]}\",end=\" \")\n",
    "        print(\"\\n\")\n",
    "            \n",
    "    return new_text,definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3216b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Test Data Before And After (Uncomment the below lines)\n",
    "\n",
    "s = \"external links to other Internet sites should not be construed as an endorsement of the views or privacy policies contained therein \"\n",
    "n,definitions = simplify(s,True)\n",
    "\n",
    "print(definitions)\n",
    "\n",
    "# s2 = \"we generally do not permit transferral into the second year due to the unique nature of our course and fundamental skills that are learnt in the first year , but you are welcome to discuss your case with the admissions Tutor . \"\n",
    "# n2 = simplify(s2,True)\n",
    "\n",
    "# s3 = \"`` something will hopefully be done `` , to ensure that even if the mission were to close tomorrow , FM103 `` continues in some form or sort , to consolidate the peace process .\"\n",
    "# n3 = simplify(s3,True)\n",
    "\n",
    "# s4 = \"local churches reproduce themselves in their neighborhood and on the mission field .\"\n",
    "# n4 = simplify(s4,True)\n",
    "\n",
    "# s5 = \"that said , the best foundations are focused on accomplishing programmatic missions . \"\n",
    "# n5 = simplify(s5,True)\n",
    "\n",
    "# s6 = \"if you hesitate , therefore , a moment , or if , after reflection , you produce any intricate or profound argument , you , in a manner , give up the question , and confess that it is not reasoning which engages us to suppose the past resembling the future , and to expect similar effects from causes which are , to appearance , similar . \"\n",
    "# n6 = simplify(s6,True)\n",
    "\n",
    "# s7 = \"these functions also use scheduling priority to decide which thread gets to execute when there is contention .\"\n",
    "# n6 = simplify(s7,True)\n",
    "\n",
    "\n",
    "# for line in test_data:\n",
    "#     sentence = line[0]\n",
    "#     new_sentence = simplify(sentence,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "a2b57a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(model, n):\n",
    "    \n",
    "    # We use TRank metric : # of candidates chosen with rank <= n\n",
    "    \n",
    "    candidates_matrix = []\n",
    "    sentence_list = []\n",
    "    word_candCount_dict = {}\n",
    "    word_phrase_dict = {}\n",
    "    scores_matrix = []\n",
    "    complex_word_list = []\n",
    "    num_candidates = 0\n",
    "    num_features = 6\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    \n",
    "    for line in test_data:\n",
    "        candidates = []\n",
    "        scores = []\n",
    "        scores_candidates = line[3:]\n",
    "        sentence = line[0]\n",
    "        sentence_list.append(sentence)\n",
    "        target_word = line[1]\n",
    "        complex_word_list.append(target_word)\n",
    "        num_candidates += len(scores_candidates)\n",
    "        word_candCount_dict[target_word] = len(scores_candidates)\n",
    "        for score_candidate in scores_candidates:\n",
    "            candidates.append(score_candidate[2:])\n",
    "            scores.append(int(score_candidate[0]))\n",
    "        \n",
    "        candidates_matrix.append(candidates)\n",
    "        scores_matrix.append(scores)\n",
    "        \n",
    "    indx = 0\n",
    "    for line in candidates_matrix:\n",
    "        word_phrase_dict = {}\n",
    "        feature_matrix = []\n",
    "        cosine_similarities = []\n",
    "        similarity_ratios = []\n",
    "        sem_similarity_ratios = []\n",
    "        sentence = sentence_list[indx]\n",
    "        complex_word = complex_word_list[indx]\n",
    "        scores_list = scores_matrix[indx]\n",
    "        for candidate in line:\n",
    "            prev_word = word_preceding(sentence.lower(), complex_word)\n",
    "            next_word = word_following(sentence.lower(), complex_word)\n",
    "            if(prev_word and next_word):\n",
    "                word_phrase_dict[candidate] = [prev_word, candidate, next_word]\n",
    "            feature_list_ = extractFeaturesFromWord(candidate,word_phrase_dict)\n",
    "            if(complex_word in word_vectors and candidate in word_vectors):     \n",
    "                complex_word_vector = word_vectors[complex_word]\n",
    "                substitution_vector = word_vectors[candidate]\n",
    "                cos_similarity = getCosSim(complex_word_vector, substitution_vector)\n",
    "            else:\n",
    "                cos_similarity = 0\n",
    "            cosine_similarities.append(cos_similarity)\n",
    "            similarity_ratios.append(similarityRatio(complex_word, candidate))\n",
    "            sem_similarity_ratios.append(semanticSimilarityRatio(target_word.lower(), candidate.lower()))\n",
    "            feature_matrix.append(feature_list_)\n",
    "        cosine_similarities = np.array(cosine_similarities).reshape(len(feature_matrix),1)\n",
    "        similarity_ratios = np.array(similarity_ratios).reshape(len(feature_matrix),1)\n",
    "        sem_similarity_ratios = np.array(sem_similarity_ratios).reshape(len(feature_matrix),1)\n",
    "        X = np.hstack((feature_matrix,cosine_similarities))\n",
    "        max_in_column = np.max(X,axis=0)\n",
    "        for i in range(num_features):\n",
    "            if(max_in_column[i] != 0):\n",
    "                X[:, i] /= max_in_column[i]\n",
    "                \n",
    "        prediction = predict(model, X)\n",
    "#         print(complex_word)\n",
    "#         print(scores_list)\n",
    "#         print(line)\n",
    "#         print(prediction)\n",
    "        min_value = min(prediction)\n",
    "        prediction_list = prediction.tolist()\n",
    "        min_index=prediction_list.index(min_value)\n",
    "        reference_score = scores_list[min_index]\n",
    "        \n",
    "        if(reference_score <= n):\n",
    "            num_correct+=1\n",
    "        \n",
    "        num_total +=1\n",
    "        indx+=1\n",
    "    \n",
    "    accuracy = num_correct/num_total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "b1814748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ranker at n=1 is  0.6594594594594595\n",
      "Accuracy of ranker at n=2 is  0.9297297297297298\n",
      "Accuracy of ranker at n=3 is  1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = testModel(model,1)\n",
    "print(\"Accuracy of ranker at n=1 is \",accuracy)\n",
    "\n",
    "accuracy = testModel(model,2)\n",
    "print(\"Accuracy of ranker at n=2 is \",accuracy)\n",
    "\n",
    "accuracy = testModel(model,3)\n",
    "print(\"Accuracy of ranker at n=3 is \",accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
