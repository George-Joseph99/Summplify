{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c501685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk\n",
    "import io\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from itertools import islice\n",
    "from rouge import Rouge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9fd77f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download()  # uncomment these lines once they are not downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb19c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(article):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "    article_preprocessed = []\n",
    "    sentences = sent_tokenize(article)\n",
    "    for sentence in sentences:\n",
    "        sentence_preprocessed = []\n",
    "        sentence = re.sub(r\"[^a-zA-Z\\s]+\", \"\", sentence)\n",
    "        words = word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            if (word not in stopwords_english and word not in string.punctuation):\n",
    "                word_stemmed = stemmer.stem(word)  \n",
    "                sentence_preprocessed.append(word_stemmed)\n",
    "#         sentence_preprocessed = \" \".join(sentence_preprocessed)\n",
    "        if sentence_preprocessed:\n",
    "            article_preprocessed.append(sentence_preprocessed)\n",
    "            \n",
    "        \n",
    "        \n",
    "#     words = [word_tokenize(sent) for sent in sentences]\n",
    "#     words_without_stopwords = [[word for word in sent if word not in stopwords.words('english')] for sent in words]\n",
    "    return article_preprocessed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f41b89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_string(sentences):  # converts list of lists to list of strings\n",
    "    sentences_modified = []   # list of strings\n",
    "    for sentence in sentences:\n",
    "        sentence_modified = ' '.join(sentence)\n",
    "        sentences_modified.append(sentence_modified)\n",
    "    return sentences_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4a6b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature 1\n",
    "\n",
    "def calculate_TF_IDF(content):\n",
    "    flat_words = [word for sent in content for word in sent]\n",
    "    words_set = set(flat_words)\n",
    "    words_num = len(words_set)\n",
    "#     print(words_set)\n",
    "    tf = pd.DataFrame(np.zeros((len(content), words_num)), columns = words_set)\n",
    "    for i in range (len(content)):\n",
    "        for w in content[i]:\n",
    "                      tf[w][i] += 1/len(content[i])\n",
    "                      \n",
    "    idf = {}\n",
    "    \n",
    "    for word in words_set:\n",
    "        num_docs = 0\n",
    "        for i in range(len(content)):\n",
    "            if word in content[i]:\n",
    "                num_docs += 1\n",
    "                \n",
    "        idf[word] = np.log10(len(content) / num_docs)\n",
    "        \n",
    "    tf_idf = np.zeros(len(content))\n",
    "    \n",
    "    for i in range (len(content)):\n",
    "        for word in content[i]:\n",
    "            tf_idf[i] += tf[word][i] * idf[word]\n",
    "            \n",
    "#     print(tf_idf/max(tf_idf))\n",
    "    tf_idf = tf_idf/max(tf_idf)  # might be commented (this normalizes the tf-idf)\n",
    "            \n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf6dcc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# article = [['data', 'science', 'is' ,'one', 'of', 'the', 'most', 'important', 'fields', 'of', 'science'],\n",
    "#            ['this', 'is', 'one', 'of', 'the', 'best', 'data', 'science', 'courses'],\n",
    "#            ['data', 'scientists', 'analyze', 'data']]\n",
    "\n",
    "# x = calculate_TF_IDF(article)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c5125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_file = io.open(\"articles/original (\" + str(1) +\").txt\", \"r\", encoding='utf-8-sig')\n",
    "# article_file.readline()\n",
    "# article = article_file.read()\n",
    "# article_file.close()\n",
    "\n",
    "# # print(article)\n",
    "# article_preprocessed = preprocessing(article)\n",
    "# x = calculate_TF_IDF(article)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17129375",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature 2\n",
    "\n",
    "def sentence_length(article_preprocessed):\n",
    "    article_preprocessed = convert_list_to_string(article_preprocessed)\n",
    "    max_length = 0\n",
    "    for sentence in article_preprocessed:\n",
    "        # print(sentence)\n",
    "        if len(sentence.split()) > max_length:\n",
    "            max_length = len(sentence.split())\n",
    "            \n",
    "    sentence_length_feature = []\n",
    "    for sentence in article_preprocessed:\n",
    "        sentence_length_feature.append(len(sentence.split()) / max_length)\n",
    "\n",
    "#     sentence_length_feature = []\n",
    "#     for sentence in article_preprocessed:\n",
    "#         sentence_length_feature.append(1 / len(sentence.split()))\n",
    "\n",
    "    return sentence_length_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a4b22f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_file = io.open(\"articles/original (\" + str(1) +\").txt\", \"r\", encoding='utf-8-sig')\n",
    "# article_file.readline()\n",
    "# art = article_file.read()\n",
    "# article_file.close()\n",
    "\n",
    "# art = preprocessing(art)\n",
    "# art = convert_list_to_string(art)\n",
    "# sentence_length(article_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fc1f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_X_labels(article_preprocessed):\n",
    "    # feature 1 (tf_idf)\n",
    "#     word_scores = calculate_TF_IDF(article_preprocessed)\n",
    "#     tf_idf_score_feature = calculate_each_sentence_score(article_preprocessed, word_scores)\n",
    "    tf_idf_score_feature = calculate_TF_IDF(article_preprocessed)\n",
    "    \n",
    "    \n",
    "    # feature 2 (sentence_length)\n",
    "    sentence_length_feature = sentence_length(article_preprocessed)\n",
    "    \n",
    "    matrix = np.column_stack((tf_idf_score_feature, sentence_length_feature))\n",
    "#     matrix = np.array(tf_idf_score_feature).reshape(len(tf_idf_score_feature), 1)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c41f3ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_file = io.open(\"articles/original (\" + str(1) +\").txt\", \"r\", encoding='utf-8-sig')\n",
    "# article_file.readline()\n",
    "# article = article_file.read()\n",
    "# article_file.close()\n",
    "\n",
    "# art = preprocessing(article)\n",
    "# art = convert_list_to_string(art)\n",
    "# print(generate_X_labels(article_preprocessed))\n",
    "# print(len(generate_X_labels(article_preprocessed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1be1abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Y_labels(original, summarized):\n",
    "    Y_list = []\n",
    "    original_sentences = sent_tokenize(original)\n",
    "    original_sentences[0] = original_sentences[0][1:] # to remove the \\n\n",
    "    summarized_sentences = sent_tokenize(summarized)\n",
    "    \n",
    "    for original_sentence in original_sentences:\n",
    "        added = 0\n",
    "        for summarized_sentence in summarized_sentences:\n",
    "            if original_sentence in summarized_sentence:\n",
    "                Y_list.append(1)\n",
    "                added = 1\n",
    "                break\n",
    "        if added == 0:\n",
    "            Y_list.append(0)\n",
    "    \n",
    "    return Y_list, original_sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad975c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_file = io.open(\"articles/original (\" + str(301) +\").txt\", \"r\", encoding='utf-8-sig')\n",
    "# article_file.readline()\n",
    "# article = article_file.read()\n",
    "# article_file.close()\n",
    "\n",
    "# summarized_file = io.open(\"articles/summarized (\" + str(301) +\").txt\", \"r\", encoding='utf-8-sig')\n",
    "# summarized = summarized_file.read()\n",
    "# summarized_file.close()\n",
    "\n",
    "# Y,_ = generate_Y_labels(article, summarized)\n",
    "# print(Y)\n",
    "# print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5378413a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4179\n",
      "4179\n"
     ]
    }
   ],
   "source": [
    "X_matrix = []\n",
    "X = []\n",
    "Y = []\n",
    "sentences = []\n",
    "\n",
    "article_types = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "\n",
    "for article_type in article_types:\n",
    "    for i in range (1, 51):   # loading business articles\n",
    "        article_file = io.open(\"train_original/\" + article_type + \"/article (\" + str(i) +\").txt\", \"r\", encoding='utf-8-sig')\n",
    "        article_file.readline()\n",
    "        article = article_file.read()\n",
    "        article_file.close()\n",
    "\n",
    "        summarized_file = io.open(\"train_summary/\" + article_type + \"/summary (\" + str(i) +\").txt\", \"r\", encoding='utf-8-sig')\n",
    "        summarized = summarized_file.read()\n",
    "        summarized_file.close()\n",
    "\n",
    "        article_preprocessed = preprocessing(article)\n",
    "    #     article_preprocessed = convert_list_to_string(article_preprocessed)\n",
    "        X_i = generate_X_labels(article_preprocessed)\n",
    "        Y_i, original_list_no_first_space = generate_Y_labels(article, summarized)\n",
    "\n",
    "        if(len(X_i) != len(Y_i)):\n",
    "            print('Error! features and labels are not equal in length')\n",
    "\n",
    "        Y.extend(Y_i)\n",
    "        X_matrix.extend(X_i)\n",
    "        sentences.extend(original_list_no_first_space)\n",
    "    \n",
    "\n",
    "for x in X_matrix:\n",
    "    X.append(x.tolist())\n",
    "    \n",
    "X = np.matrix(X)\n",
    "\n",
    "m = len(X)\n",
    "\n",
    "print(len(X))\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55b66762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.matrix'>\n",
      "[[0.73203128 0.54545455]\n",
      " [0.87185081 0.63636364]\n",
      " [0.69813911 0.36363636]\n",
      " [0.75779078 0.59090909]\n",
      " [0.74727107 0.31818182]\n",
      " [0.72061857 0.27272727]\n",
      " [0.78105634 0.45454545]\n",
      " [0.69638967 0.63636364]\n",
      " [0.8741924  0.81818182]\n",
      " [0.78987147 0.59090909]]\n",
      "<class 'list'>\n",
      "[1, 0, 1, 1, 0, 0, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(type(X))\n",
    "print(X[: 10, :])\n",
    "print(type(Y))\n",
    "print(Y[: 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d42a131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_input_dim = 2 # input layer size (we have two input features)\n",
    "nn_output_dim = 1  # output layer size (we have one output)\n",
    "\n",
    "# Gradient descent parameters\n",
    "alpha = 0.1  # learning rate for gradient descent\n",
    "# print(Y)\n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be4a9b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # TODO 1: Compute the sigmoid function at the given x (~1 line)\n",
    "    # For example: sigmoid(2) should compute the value of sigmoid function at x = 2.\n",
    "    # Hint: Use np.exp instead of math.exp to allow for vectorization.\n",
    "    #----------------------------------------------------------------------------------------------\n",
    "    sig = (1/(1+np.exp(-x)))\n",
    "    #----------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1468bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(nn_hdim, num_passes=20000, print_loss=False):\n",
    "    \n",
    "    # This function learns parameters for the neural network and returns the model.\n",
    "    # - nn_hdim: Number of nodes in the hidden layer\n",
    "    # - num_passes: Number of iterations (epochs) through the training data for gradient descent\n",
    "    # - print_loss: If True, print the loss every 1000 iterations\n",
    "\n",
    "    # Initialize the parameters to random values. We need to learn these at the end.\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(nn_hdim, nn_input_dim) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((nn_hdim, 1))\n",
    "    W2 = np.random.randn(nn_hdim, nn_hdim) / np.sqrt(nn_input_dim)\n",
    "    b2 = np.zeros((nn_hdim, 1))\n",
    "    W3 = np.random.randn(nn_hdim, nn_hdim) / np.sqrt(nn_input_dim)\n",
    "    b3 = np.zeros((nn_hdim, 1))\n",
    "    W4 = np.random.randn(nn_output_dim, nn_hdim) / np.sqrt(nn_hdim)\n",
    "    b4 = np.zeros((nn_output_dim, 1))\n",
    "\n",
    "    # This is what we return at the end\n",
    "    model = {}\n",
    "\n",
    "    # Batch Gradient descent (We accumulate the loss for each training point before updating the weights)\n",
    "    # For each iteration:\n",
    "    for i in range(0, num_passes):\n",
    "        DW1 = 0\n",
    "        DW2 = 0\n",
    "        DW3 = 0\n",
    "        DW4 = 0\n",
    "        Db1 = 0\n",
    "        Db2 = 0\n",
    "        Db3 = 0\n",
    "        Db4 = 0\n",
    "        cost = 0\n",
    "        # Loop on every training example...\n",
    "        for j in range(0, m):\n",
    "            a0 = X[j, :].reshape(-1, 1)  # Every training example is a column vector.\n",
    "            y = Y[j]\n",
    "            \n",
    "            # TODO 2: Apply forward propagation on every training example a0 (a column vector 2x1) with its\n",
    "            # corresponding label y. It is required to compute z1, a1, z2, and a2\n",
    "            #----------------------------------------------------------------------------------------------\n",
    "            # Forward propagation\n",
    "            z1 = np.dot(W1 , a0 )+ b1\n",
    "            a1 = np.tanh(z1)\n",
    "            z2 = np.dot(W2 , a1 )+ b2\n",
    "            a2 = np.tanh(z2)\n",
    "            z3 = np.dot(W3 , a2 )+ b3\n",
    "            a3 = np.tanh(z3)\n",
    "            z4 = np.dot(W4 , a3) + b4\n",
    "            a4 = sigmoid(z4)\n",
    "            #----------------------------------------------------------------------------------------------\n",
    "\n",
    "            # TODO 3: Compute the cost/loss function for every training example (Hint: use np.log)\n",
    "            # ---------------------------------------------------------------------------------------------\n",
    "            cost_j = -1 * ((np.log(a4) * y + (1-y)* np.log(1-a4)))\n",
    "            # ---------------------------------------------------------------------------------------------\n",
    "\n",
    "            # TODO 4: Derive the equations of backpropagation to find dW2, db2, dW1, and db1.\n",
    "            # Hint: Check the dimensions at each step. \n",
    "            # Hint: For element-wise multiplication use *, for matrix multiplication use @\n",
    "            # Example: y = A * B performs element wise multiplication \n",
    "            #          y = A @ B performs matrix multiplication\n",
    "            # ---------------------------------------------------------------------------------------------\n",
    "            da4 =  ( -y/a4  + (1-y)/(1-a4) )\n",
    "            dz4 =  da4 * a4 * ( 1 - a4)\n",
    "            dW4 = np.dot(dz4 , a3.T)\n",
    "            db4 = dz4\n",
    "            \n",
    "            da3 =  np.dot(W4.T, dz4)\n",
    "            dz3 = np.multiply(da3 , 1 - np.square(a3) )\n",
    "            dW3 = np.dot(dz3 , a2.T )\n",
    "            db3 = dz3\n",
    "            \n",
    "            da2 =  np.dot(W3.T, dz3)\n",
    "            dz2 = np.multiply(da2 , 1 - np.square(a2) )\n",
    "            dW2 = np.dot(dz2 , a1.T )\n",
    "            db2 = dz2\n",
    "\n",
    "            da1 =  np.dot(W2.T, dz2)\n",
    "            dz1 = np.multiply(da1 , 1 - np.square(a1) )\n",
    "            dW1 = np.dot(dz1 , a0.T )\n",
    "            db1 = dz1\n",
    "            \n",
    "            # ---------------------------------------------------------------------------------------------\n",
    "            \n",
    "            # Accumulating the sum of dW1, db1, dW2, db2 and cost_j into the variables DW1, Db1, DW2, Db2 and cost\n",
    "            # for all training set. \n",
    "            DW1 += dW1\n",
    "            DW2 += dW2\n",
    "            DW3 += dW3\n",
    "            DW4 += dW4\n",
    "            Db4 += db4\n",
    "            Db3 += db3\n",
    "            Db2 += db2\n",
    "            Db1 += db1\n",
    "            cost += cost_j\n",
    "        \n",
    "        # Averaging DW1, DW2, Db1, Db2 and cost over the m training examples. \n",
    "        DW1 /= m\n",
    "        DW2 /= m\n",
    "        DW3 /= m\n",
    "        DW4 /= m\n",
    "        Db1 /= m\n",
    "        Db2 /= m\n",
    "        Db3 /= m\n",
    "        Db4 /= m\n",
    "        cost /= m\n",
    "\n",
    "        # TODO 5: Perform the gradient descent parameter update.\n",
    "        # ---------------------------------------------------------------------------------------------------\n",
    "        # Gradient descent parameter update\n",
    "        W1 -= alpha * DW1\n",
    "        b1 -= alpha * Db1\n",
    "        W2 -= alpha * DW2\n",
    "        b2 -= alpha * Db2\n",
    "        W3 -= alpha * DW3\n",
    "        b3 -= alpha * Db3\n",
    "        W4 -= alpha * DW4\n",
    "        b4 -= alpha * Db4\n",
    "        # ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Assign new parameters to the model\n",
    "        model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3, 'W4': W4, 'b4': b4}\n",
    "\n",
    "        # Optionally print the loss.\n",
    "        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "        if print_loss and i % 1000 == 0:\n",
    "            print(\"Loss after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bd38e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to predict an output (0 or 1)\n",
    "def predict(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    a0 = x.T\n",
    "    \n",
    "    # TODO 6 (aka TODO 2): Apply forward propagation on every test example a0 (a column vector 2x1) with its\n",
    "    #  corresponding label y. It is required to compute z1, a1, z2, and a2  (SAME AS TODO2).\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    z1 = np.dot(W1 , a0) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = np.dot(W2 , a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    # ------------------------------------------------------------------------------------------------\n",
    "    # Applying a threshold of 0.5 (i.e. predictions greater than 0.5 are mapped to 1, and 0 otherwise)\n",
    "#     prediction = np.round(a2)\n",
    "    prediction = a2\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# def predict(model, x):\n",
    "#     W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model['b3'], model['W4'], model['b4']\n",
    "#     a0 = x.T\n",
    "    \n",
    "#     # TODO 6 (aka TODO 2): Apply forward propagation on every test example a0 (a column vector 2x1) with its\n",
    "#     #  corresponding label y. It is required to compute z1, a1, z2, and a2  (SAME AS TODO2).\n",
    "#     # -----------------------------------------------------------------------------------------------\n",
    "#     z1 = np.dot(W1 , a0 )+ b1\n",
    "#     a1 = np.tanh(z1)\n",
    "#     z2 = np.dot(W2 , a1 )+ b2\n",
    "#     a2 = np.tanh(z2)\n",
    "#     z3 = np.dot(W3 , a2 )+ b3\n",
    "#     a3 = np.tanh(z3)\n",
    "#     z4 = np.dot(W4 , a3) + b4\n",
    "#     a4 = sigmoid(z4)\n",
    "#     # ------------------------------------------------------------------------------------------------\n",
    "#     # Applying a threshold of 0.5 (i.e. predictions greater than 0.5 are mapped to 1, and 0 otherwise)\n",
    "#     # prediction = np.round(a2)\n",
    "#     prediction = a4    \n",
    "#     return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9854a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.670913\n",
      "Loss after iteration 1000: 0.566402\n",
      "Loss after iteration 2000: 0.563649\n",
      "Loss after iteration 3000: 0.563039\n",
      "Loss after iteration 4000: 0.562859\n",
      "Loss after iteration 5000: 0.562725\n",
      "Loss after iteration 6000: 0.562606\n",
      "Loss after iteration 7000: 0.562495\n",
      "Loss after iteration 8000: 0.562387\n",
      "Loss after iteration 9000: 0.562281\n",
      "Loss after iteration 10000: 0.562172\n"
     ]
    }
   ],
   "source": [
    "model = build_model(nn_hdim= 8, num_passes = 10001, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52610186",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26288\\2474145594.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3be39ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(model)\n",
    "# model tf-idf not normalized\n",
    "# model = {'W1': np.array([[ 1.38785383,  0.49082933],\n",
    "#        [ 0.55641748,  1.62806027],\n",
    "#        [ 1.85376828, -0.3225326 ],\n",
    "#        [ 1.78168709,  0.05507265],\n",
    "#        [ 1.5035727 ,  0.15089058],\n",
    "#        [-0.21027833,  1.92211576],\n",
    "#        [ 0.46820238,  0.16111129],\n",
    "#        [ 0.84196808,  0.12407808]]), 'b1': np.array([[ 0.38819622],\n",
    "#        [-0.02713244],\n",
    "#        [-0.8429403 ],\n",
    "#        [-0.91256074],\n",
    "#        [-0.66510711],\n",
    "#        [ 0.01664907],\n",
    "#        [ 0.03959506],\n",
    "#        [-0.22607371]]), 'W2': np.array([[ 0.6266244 ,  0.50983672, -1.1677148 , -1.47645926, -1.39277239,\n",
    "#          1.53280975,  0.17978621, -0.50006151]]), 'b2': np.array([[0.58550391]])}\n",
    "\n",
    "# model = {'W1': np.array([[ 1.18336695,  0.1250435 ], # 3 layers\n",
    "#        [ 0.38733117,  1.8530943 ],\n",
    "#        [ 1.75886377, -0.58510996],\n",
    "#        [ 0.54240451, -0.05259935],\n",
    "#        [-0.09248666,  0.2141493 ],\n",
    "#        [ 0.0432581 ,  0.9329085 ],\n",
    "#        [ 0.66859945,  0.25386436],\n",
    "#        [ 0.5554128 ,  0.36237266]]), 'b1': np.array([[-0.28405656],\n",
    "#        [-0.40425758],\n",
    "#        [-0.99870768],\n",
    "#        [ 0.05080746],\n",
    "#        [-0.09605534],\n",
    "#        [-0.26051699],\n",
    "#        [-0.18459905],\n",
    "#        [-0.28472492]]), 'W2': np.array([[ 1.13457184, -0.20822159,  0.38311448, -0.51971462, -1.7927709 ,\n",
    "#          0.47739742,  0.70361151, -0.44614564],\n",
    "#        [ 1.63177288, -1.0197636 , -0.08924496, -0.14037122,  1.09361726,\n",
    "#          1.04981611,  0.09540724,  0.2347938 ],\n",
    "#        [-0.67150498, -1.45686798, -0.20758122,  0.09659014,  0.85981688,\n",
    "#          0.80570778, -0.296603  , -0.22930296],\n",
    "#        [-0.8237228 , -1.15021352, -1.24466209,  1.34648287, -0.36599564,\n",
    "#         -0.38097044, -0.92866868,  0.50709689],\n",
    "#        [-1.05799442, -0.01422579, -0.90377796,  0.28580989, -0.35451199,\n",
    "#         -0.7717647 , -0.02472574,  0.26142491],\n",
    "#        [ 0.01077594,  0.59710575, -0.83069778, -0.40593486, -0.49098818,\n",
    "#         -0.20682705, -0.76479984, -1.38033905],\n",
    "#        [ 0.03789108, -0.00967381, -1.58485507,  0.14915192, -0.62770378,\n",
    "#          0.15492018,  0.36470344, -0.01501329],\n",
    "#        [ 0.78647832, -1.02272337,  0.42132782, -0.45981076, -0.62587553,\n",
    "#         -0.50757541, -0.22120812,  0.02481316]]), 'b2': np.array([[-0.05359941],\n",
    "#        [ 0.12717962],\n",
    "#        [-0.061023  ],\n",
    "#        [-0.05207335],\n",
    "#        [ 0.31889328],\n",
    "#        [ 0.25685478],\n",
    "#        [ 0.1398867 ],\n",
    "#        [-0.04797326]]), 'W3': np.array([[-0.80349617,  0.71868534,  0.20718625, -1.20396728,  1.00463432,\n",
    "#          1.57134532,  1.02363706, -0.37870051],\n",
    "#        [-0.89039626,  0.50355204, -0.02357477,  1.08788051,  0.29202038,\n",
    "#          0.60301908,  0.07305096,  0.60210272],\n",
    "#        [ 0.03802195,  1.28299045,  0.06325746,  0.27166014,  1.30692966,\n",
    "#         -1.09726151, -1.02922169,  0.86082546],\n",
    "#        [-0.90207611,  1.29694031, -0.26202875, -0.47270786,  1.46564552,\n",
    "#          1.30205973,  1.4459055 ,  0.57549995],\n",
    "#        [-0.66125529,  1.43636039, -0.21285799,  0.59702511,  0.6944864 ,\n",
    "#         -0.04123769,  0.48235273,  0.71124213],\n",
    "#        [ 0.263836  , -0.77833376,  0.21475251,  0.94851392, -0.48556096,\n",
    "#         -0.10818819, -0.30816457,  1.30529932],\n",
    "#        [ 0.487098  ,  0.38296521, -0.62201678,  0.30699157, -0.51424632,\n",
    "#          0.09400846, -0.3431704 ,  0.40476813],\n",
    "#        [ 0.39364728, -0.2458679 ,  0.37127184, -0.72310426, -1.03654804,\n",
    "#          0.2336883 ,  0.0376808 ,  0.47617952]]), 'b3': np.array([[ 0.14588719],\n",
    "#        [-0.32320868],\n",
    "#        [ 0.02159727],\n",
    "#        [-0.02307498],\n",
    "#        [ 0.08842675],\n",
    "#        [ 0.00334234],\n",
    "#        [ 0.09982744],\n",
    "#        [-0.13245038]]), 'W4': np.array([[ 1.27121312,  0.57705831, -0.72331337,  0.69760112, -0.86182132,\n",
    "#          0.17466716, -0.23174607,  0.40275134]]), 'b4': np.array([[-0.47140409]])}\n",
    "# model tf-idf normalized\n",
    "model = {'W1': np.array([[ 1.29643591,  0.45074824],\n",
    "       [ 0.70773118,  1.58460852],\n",
    "       [ 2.67738766, -1.74837318],\n",
    "       [ 1.33109077,  0.10529432],\n",
    "       [ 0.69949236,  0.23108707],\n",
    "       [-0.35599099,  2.07436754],\n",
    "       [ 0.47995731,  0.24472955],\n",
    "       [ 0.63720221,  0.2369889 ]]), 'b1': np.array([[ 0.24555316],\n",
    "       [-0.02617257],\n",
    "       [-1.61662627],\n",
    "       [-1.31429877],\n",
    "       [-0.17949485],\n",
    "       [ 0.07467697],\n",
    "       [ 0.13714999],\n",
    "       [ 0.03253189]]), 'W2': np.array([[-0.20112114,  0.1343918 , -2.29593635, -1.68775395, -1.66578869,\n",
    "         1.04315377, -0.56503497, -1.11453418]]), 'b2': np.array([[-0.64778329]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec7bb518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(article_preprocessed_test, original_test, summarized_text, compression_ratio, file_number = 0):\n",
    "    X_test = generate_X_labels(article_preprocessed_test)\n",
    "    predicton = predict(model, X_test)\n",
    "    Y_test, original_sentences = generate_Y_labels(original_test, summarized_text)\n",
    "    num_sentences_summarized = math.ceil(compression_ratio * len(original_sentences))\n",
    "    \n",
    "    \n",
    "    highest = np.argsort(predicton[0]) [::-1]\n",
    "    highest = highest[: num_sentences_summarized]\n",
    "#     highest = sorted(highest) # uncomment to arrange the article\n",
    "    output_sentences = []\n",
    "    output_indices = []\n",
    "    \n",
    "\n",
    "    \n",
    "    for i in range (0, num_sentences_summarized):\n",
    "        output_sentences.append(original_sentences[highest[i]])\n",
    "        output_indices.append(highest[i])\n",
    "        \n",
    "    output_sentences = ''.join(output_sentences)\n",
    "    \n",
    "    rouge = Rouge(metrics=['rouge-n', 'rouge-l'], max_n=2)\n",
    "    scores_nn = rouge.get_scores(output_sentences, summarized_text)\n",
    "    \n",
    "    rouge_1_nn = scores_nn['rouge-1']['f']\n",
    "    rouge_2_nn = scores_nn['rouge-2']['f']\n",
    "    rouge_l_nn = scores_nn['rouge-l']['f']\n",
    "    \n",
    "#     print('article number: %d' % (file_number))\n",
    "#     print('nn accuracy')\n",
    "#     print('Rouge 1 score is: %f' % (rouge_1_nn))\n",
    "#     print('Rouge 2 score is: %f' % (rouge_2_nn))\n",
    "#     print('Rouge l score is: %f' % (rouge_l_nn))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### classified using TF_idf score\n",
    "\n",
    "    output_tf_idf = summary_using_tf_idf_only(original_test, num_sentences_summarized)\n",
    "    \n",
    "    scores_tf_idf = rouge.get_scores(output_tf_idf, summarized_text)\n",
    "    \n",
    "    rouge_1_tf_idf = scores_tf_idf['rouge-1']['f']\n",
    "    rouge_2_tf_idf = scores_tf_idf['rouge-2']['f']\n",
    "    rouge_l_tf_idf = scores_tf_idf['rouge-l']['f']\n",
    "    \n",
    "#     print('t-idf accuracy')\n",
    "#     print('Rouge 1 score is: %f' % (rouge_1_tf_idf))\n",
    "#     print('Rouge 2 score is: %f' % (rouge_2_tf_idf))\n",
    "#     print('Rouge l score is: %f' % (rouge_l_tf_idf))\n",
    "\n",
    "#     print(summarized_text)\n",
    "#     print('-----')\n",
    "#     print(output_sentences)\n",
    "#     print('------------------------------')\n",
    "    \n",
    "    return rouge_1_nn, rouge_2_nn, rouge_l_nn, rouge_1_tf_idf, rouge_2_tf_idf, rouge_l_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c087f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_using_tf_idf_only(text, num_sentences):\n",
    "    article_sentences = sent_tokenize(text)\n",
    "    article_preprocessed = preprocessing(text)\n",
    "    sentence_scores = calculate_TF_IDF(article_preprocessed)\n",
    "    highest = np.argsort(sentence_scores) [::-1]\n",
    "    highest = highest[: num_sentences]\n",
    "    highest = sorted(highest) # uncomment to arrange the article\n",
    "    output_sentences = []\n",
    "    \n",
    "    for i in range (0, num_sentences):\n",
    "        output_sentences.append(article_sentences[highest[i]])\n",
    "        \n",
    "    \n",
    "    output_sentences = ' '.join(output_sentences)\n",
    "    \n",
    "    return output_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b8d2da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Madrid forward Cristiano Ronaldo has said that he is the \"best player in history\" after winning his fifth Ballon d'Or on Thursday. Ronaldo picked up the award for the second year in a row to equal the record of Barcelona star Lionel Messi, and he said he does not believe any player is better than him. He told France Football (h/t Goal's Robin Bairner): \"I've never seen anyone better than me. I play well with both feet, I’m quick, powerful, good with the head, I score goals, I make assists. That says something, doesn’t it? Legends like Floyd Mayweather [Jr.] and LeBron James don’t get to their perfect level by chance. To be at the top and to stay there, you have to have more talent than the others.\"\n"
     ]
    }
   ],
   "source": [
    "article_file = io.open(\"cr7.txt\", \"r\", encoding='utf-8-sig')\n",
    "article = article_file.read()\n",
    "article_file.close\n",
    "summary = summary_using_tf_idf_only(article, 7)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41f7af17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using nn\n",
      "Average Rouge 1 score is: 0.742477\n",
      "Average Rouge 2 score is: 0.660360\n",
      "Average Rouge l score is: 0.545702\n",
      "Using tf_idf only\n",
      "Average Rouge 1 score is: 0.371251\n",
      "Average Rouge 2 score is: 0.134294\n",
      "Average Rouge l score is: 0.213952\n"
     ]
    }
   ],
   "source": [
    "# precision_nn = []\n",
    "# recall_nn = []\n",
    "# precision_tf_idf = []\n",
    "# recall_tf_idf = []\n",
    "\n",
    "rouge_1_list_nn = []\n",
    "rouge_2_list_nn = []\n",
    "rouge_l_list_nn = []\n",
    "\n",
    "rouge_1_list_tf_idf = []\n",
    "rouge_2_list_tf_idf = []\n",
    "rouge_l_list_tf_idf = []\n",
    "\n",
    "article_types = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "\n",
    "for article_type in article_types:\n",
    "    for i in range(101, 131):\n",
    "\n",
    "        article_file = io.open(\"train_original/\" + article_type + \"/article (\" + str(i) +\").txt\", \"r\", encoding='utf-8-sig')\n",
    "        article_file.readline()\n",
    "        article = article_file.read()\n",
    "        article_preprocessed = preprocessing(article)\n",
    "        article_file.close()\n",
    "\n",
    "        summarized_file = io.open(\"train_summary/\" + article_type + \"/summary (\" + str(i) +\").txt\", \"r\", encoding='utf-8-sig')\n",
    "        summarized = summarized_file.read()\n",
    "        summarized_file.close()\n",
    "\n",
    "        rouge_1_nn, rouge_2_nn, rouge_l_nn, rouge_1_tf_idf, rouge_2_tf_idf, rouge_l_tf_idf = test(\n",
    "            article_preprocessed, article, summarized, 0.35, i)\n",
    "\n",
    "        rouge_1_list_nn.append(rouge_1_nn)\n",
    "        rouge_2_list_nn.append(rouge_2_nn)\n",
    "        rouge_l_list_nn.append(rouge_l_nn)\n",
    "\n",
    "        rouge_1_list_tf_idf.append(rouge_1_tf_idf)\n",
    "        rouge_2_list_tf_idf.append(rouge_2_tf_idf)\n",
    "        rouge_l_list_tf_idf.append(rouge_l_tf_idf)\n",
    "\n",
    "\n",
    "print('Using nn')\n",
    "print('Average Rouge 1 score is: %f' % (np.average(rouge_1_list_nn)))\n",
    "print('Average Rouge 2 score is: %f' % (np.average(rouge_2_list_nn)))\n",
    "print('Average Rouge l score is: %f' % (np.average(rouge_l_list_nn)))\n",
    "\n",
    "print('Using tf_idf only')\n",
    "print('Average Rouge 1 score is: %f' % (np.average(rouge_1_list_tf_idf)))\n",
    "print('Average Rouge 2 score is: %f' % (np.average(rouge_2_list_tf_idf)))\n",
    "print('Average Rouge l score is: %f' % (np.average(rouge_l_list_tf_idf)))\n",
    "\n",
    "# print('Neural network accuracy: ')\n",
    "# print('Average precision score is: %f' % (np.average(precision_nn)))\n",
    "# print('Average recall score is: %f' % (np.average(recall_nn)))\n",
    "      \n",
    "# print('Classical approach accuracy using tf-idf: ')\n",
    "# print('Average precision score is: %f' % (np.average(precision_tf_idf)))\n",
    "# print('Average recall score is: %f' % (np.average(recall_tf_idf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f249d2db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using nn\n",
      "Average Rouge 1 score is: 0.336225\n",
      "Average Rouge 2 score is: 0.138137\n",
      "Average Rouge l score is: 0.277759\n",
      "Using tf_idf only\n",
      "Average Rouge 1 score is: 0.264159\n",
      "Average Rouge 2 score is: 0.074046\n",
      "Average Rouge l score is: 0.209739\n"
     ]
    }
   ],
   "source": [
    "rouge_1_list_nn = []\n",
    "rouge_2_list_nn = []\n",
    "rouge_l_list_nn = []\n",
    "\n",
    "rouge_1_list_tf_idf = []\n",
    "rouge_2_list_tf_idf = []\n",
    "rouge_l_list_tf_idf = []\n",
    "\n",
    "df = pd.read_csv('test.csv')\n",
    "articles_cnn = df['article']\n",
    "summaries_cnn = df['highlights']\n",
    "\n",
    "i = 0\n",
    "for article_cnn, summary_cnn in zip(articles_cnn, summaries_cnn):\n",
    "    article_cnn_preprocessed = preprocessing(article_cnn)\n",
    "    rouge_1_nn, rouge_2_nn, rouge_l_nn, rouge_1_tf_idf, rouge_2_tf_idf, rouge_l_tf_idf = test(\n",
    "        article_cnn_preprocessed, article_cnn, summary_cnn, 0.35, i)\n",
    "    i += 1\n",
    "    if (i == 300):\n",
    "        break\n",
    "    \n",
    "    rouge_1_list_nn.append(rouge_1_nn)\n",
    "    rouge_2_list_nn.append(rouge_2_nn)\n",
    "    rouge_l_list_nn.append(rouge_l_nn)\n",
    "\n",
    "    rouge_1_list_tf_idf.append(rouge_1_tf_idf)\n",
    "    rouge_2_list_tf_idf.append(rouge_2_tf_idf)\n",
    "    rouge_l_list_tf_idf.append(rouge_l_tf_idf)\n",
    "\n",
    "print('Using nn')\n",
    "print('Average Rouge 1 score is: %f' % (np.average(rouge_1_list_nn)))\n",
    "print('Average Rouge 2 score is: %f' % (np.average(rouge_2_list_nn)))\n",
    "print('Average Rouge l score is: %f' % (np.average(rouge_l_list_nn)))\n",
    "\n",
    "print('Using tf_idf only')\n",
    "print('Average Rouge 1 score is: %f' % (np.average(rouge_1_list_tf_idf)))\n",
    "print('Average Rouge 2 score is: %f' % (np.average(rouge_2_list_tf_idf)))\n",
    "print('Average Rouge l score is: %f' % (np.average(rouge_l_list_tf_idf)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf2bc8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(article, compression_ratio):\n",
    "    original_sentences = sent_tokenize(article)\n",
    "    article_preprocessed_entered = preprocessing(article)\n",
    "    X_test_entered = generate_X_labels(article_preprocessed_entered)\n",
    "    summary_predicted = predict(model, X_test_entered)\n",
    "    num_sentences_summarized = math.ceil(compression_ratio * len(original_sentences))\n",
    "    \n",
    "    \n",
    "    highest = np.argsort(summary_predicted[0]) [::-1]\n",
    "    highest = highest[: num_sentences_summarized]\n",
    "    highest = sorted(highest) # uncomment to arrange the article\n",
    "    output_sentences = []\n",
    "    \n",
    "    for i in range (0, num_sentences_summarized):\n",
    "        output_sentences.append(original_sentences[highest[i]])\n",
    "        \n",
    "    \n",
    "    output_sentences = ' '.join(output_sentences)\n",
    "    \n",
    "    return output_sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a19c6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Madrid forward Cristiano Ronaldo has said that he is the \"best player in history\" after winning his fifth Ballon d'Or on Thursday. Ronaldo picked up the award for the second year in a row to equal the record of Barcelona star Lionel Messi, and he said he does not believe any player is better than him. He told France Football (h/t Goal's Robin Bairner): \"I've never seen anyone better than me. No footballer can do the things I can. \"There’s no player more complete than me. But I tell you: there’s no one more complete than me.\n"
     ]
    }
   ],
   "source": [
    "article_file = io.open(\"cr7.txt\", \"r\", encoding='utf-8-sig')\n",
    "article = article_file.read()\n",
    "article_file.close\n",
    "# print(article)\n",
    "summary = summarize(article, 0.35)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7fa25ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An actor dressed as the giant creature breathed smoke over photographers on Monday as Godzilla received the 2,271st star on Hollywood Boulevard. \"Godzilla should thank you for this historical and monumental star,\" said Final Wars producer Shogo Tomiyama. Hollywood's honorary mayor, Johnny Grant, said: \"I do hereby proclaim this Godzilla Day in Hollywood. The premiere of Godzilla: Final Wars at Grauman's Chinese Theatre followed the ceremony on Hollywood Boulevard. Director Ryuhei Kitamura said it may not be Godzilla's final outing, as it has been billed. And producer Shogo Tomiyama added: \"So long as Godzilla can fascinate people, I believe he will be resurrected by new generations of filmmakers in the future.\"\n"
     ]
    }
   ],
   "source": [
    "article_file = io.open(\"godzilla.txt\", \"r\", encoding='utf-8-sig')\n",
    "article = article_file.read()\n",
    "article_file.close\n",
    "# print(article)\n",
    "summary = summarize(article, 0.35)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9504be0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
