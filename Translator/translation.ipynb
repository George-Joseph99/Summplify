{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English to Arabic Translation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.ar import Arabic\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchtext import data\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>ar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>مرحبًا.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>اركض!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Help!</td>\n",
       "      <td>النجدة!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jump!</td>\n",
       "      <td>اقفز!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop!</td>\n",
       "      <td>قف!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24633</th>\n",
       "      <td>rising voices promoting a more linguistically ...</td>\n",
       "      <td>شاركنا تحدي ابداع ميم بلغتك الام تعزيزا للتنوع...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24634</th>\n",
       "      <td>following last year s successful campaign we i...</td>\n",
       "      <td>استكمالا لنجاح حملة العام السابق ندعوكم للمشار...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24635</th>\n",
       "      <td>during last year s challenge we also met langu...</td>\n",
       "      <td>تعرفنا خلال تحدي العام الماضي على ابطال لغويين...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24636</th>\n",
       "      <td>to take part just follow the simple steps outl...</td>\n",
       "      <td>للمشاركة في التحدي اتبع الخطوات الموضحة على ال...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24637</th>\n",
       "      <td>you will also find links to some free web base...</td>\n",
       "      <td>ستجد ايضا روابط لمجموعة من منصات ابداع الميم ا...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24407 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     eng  \\\n",
       "0                                                    Hi.   \n",
       "1                                                   Run!   \n",
       "2                                                  Help!   \n",
       "3                                                  Jump!   \n",
       "4                                                  Stop!   \n",
       "...                                                  ...   \n",
       "24633  rising voices promoting a more linguistically ...   \n",
       "24634  following last year s successful campaign we i...   \n",
       "24635  during last year s challenge we also met langu...   \n",
       "24636  to take part just follow the simple steps outl...   \n",
       "24637  you will also find links to some free web base...   \n",
       "\n",
       "                                                      ar  \n",
       "0                                                مرحبًا.  \n",
       "1                                                  اركض!  \n",
       "2                                                النجدة!  \n",
       "3                                                  اقفز!  \n",
       "4                                                    قف!  \n",
       "...                                                  ...  \n",
       "24633  شاركنا تحدي ابداع ميم بلغتك الام تعزيزا للتنوع...  \n",
       "24634  استكمالا لنجاح حملة العام السابق ندعوكم للمشار...  \n",
       "24635  تعرفنا خلال تحدي العام الماضي على ابطال لغويين...  \n",
       "24636  للمشاركة في التحدي اتبع الخطوات الموضحة على ال...  \n",
       "24637  ستجد ايضا روابط لمجموعة من منصات ابداع الميم ا...  \n",
       "\n",
       "[24407 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/arabic_english.txt\",delimiter=\"\\t\",names=[\"eng\",\"ar\"])\n",
    "# remove duplicates\n",
    "df = df.drop_duplicates(subset=['eng','ar'],keep='first')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eng': ['Let', \"'s\", 'just', 'be', 'friends', '.'], 'ar': ['دعنا', 'فقط', 'نصبح', 'أصدقاء']}\n"
     ]
    }
   ],
   "source": [
    "seed=42\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "arab = Arabic()\n",
    "ar_Tokenizer = Tokenizer(arab.vocab)\n",
    "\n",
    "def engTokenizer(text):\n",
    " return  [word.text for word in spacy_eng.tokenizer(text)] \n",
    "\n",
    "def arTokenizer(sentence):\n",
    "    return  [word.text for word in ar_Tokenizer(re.sub(r\"\\s+\",\" \",re.sub(r\"[\\.\\'\\\"\\n+]\",\" \",sentence)).strip())]\n",
    "\n",
    "SRC = data.Field(tokenize=engTokenizer,batch_first=False,init_token=\"<sos>\",eos_token=\"<eos>\")\n",
    "TRG = data.Field(tokenize=arTokenizer,batch_first=False,tokenizer_language=\"ar\",init_token=\"بداية\",eos_token=\"نهاية\")\n",
    "\n",
    "class TextDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, df, src_field, target_field, is_test=False, **kwargs):\n",
    "        fields = [('eng', src_field), ('ar',target_field)]\n",
    "        samples = []\n",
    "        for i, row in df.iterrows():\n",
    "            eng = row.eng \n",
    "            ar = row.ar\n",
    "            samples.append(data.Example.fromlist([eng, ar], fields))\n",
    "\n",
    "        super().__init__(samples, fields, **kwargs)\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "torchdataset = TextDataset(df,SRC,TRG)\n",
    "\n",
    "train_data, valid_data = torchdataset.split(split_ratio=0.8, random_state = random.seed(seed))\n",
    "\n",
    "SRC.build_vocab(train_data,min_freq=2)\n",
    "TRG.build_vocab(train_data,min_freq=2)\n",
    "\n",
    "print(train_data[1].__dict__)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seting up the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NVIDIA GeForce RTX 3080 Ti\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,embedding_size,src_vocab_size,trg_vocab_size,src_pad_idx,num_heads,num_encoder_layers,num_decoder_layers,max_len,):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_embeddings = nn.Embedding(src_vocab_size,embedding_size)\n",
    "        self.src_positional_embeddings= nn.Embedding(max_len,embedding_size)\n",
    "        self.trg_embeddings= nn.Embedding(trg_vocab_size,embedding_size)\n",
    "        self.trg_positional_embeddings= nn.Embedding(max_len,embedding_size)\n",
    "        self.device = device\n",
    "        self.transformer = nn.Transformer(embedding_size,num_heads,num_encoder_layers,num_decoder_layers,)\n",
    "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "    \n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = src.transpose(0,1) == self.src_pad_idx\n",
    "        return src_mask.to(device)\n",
    "\n",
    "    def forward(self,src,trg) :\n",
    "        src_seq_length, S = src.shape\n",
    "        trg_seq_length, S = trg.shape\n",
    "        #adding zeros is an easy way\n",
    "        src_positions = (torch.arange(0, src_seq_length).unsqueeze(1).expand(src_seq_length, S).to(self.device))\n",
    "        trg_positions = (torch.arange(0, trg_seq_length).unsqueeze(1).expand(trg_seq_length, S).to(self.device))\n",
    "        embed_src  = self.dropout(( self.src_embeddings(src) + self.src_positional_embeddings(src_positions)))\n",
    "        embed_trg = self.dropout(( self.trg_embeddings(trg) + self.trg_positional_embeddings(trg_positions)))\n",
    "        src_padding_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(device)\n",
    "        out = self.transformer(embed_src,embed_trg, src_key_padding_mask=src_padding_mask,tgt_mask=trg_mask )\n",
    "        out= self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter = data.BucketIterator.splits((train_data,valid_data), batch_size = BATCH_SIZE,sort=None,sort_within_batch=False,sort_key=lambda x: len(x.eng),device=device,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of english vocabulary: 12766\n",
      "Size of arabic vocabulary: 21981\n"
     ]
    }
   ],
   "source": [
    "load_model = False\n",
    "save_model = True\n",
    "BATCH_SIZE = 16 \n",
    "learning_rate = 0.0001\n",
    "num_epochs = 30\n",
    "# Model\n",
    "num_heads = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "max_len= 230\n",
    "dropout = 0.4\n",
    "embedding_size= 256\n",
    "src_pad_idx = SRC.vocab.stoi[\"<pad>\"]\n",
    "src_vocab_size  = len(SRC.vocab)\n",
    "print(\"Size of english vocabulary:\",src_vocab_size)\n",
    "trg_vocab_size =len(TRG.vocab)\n",
    "print(\"Size of arabic vocabulary:\",trg_vocab_size)\n",
    "model = Transformer(embedding_size,src_vocab_size,trg_vocab_size,src_pad_idx,num_heads,num_encoder_layers,num_decoder_layers,max_len,).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (src_embeddings): Embedding(12766, 256)\n",
      "  (src_positional_embeddings): Embedding(230, 256)\n",
      "  (trg_embeddings): Embedding(21981, 256)\n",
      "  (trg_positional_embeddings): Embedding(230, 256)\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=256, out_features=21981, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 0 | Train Cross Entropy Loss:  7.486914330091172\n",
      " Epoch 0 | Validation Cross Entropy Loss:  6.143818473504259\n",
      " Epoch 1 | Train Cross Entropy Loss:  7.000222254541446\n",
      " Epoch 1 | Validation Cross Entropy Loss:  5.8936981999017055\n",
      " Epoch 2 | Train Cross Entropy Loss:  6.728391125200226\n",
      " Epoch 2 | Validation Cross Entropy Loss:  5.72275688445646\n",
      " Epoch 3 | Train Cross Entropy Loss:  6.478139531309735\n",
      " Epoch 3 | Validation Cross Entropy Loss:  5.572155557426751\n",
      " Epoch 4 | Train Cross Entropy Loss:  6.237511396212816\n",
      " Epoch 4 | Validation Cross Entropy Loss:  5.474729069697312\n",
      " Epoch 5 | Train Cross Entropy Loss:  5.997134029035388\n",
      " Epoch 5 | Validation Cross Entropy Loss:  5.367053468242969\n",
      " Epoch 6 | Train Cross Entropy Loss:  5.768872600027423\n",
      " Epoch 6 | Validation Cross Entropy Loss:  5.364213545338001\n",
      " Epoch 7 | Train Cross Entropy Loss:  5.5389819219403345\n",
      " Epoch 7 | Validation Cross Entropy Loss:  5.30028329955207\n",
      " Epoch 8 | Train Cross Entropy Loss:  5.3093830611645245\n",
      " Epoch 8 | Validation Cross Entropy Loss:  5.1406835231905665\n",
      " Epoch 9 | Train Cross Entropy Loss:  5.080482178101473\n",
      " Epoch 9 | Validation Cross Entropy Loss:  5.191229821809756\n",
      " Epoch 10 | Train Cross Entropy Loss:  4.850972686411414\n",
      " Epoch 10 | Validation Cross Entropy Loss:  5.082766057619082\n",
      " Epoch 11 | Train Cross Entropy Loss:  4.621678598194607\n",
      " Epoch 11 | Validation Cross Entropy Loss:  5.056883636642905\n",
      " Epoch 12 | Train Cross Entropy Loss:  4.3990248311844935\n",
      " Epoch 12 | Validation Cross Entropy Loss:  5.031383307151545\n",
      " Epoch 13 | Train Cross Entropy Loss:  4.178118045949038\n",
      " Epoch 13 | Validation Cross Entropy Loss:  5.070484564195271\n",
      " Epoch 14 | Train Cross Entropy Loss:  3.9589483536822594\n",
      " Epoch 14 | Validation Cross Entropy Loss:  5.128305997723848\n",
      " Epoch 15 | Train Cross Entropy Loss:  3.739143842951769\n",
      " Epoch 15 | Validation Cross Entropy Loss:  5.027944973091674\n",
      " Epoch 16 | Train Cross Entropy Loss:  3.529170953475677\n",
      " Epoch 16 | Validation Cross Entropy Loss:  5.113830371619829\n",
      " Epoch 17 | Train Cross Entropy Loss:  3.3316374259537893\n",
      " Epoch 17 | Validation Cross Entropy Loss:  5.098091434809118\n",
      " Epoch 18 | Train Cross Entropy Loss:  3.1388869493540734\n",
      " Epoch 18 | Validation Cross Entropy Loss:  5.154383510545967\n",
      " Epoch 19 | Train Cross Entropy Loss:  2.939881731797983\n",
      " Epoch 19 | Validation Cross Entropy Loss:  5.092863342341254\n",
      " Epoch 20 | Train Cross Entropy Loss:  2.7684582278535173\n",
      " Epoch 20 | Validation Cross Entropy Loss:  5.158184610161126\n",
      " Epoch 21 | Train Cross Entropy Loss:  2.6041164191212447\n",
      " Epoch 21 | Validation Cross Entropy Loss:  5.209176486613703\n",
      " Epoch 22 | Train Cross Entropy Loss:  2.447495502004069\n",
      " Epoch 22 | Validation Cross Entropy Loss:  5.232866747706544\n",
      " Epoch 23 | Train Cross Entropy Loss:  2.3053788532496085\n",
      " Epoch 23 | Validation Cross Entropy Loss:  5.31128133823669\n",
      " Epoch 24 | Train Cross Entropy Loss:  2.1669174911153797\n",
      " Epoch 24 | Validation Cross Entropy Loss:  5.225600468566994\n",
      " Epoch 25 | Train Cross Entropy Loss:  2.0412457463782308\n",
      " Epoch 25 | Validation Cross Entropy Loss:  5.40032319305769\n",
      " Epoch 26 | Train Cross Entropy Loss:  1.9153327342919109\n",
      " Epoch 26 | Validation Cross Entropy Loss:  5.4091040947858025\n",
      " Epoch 27 | Train Cross Entropy Loss:  1.8057676164172498\n",
      " Epoch 27 | Validation Cross Entropy Loss:  5.544270340134116\n",
      " Epoch 28 | Train Cross Entropy Loss:  1.7039113206320566\n",
      " Epoch 28 | Validation Cross Entropy Loss:  5.532710856082392\n",
      " Epoch 29 | Train Cross Entropy Loss:  1.6101107705234994\n",
      " Epoch 29 | Validation Cross Entropy Loss:  5.643152144999286\n",
      " Epoch 30 | Train Cross Entropy Loss:  1.518437470694627\n",
      " Epoch 30 | Validation Cross Entropy Loss:  5.642103583984126\n",
      " Epoch 31 | Train Cross Entropy Loss:  1.4400806927856709\n",
      " Epoch 31 | Validation Cross Entropy Loss:  5.711102689792908\n",
      " Epoch 32 | Train Cross Entropy Loss:  1.3628172521509174\n",
      " Epoch 32 | Validation Cross Entropy Loss:  5.82501331341812\n",
      " Epoch 33 | Train Cross Entropy Loss:  1.2913095530284222\n",
      " Epoch 33 | Validation Cross Entropy Loss:  5.8899886911990595\n",
      " Epoch 34 | Train Cross Entropy Loss:  1.2261339058942429\n",
      " Epoch 34 | Validation Cross Entropy Loss:  5.872735921074362\n",
      " Epoch 35 | Train Cross Entropy Loss:  1.1609603656207217\n",
      " Epoch 35 | Validation Cross Entropy Loss:  6.027707223798714\n",
      " Epoch 36 | Train Cross Entropy Loss:  1.1022386800354373\n",
      " Epoch 36 | Validation Cross Entropy Loss:  6.010118933284984\n",
      " Epoch 37 | Train Cross Entropy Loss:  1.0508429803973236\n",
      " Epoch 37 | Validation Cross Entropy Loss:  6.108438223016028\n",
      " Epoch 38 | Train Cross Entropy Loss:  0.9984076437063631\n",
      " Epoch 38 | Validation Cross Entropy Loss:  6.272954990661222\n",
      " Epoch 39 | Train Cross Entropy Loss:  0.954862195385176\n",
      " Epoch 39 | Validation Cross Entropy Loss:  6.18510736593234\n"
     ]
    }
   ],
   "source": [
    "loss_track = []\n",
    "loss_validation_track= []\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "pad_idx = SRC.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
    "for epoch in range(num_epochs):\n",
    "    stepLoss=[]\n",
    "    model.train()\n",
    "    for batch  in train_iter:\n",
    "        input_data = batch.eng.to(device)\n",
    "        target = batch.ar.to(device)\n",
    "        output = model(input_data,target[:-1])\n",
    "        optimizer.zero_grad()       \n",
    "        output = output.reshape(-1,trg_vocab_size)\n",
    "        target = target[1:].reshape(-1)\n",
    "        loss = criterion(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        stepLoss.append(loss.item())\n",
    "    loss_track.append(np.mean(stepLoss))\n",
    "    print(\" Epoch {} | Train Cross Entropy Loss: \".format(epoch),np.mean(stepLoss))\n",
    "    with torch.no_grad():    \n",
    "      stepValidLoss=[]\n",
    "      model.eval() # the evaluation mode for the model (doesn't apply dropout and batchNorm)\n",
    "      for i,batch  in enumerate(valid_iter):\n",
    "            input_sentence = batch.eng.to(device)\n",
    "            target = batch.ar.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_sentence,target[:-1])\n",
    "            output = output.reshape(-1,trg_vocab_size)\n",
    "            target = target[1:].reshape(-1)\n",
    "            loss = criterion(output,target)             \n",
    "            stepValidLoss.append(loss.item())  \n",
    "    loss_validation_track.append(np.mean(stepValidLoss))\n",
    "    print(\" Epoch {} | Validation Cross Entropy Loss: \".format(epoch),np.mean(stepValidLoss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(30),loss_track,label=\"train loss\")\n",
    "plt.plot(range(30),loss_validation_track,label=\"valiadtion loss\")\n",
    "plt.legend()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model,sentence,srcField,targetField,srcTokenizer):\n",
    "    model.eval()\n",
    "    processed_sentence = srcField.process([srcTokenizer(sentence)]).to(device)\n",
    "    trg = [\"بداية\"]\n",
    "    for _ in range(60):\n",
    "        trg_indecies = [targetField.vocab.stoi[word] for word in trg]\n",
    "        trg_tensor = torch.LongTensor(trg_indecies).unsqueeze(1).to(device)\n",
    "        outputs = model(processed_sentence,trg_tensor)    \n",
    "        if targetField.vocab.itos[outputs.argmax(2)[-1:].item()] == \"<unk>\":\n",
    "            continue \n",
    "        trg.append(targetField.vocab.itos[outputs.argmax(2)[-1:].item()])\n",
    "        if targetField.vocab.itos[outputs.argmax(2)[-1:].item()] == \"نهاية\":\n",
    "            break\n",
    "    return \" \".join([word for word in trg if word != \"<unk>\"][1:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'أنا'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_sentence(model,\"I am ready\" ,SRC,TRG,engTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'انا اري'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_sentence(model,\"I am lucky\" ,SRC,TRG,engTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_sentence(model,\"I am sad\" ,SRC,TRG,engTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'انا سعيد سعيد'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_sentence(model,\"I am happy\" ,SRC,TRG,engTokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_sentence(model,\"I am angry\" ,SRC,TRG,engTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_sentence(model,\"I am tired\" ,SRC,TRG,engTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'أشعر بالجوع'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_sentence(model,\"I am hungry\" ,SRC,TRG,engTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_sentence(model,\"I am going outside\" ,SRC,TRG,engTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_sentence(model,\"I am going to the market\" ,SRC,TRG,engTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'هنا'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_sentence(model,\"He is here\" ,SRC,TRG,engTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ليس هنا'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_sentence(model,\"He is not here\" ,SRC,TRG,engTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'لا يوجد في المنزل'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_sentence(model,\"I am not at home\" ,SRC,TRG,engTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مجلة العالم الإسلامي لحقوق الإنسان للأصوات العالمية لحقوق الإنسان.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-ar\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "english_text = \"egypt muslim world journal of human rights global voices.\"\n",
    "\n",
    "# Tokenize the input\n",
    "encoded_input = tokenizer.encode(english_text, return_tensors=\"pt\")\n",
    "\n",
    "# Translate the input\n",
    "translated = model.generate(encoded_input)\n",
    "\n",
    "# Decode the translated output\n",
    "arabic_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "print(arabic_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training dataset: 19710\n",
      "length of validation dataset: 4928\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer, AdamW\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Read and process the dataset file\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                pair = line.strip().split(\"\\t\")\n",
    "                if len(pair) == 2:\n",
    "                    source_text = pair[0].strip()\n",
    "                    target_text = pair[1].strip()\n",
    "                    self.data.append((source_text, target_text))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_text, target_text = self.data[idx]\n",
    "        source_inputs = self.tokenizer.encode(source_text, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "        target_inputs = self.tokenizer.encode(target_text, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "        return source_inputs.squeeze(), target_inputs.squeeze()\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Set up the model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-ar\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "# Prepare your training dataset\n",
    "# split the dataset into train and validation sets\n",
    "train_dataset = TranslationDataset(\"data/arabic_english.txt\", tokenizer)\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "valid_size = len(train_dataset) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n",
    "print(\"length of training dataset:\", len(train_dataset))\n",
    "print(\"length of validation dataset:\", len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "c:\\Users\\pc\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Batch 100 - Loss: 3.2364\n",
      "Epoch 1/10 - Batch 200 - Loss: 1.9338\n",
      "Epoch 1/10 - Batch 300 - Loss: 1.3395\n",
      "Epoch 1/10 - Batch 400 - Loss: 1.2951\n",
      "Epoch 1/10 - Batch 500 - Loss: 1.0964\n",
      "Epoch 1/10 - Batch 600 - Loss: 1.0777\n",
      "Epoch 1/10 - Batch 700 - Loss: 1.0991\n",
      "Epoch 1/10 - Batch 800 - Loss: 0.9989\n",
      "Epoch 1/10 - Batch 900 - Loss: 0.9804\n",
      "Epoch 1/10 - Batch 1000 - Loss: 0.9773\n",
      "Epoch 1/10 - Batch 1100 - Loss: 0.9402\n",
      "Epoch 1/10 - Batch 1200 - Loss: 0.8626\n",
      "Epoch 1/10 - Batch 1300 - Loss: 0.9564\n",
      "Epoch 1/10 - Batch 1400 - Loss: 0.7271\n",
      "Epoch 1/10 - Batch 1500 - Loss: 0.8559\n",
      "Epoch 1/10 - Batch 1600 - Loss: 0.8294\n",
      "Epoch 1/10 - Batch 1700 - Loss: 0.7642\n",
      "Epoch 1/10 - Batch 1800 - Loss: 0.8184\n",
      "Epoch 1/10 - Batch 1900 - Loss: 0.8525\n",
      "Epoch 1/10 - Batch 2000 - Loss: 0.8868\n",
      "Epoch 1/10 - Batch 2100 - Loss: 0.6263\n",
      "Epoch 1/10 - Batch 2200 - Loss: 0.8006\n",
      "Epoch 1/10 - Batch 2300 - Loss: 0.8213\n",
      "Epoch 1/10 - Batch 2400 - Loss: 0.8269\n",
      "Epoch 1/10 - Batch 2500 - Loss: 0.6837\n",
      "Epoch 1/10 - Batch 2600 - Loss: 0.8656\n",
      "Epoch 1/10 - Batch 2700 - Loss: 0.8368\n",
      "Epoch 1/10 - Batch 2800 - Loss: 0.8486\n",
      "Epoch 1/10 - Batch 2900 - Loss: 0.6660\n",
      "Epoch 1/10 - Batch 3000 - Loss: 0.7051\n",
      "Epoch 1/10 - Batch 3100 - Loss: 0.7620\n",
      "Epoch 1/10 - Batch 3200 - Loss: 0.8444\n",
      "Epoch 1/10 - Batch 3300 - Loss: 0.8803\n",
      "Epoch 1/10 - Batch 3400 - Loss: 0.6523\n",
      "Epoch 1/10 - Batch 3500 - Loss: 0.8225\n",
      "Epoch 1/10 - Batch 3600 - Loss: 0.7046\n",
      "Epoch 1/10 - Batch 3700 - Loss: 0.7949\n",
      "Epoch 1/10 - Batch 3800 - Loss: 0.8973\n",
      "Epoch 1/10 - Batch 3900 - Loss: 0.6597\n",
      "Epoch 1/10 - Batch 4000 - Loss: 0.7106\n",
      "Epoch 1/10 - Batch 4100 - Loss: 0.7720\n",
      "Epoch 1/10 - Batch 4200 - Loss: 0.8646\n",
      "Epoch 1/10 - Batch 4300 - Loss: 0.7367\n",
      "Epoch 1/10 - Batch 4400 - Loss: 0.7710\n",
      "Epoch 1/10 - Batch 4500 - Loss: 0.7552\n",
      "Epoch 1/10 - Batch 4600 - Loss: 0.7147\n",
      "Epoch 1/10 - Batch 4700 - Loss: 0.7213\n",
      "Epoch 1/10 - Batch 4800 - Loss: 0.5811\n",
      "Epoch 1/10 - Batch 4900 - Loss: 0.7964\n",
      "Epoch 1/10 - Batch 5000 - Loss: 0.7756\n",
      "Epoch 1/10 - Batch 5100 - Loss: 0.6981\n",
      "Epoch 1/10 - Batch 5200 - Loss: 0.7892\n",
      "Epoch 1/10 - Batch 5300 - Loss: 0.7057\n",
      "Epoch 1/10 - Batch 5400 - Loss: 0.7383\n",
      "Epoch 1/10 - Batch 5500 - Loss: 0.6746\n",
      "Epoch 1/10 - Batch 5600 - Loss: 0.6031\n",
      "Epoch 1/10 - Batch 5700 - Loss: 0.7170\n",
      "Epoch 1/10 - Batch 5800 - Loss: 0.7473\n",
      "Epoch 1/10 - Batch 5900 - Loss: 0.6818\n",
      "Epoch 1/10 - Batch 6000 - Loss: 0.7269\n",
      "Epoch 1/10 - Batch 6100 - Loss: 0.8268\n",
      "Epoch 1/10 - Batch 6200 - Loss: 0.6822\n",
      "Epoch 1/10 - Batch 6300 - Loss: 0.7052\n",
      "Epoch 1/10 - Batch 6400 - Loss: 0.7301\n",
      "Epoch 1/10 - Batch 6500 - Loss: 0.7341\n",
      "Epoch 1/10 - Batch 6600 - Loss: 0.6814\n",
      "Epoch 1/10 - Batch 6700 - Loss: 0.8095\n",
      "Epoch 1/10 - Batch 6800 - Loss: 0.7665\n",
      "Epoch 1/10 - Batch 6900 - Loss: 0.6942\n",
      "Epoch 1/10 - Batch 7000 - Loss: 0.6398\n",
      "Epoch 1/10 - Batch 7100 - Loss: 0.7561\n",
      "Epoch 1/10 - Batch 7200 - Loss: 0.7054\n",
      "Epoch 1/10 - Batch 7300 - Loss: 0.6868\n",
      "Epoch 1/10 - Batch 7400 - Loss: 0.7155\n",
      "Epoch 1/10 - Batch 7500 - Loss: 0.6671\n",
      "Epoch 1/10 - Batch 7600 - Loss: 0.6826\n",
      "Epoch 1/10 - Batch 7700 - Loss: 0.7654\n",
      "Epoch 1/10 - Batch 7800 - Loss: 0.6938\n",
      "Epoch 1/10 - Batch 7900 - Loss: 0.6740\n",
      "Epoch 1/10 - Batch 8000 - Loss: 0.7686\n",
      "Epoch 1/10 - Batch 8100 - Loss: 0.6293\n",
      "Epoch 1/10 - Batch 8200 - Loss: 0.7393\n",
      "Epoch 1/10 - Batch 8300 - Loss: 0.6951\n",
      "Epoch 1/10 - Batch 8400 - Loss: 0.6540\n",
      "Epoch 1/10 - Batch 8500 - Loss: 0.6450\n",
      "Epoch 1/10 - Batch 8600 - Loss: 0.7090\n",
      "Epoch 1/10 - Batch 8700 - Loss: 0.7280\n",
      "Epoch 1/10 - Batch 8800 - Loss: 0.6957\n",
      "Epoch 1/10 - Batch 8900 - Loss: 0.7081\n",
      "Epoch 1/10 - Batch 9000 - Loss: 0.7056\n",
      "Epoch 1/10 - Batch 9100 - Loss: 0.7227\n",
      "Epoch 1/10 - Batch 9200 - Loss: 0.6563\n",
      "Epoch 1/10 - Batch 9300 - Loss: 0.6670\n",
      "Epoch 1/10 - Batch 9400 - Loss: 0.7238\n",
      "Epoch 1/10 - Batch 9500 - Loss: 0.7410\n",
      "Epoch 1/10 - Batch 9600 - Loss: 0.7612\n",
      "Epoch 1/10 - Batch 9700 - Loss: 0.7100\n",
      "Epoch 1/10 - Batch 9800 - Loss: 0.6674\n",
      "Epoch 1/10 - Batch 9900 - Loss: 0.7031\n",
      "Epoch 1/10 - Batch 10000 - Loss: 0.7330\n",
      "Epoch 1/10 - Batch 10100 - Loss: 0.7928\n",
      "Epoch 1/10 - Batch 10200 - Loss: 0.7349\n",
      "Epoch 1/10 - Batch 10300 - Loss: 0.7840\n",
      "Epoch 1/10 - Batch 10400 - Loss: 0.7259\n",
      "Epoch 1/10 - Batch 10500 - Loss: 0.7663\n",
      "Epoch 1/10 - Batch 10600 - Loss: 0.8229\n",
      "Epoch 1/10 - Batch 10700 - Loss: 0.8524\n",
      "Epoch 1/10 - Batch 10800 - Loss: 0.8171\n",
      "Epoch 1/10 - Batch 10900 - Loss: 0.8314\n",
      "Epoch 1/10 - Batch 11000 - Loss: 0.9345\n",
      "Epoch 1/10 - Batch 11100 - Loss: 1.5388\n",
      "Epoch 1/10 - Batch 11200 - Loss: 1.4991\n",
      "Epoch 1/10 - Batch 11300 - Loss: 1.6550\n",
      "Epoch 1/10 - Batch 11400 - Loss: 1.4603\n",
      "Epoch 1/10 - Batch 11500 - Loss: 1.4591\n",
      "Epoch 1/10 - Batch 11600 - Loss: 1.5956\n",
      "Epoch 1/10 - Batch 11700 - Loss: 1.4246\n",
      "Epoch 1/10 - Batch 11800 - Loss: 1.4231\n",
      "Epoch 1/10 - Batch 11900 - Loss: 1.4673\n",
      "Epoch 1/10 - Batch 12000 - Loss: 1.5344\n",
      "Epoch 1/10 - Batch 12100 - Loss: 1.4133\n",
      "Epoch 1/10 - Batch 12200 - Loss: 1.2900\n",
      "Epoch 1/10 - Batch 12300 - Loss: 1.3725\n",
      "Epoch 1/10 - Batch 12400 - Loss: 1.2577\n",
      "Epoch 1/10 - Batch 12500 - Loss: 1.5067\n",
      "Epoch 1/10 - Batch 12600 - Loss: 1.2828\n",
      "Epoch 1/10 - Batch 12700 - Loss: 1.3282\n",
      "Epoch 1/10 - Batch 12800 - Loss: 1.4663\n",
      "Epoch 1/10 - Batch 12900 - Loss: 1.1627\n",
      "Epoch 1/10 - Batch 13000 - Loss: 1.2872\n",
      "Epoch 1/10 - Batch 13100 - Loss: 1.3546\n",
      "Epoch 1/10 - Batch 13200 - Loss: 1.5102\n",
      "Epoch 1/10 - Batch 13300 - Loss: 1.3300\n",
      "Epoch 1/10 - Batch 13400 - Loss: 1.2951\n",
      "Epoch 1/10 - Batch 13500 - Loss: 1.4998\n",
      "Epoch 1/10 - Batch 13600 - Loss: 1.2275\n",
      "Epoch 1/10 - Batch 13700 - Loss: 1.2271\n",
      "Epoch 1/10 - Batch 13800 - Loss: 1.3069\n",
      "Epoch 1/10 - Batch 13900 - Loss: 1.2480\n",
      "Epoch 1/10 - Batch 14000 - Loss: 1.3681\n",
      "Epoch 1/10 - Batch 14100 - Loss: 1.2591\n",
      "Epoch 1/10 - Batch 14200 - Loss: 1.2459\n",
      "Epoch 1/10 - Batch 14300 - Loss: 1.2624\n",
      "Epoch 1/10 - Batch 14400 - Loss: 1.2170\n",
      "Epoch 1/10 - Batch 14500 - Loss: 1.1867\n",
      "Epoch 1/10 - Batch 14600 - Loss: 1.4613\n",
      "Epoch 1/10 - Batch 14700 - Loss: 1.1867\n",
      "Epoch 1/10 - Batch 14800 - Loss: 1.2618\n",
      "Epoch 1/10 - Batch 14900 - Loss: 1.2500\n",
      "Epoch 1/10 - Batch 15000 - Loss: 1.2705\n",
      "Epoch 1/10 - Batch 15100 - Loss: 1.3199\n",
      "Epoch 1/10 - Batch 15200 - Loss: 1.3591\n",
      "Epoch 1/10 - Batch 15300 - Loss: 1.2355\n",
      "Epoch 1/10 - Batch 15400 - Loss: 1.1366\n",
      "Epoch 1/10 - Batch 15500 - Loss: 1.1184\n",
      "Epoch 1/10 - Batch 15600 - Loss: 1.3909\n",
      "Epoch 1/10 - Batch 15700 - Loss: 1.3755\n",
      "Epoch 1/10 - Batch 15800 - Loss: 1.1695\n",
      "Epoch 1/10 - Batch 15900 - Loss: 1.0450\n",
      "Epoch 1/10 - Batch 16000 - Loss: 1.2146\n",
      "Epoch 1/10 - Batch 16100 - Loss: 1.1430\n",
      "Epoch 1/10 - Batch 16200 - Loss: 1.4463\n",
      "Epoch 1/10 - Batch 16300 - Loss: 1.3461\n",
      "Epoch 1/10 - Batch 16400 - Loss: 1.4437\n",
      "Epoch 1/10 - Batch 16500 - Loss: 1.1876\n",
      "Epoch 1/10 - Batch 16600 - Loss: 1.3229\n",
      "Epoch 1/10 - Batch 16700 - Loss: 1.1612\n",
      "Epoch 1/10 - Batch 16800 - Loss: 1.5161\n",
      "Epoch 1/10 - Batch 16900 - Loss: 1.3558\n",
      "Epoch 1/10 - Batch 17000 - Loss: 1.2131\n",
      "Epoch 1/10 - Batch 17100 - Loss: 1.3799\n",
      "Epoch 1/10 - Batch 17200 - Loss: 1.0582\n",
      "Epoch 1/10 - Batch 17300 - Loss: 1.1819\n",
      "Epoch 1/10 - Batch 17400 - Loss: 1.1298\n",
      "Epoch 1/10 - Batch 17500 - Loss: 1.2267\n",
      "Epoch 1/10 - Batch 17600 - Loss: 1.3462\n",
      "Epoch 1/10 - Batch 17700 - Loss: 1.1334\n",
      "Epoch 1/10 - Batch 17800 - Loss: 1.2212\n",
      "Epoch 1/10 - Batch 17900 - Loss: 1.2588\n",
      "Epoch 1/10 - Batch 18000 - Loss: 1.0204\n",
      "Epoch 1/10 - Batch 18100 - Loss: 1.0494\n",
      "Epoch 1/10 - Batch 18200 - Loss: 1.0591\n",
      "Epoch 1/10 - Batch 18300 - Loss: 1.1941\n",
      "Epoch 1/10 - Batch 18400 - Loss: 1.2009\n",
      "Epoch 1/10 - Batch 18500 - Loss: 1.0426\n",
      "Epoch 1/10 - Batch 18600 - Loss: 1.0322\n",
      "Epoch 1/10 - Batch 18700 - Loss: 1.0549\n",
      "Epoch 1/10 - Batch 18800 - Loss: 0.8813\n",
      "Epoch 1/10 - Batch 18900 - Loss: 1.0836\n",
      "Epoch 1/10 - Batch 19000 - Loss: 0.8973\n",
      "Epoch 1/10 - Batch 19100 - Loss: 0.8528\n",
      "Epoch 1/10 - Batch 19200 - Loss: 0.7863\n",
      "Epoch 1/10 - Batch 19300 - Loss: 1.0370\n",
      "Epoch 1/10 - Batch 19400 - Loss: 0.8702\n",
      "Epoch 1/10 - Batch 19500 - Loss: 0.7558\n",
      "Epoch 1/10 - Batch 19600 - Loss: 0.8893\n",
      "Epoch 1/10 - Batch 19700 - Loss: 0.8845\n",
      "Epoch 1/10 - Batch 19800 - Loss: 0.9511\n",
      "Epoch 1/10 - Batch 19900 - Loss: 0.9895\n",
      "Epoch 1/10 - Batch 20000 - Loss: 1.0551\n",
      "Epoch 1/10 - Batch 20100 - Loss: 0.7586\n",
      "Epoch 1/10 - Batch 20200 - Loss: 0.8279\n",
      "Epoch 1/10 - Batch 20300 - Loss: 1.2962\n",
      "Epoch 1/10 - Batch 20400 - Loss: 0.8306\n",
      "Epoch 1/10 - Batch 20500 - Loss: 0.6179\n",
      "Epoch 1/10 - Batch 20600 - Loss: 0.9973\n",
      "Epoch 1/10 - Batch 20700 - Loss: 0.9144\n",
      "Epoch 1/10 - Batch 20800 - Loss: 0.9470\n",
      "Epoch 1/10 - Batch 20900 - Loss: 1.3633\n",
      "Epoch 1/10 - Batch 21000 - Loss: 1.1932\n",
      "Epoch 1/10 - Batch 21100 - Loss: 1.2954\n",
      "Epoch 1/10 - Batch 21200 - Loss: 1.3789\n",
      "Epoch 1/10 - Batch 21300 - Loss: 1.2017\n",
      "Epoch 1/10 - Batch 21400 - Loss: 1.3394\n",
      "Epoch 1/10 - Batch 21500 - Loss: 1.2345\n",
      "Epoch 1/10 - Batch 21600 - Loss: 1.2488\n",
      "Epoch 1/10 - Batch 21700 - Loss: 1.2301\n",
      "Epoch 1/10 - Batch 21800 - Loss: 1.1942\n",
      "Epoch 1/10 - Batch 21900 - Loss: 1.3102\n",
      "Epoch 1/10 - Batch 22000 - Loss: 1.0244\n",
      "Epoch 1/10 - Batch 22100 - Loss: 1.0946\n",
      "Epoch 1/10 - Batch 22200 - Loss: 1.3185\n",
      "Epoch 1/10 - Batch 22300 - Loss: 1.1449\n",
      "Epoch 1/10 - Batch 22400 - Loss: 1.0307\n",
      "Epoch 1/10 - Batch 22500 - Loss: 1.2430\n",
      "Epoch 1/10 - Batch 22600 - Loss: 0.9875\n",
      "Epoch 1/10 - Batch 22700 - Loss: 1.0231\n",
      "Epoch 1/10 - Batch 22800 - Loss: 1.2644\n",
      "Epoch 1/10 - Batch 22900 - Loss: 0.9628\n",
      "Epoch 1/10 - Batch 23000 - Loss: 1.2153\n",
      "Epoch 1/10 - Batch 23100 - Loss: 1.2584\n",
      "Epoch 1/10 - Batch 23200 - Loss: 1.2507\n",
      "Epoch 1/10 - Batch 23300 - Loss: 1.1329\n",
      "Epoch 1/10 - Batch 23400 - Loss: 1.0995\n",
      "Epoch 1/10 - Batch 23500 - Loss: 1.5051\n",
      "Epoch 1/10 - Batch 23600 - Loss: 1.2090\n",
      "Epoch 1/10 - Batch 23700 - Loss: 1.0930\n",
      "Epoch 1/10 - Batch 23800 - Loss: 1.2384\n",
      "Epoch 1/10 - Batch 23900 - Loss: 1.1671\n",
      "Epoch 1/10 - Batch 24000 - Loss: 1.2207\n",
      "Epoch 1/10 - Batch 24100 - Loss: 1.1264\n",
      "Epoch 1/10 - Batch 24200 - Loss: 0.9935\n",
      "Epoch 1/10 - Batch 24300 - Loss: 1.2266\n",
      "Epoch 1/10 - Batch 24400 - Loss: 0.8205\n",
      "Epoch 1/10 - Batch 24500 - Loss: 1.2498\n",
      "Epoch 1/10 - Batch 24600 - Loss: 1.1861\n",
      "Epoch 2/10 - Batch 24700 - Loss: 1.1322\n",
      "Epoch 2/10 - Batch 24800 - Loss: 0.7978\n",
      "Epoch 2/10 - Batch 24900 - Loss: 0.7159\n",
      "Epoch 2/10 - Batch 25000 - Loss: 0.6119\n",
      "Epoch 2/10 - Batch 25100 - Loss: 0.5316\n",
      "Epoch 2/10 - Batch 25200 - Loss: 0.6087\n",
      "Epoch 2/10 - Batch 25300 - Loss: 0.5367\n",
      "Epoch 2/10 - Batch 25400 - Loss: 0.5472\n",
      "Epoch 2/10 - Batch 25500 - Loss: 0.5843\n",
      "Epoch 2/10 - Batch 25600 - Loss: 0.4790\n",
      "Epoch 2/10 - Batch 25700 - Loss: 0.4867\n",
      "Epoch 2/10 - Batch 25800 - Loss: 0.5390\n",
      "Epoch 2/10 - Batch 25900 - Loss: 0.5684\n",
      "Epoch 2/10 - Batch 26000 - Loss: 0.4193\n",
      "Epoch 2/10 - Batch 26100 - Loss: 0.5114\n",
      "Epoch 2/10 - Batch 26200 - Loss: 0.4185\n",
      "Epoch 2/10 - Batch 26300 - Loss: 0.5265\n",
      "Epoch 2/10 - Batch 26400 - Loss: 0.4305\n",
      "Epoch 2/10 - Batch 26500 - Loss: 0.4872\n",
      "Epoch 2/10 - Batch 26600 - Loss: 0.5028\n",
      "Epoch 2/10 - Batch 26700 - Loss: 0.4151\n",
      "Epoch 2/10 - Batch 26800 - Loss: 0.4758\n",
      "Epoch 2/10 - Batch 26900 - Loss: 0.4711\n",
      "Epoch 2/10 - Batch 27000 - Loss: 0.5334\n",
      "Epoch 2/10 - Batch 27100 - Loss: 0.4764\n",
      "Epoch 2/10 - Batch 27200 - Loss: 0.4796\n",
      "Epoch 2/10 - Batch 27300 - Loss: 0.4958\n",
      "Epoch 2/10 - Batch 27400 - Loss: 0.5016\n",
      "Epoch 2/10 - Batch 27500 - Loss: 0.5530\n",
      "Epoch 2/10 - Batch 27600 - Loss: 0.4025\n",
      "Epoch 2/10 - Batch 27700 - Loss: 0.5493\n",
      "Epoch 2/10 - Batch 27800 - Loss: 0.4607\n",
      "Epoch 2/10 - Batch 27900 - Loss: 0.6345\n",
      "Epoch 2/10 - Batch 28000 - Loss: 0.4009\n",
      "Epoch 2/10 - Batch 28100 - Loss: 0.4995\n",
      "Epoch 2/10 - Batch 28200 - Loss: 0.5581\n",
      "Epoch 2/10 - Batch 28300 - Loss: 0.5210\n",
      "Epoch 2/10 - Batch 28400 - Loss: 0.4551\n",
      "Epoch 2/10 - Batch 28500 - Loss: 0.4942\n",
      "Epoch 2/10 - Batch 28600 - Loss: 0.4891\n",
      "Epoch 2/10 - Batch 28700 - Loss: 0.4445\n",
      "Epoch 2/10 - Batch 28800 - Loss: 0.5541\n",
      "Epoch 2/10 - Batch 28900 - Loss: 0.5317\n",
      "Epoch 2/10 - Batch 29000 - Loss: 0.4912\n",
      "Epoch 2/10 - Batch 29100 - Loss: 0.4777\n",
      "Epoch 2/10 - Batch 29200 - Loss: 0.5357\n",
      "Epoch 2/10 - Batch 29300 - Loss: 0.4003\n",
      "Epoch 2/10 - Batch 29400 - Loss: 0.4299\n",
      "Epoch 2/10 - Batch 29500 - Loss: 0.4618\n",
      "Epoch 2/10 - Batch 29600 - Loss: 0.4888\n",
      "Epoch 2/10 - Batch 29700 - Loss: 0.4824\n",
      "Epoch 2/10 - Batch 29800 - Loss: 0.5417\n",
      "Epoch 2/10 - Batch 29900 - Loss: 0.4739\n",
      "Epoch 2/10 - Batch 30000 - Loss: 0.5037\n",
      "Epoch 2/10 - Batch 30100 - Loss: 0.4341\n",
      "Epoch 2/10 - Batch 30200 - Loss: 0.4638\n",
      "Epoch 2/10 - Batch 30300 - Loss: 0.4710\n",
      "Epoch 2/10 - Batch 30400 - Loss: 0.4768\n",
      "Epoch 2/10 - Batch 30500 - Loss: 0.4889\n",
      "Epoch 2/10 - Batch 30600 - Loss: 0.4556\n",
      "Epoch 2/10 - Batch 30700 - Loss: 0.5234\n",
      "Epoch 2/10 - Batch 30800 - Loss: 0.4841\n",
      "Epoch 2/10 - Batch 30900 - Loss: 0.5224\n",
      "Epoch 2/10 - Batch 31000 - Loss: 0.4886\n",
      "Epoch 2/10 - Batch 31100 - Loss: 0.4513\n",
      "Epoch 2/10 - Batch 31200 - Loss: 0.5699\n",
      "Epoch 2/10 - Batch 31300 - Loss: 0.5600\n",
      "Epoch 2/10 - Batch 31400 - Loss: 0.4864\n",
      "Epoch 2/10 - Batch 31500 - Loss: 0.5085\n",
      "Epoch 2/10 - Batch 31600 - Loss: 0.4713\n",
      "Epoch 2/10 - Batch 31700 - Loss: 0.4735\n",
      "Epoch 2/10 - Batch 31800 - Loss: 0.4785\n",
      "Epoch 2/10 - Batch 31900 - Loss: 0.4715\n",
      "Epoch 2/10 - Batch 32000 - Loss: 0.5483\n",
      "Epoch 2/10 - Batch 32100 - Loss: 0.4464\n",
      "Epoch 2/10 - Batch 32200 - Loss: 0.5233\n",
      "Epoch 2/10 - Batch 32300 - Loss: 0.4594\n",
      "Epoch 2/10 - Batch 32400 - Loss: 0.4865\n",
      "Epoch 2/10 - Batch 32500 - Loss: 0.4851\n",
      "Epoch 2/10 - Batch 32600 - Loss: 0.5664\n",
      "Epoch 2/10 - Batch 32700 - Loss: 0.4684\n",
      "Epoch 2/10 - Batch 32800 - Loss: 0.4594\n",
      "Epoch 2/10 - Batch 32900 - Loss: 0.5424\n",
      "Epoch 2/10 - Batch 33000 - Loss: 0.4965\n",
      "Epoch 2/10 - Batch 33100 - Loss: 0.4350\n",
      "Epoch 2/10 - Batch 33200 - Loss: 0.5036\n",
      "Epoch 2/10 - Batch 33300 - Loss: 0.5064\n",
      "Epoch 2/10 - Batch 33400 - Loss: 0.5104\n",
      "Epoch 2/10 - Batch 33500 - Loss: 0.5050\n",
      "Epoch 2/10 - Batch 33600 - Loss: 0.5064\n",
      "Epoch 2/10 - Batch 33700 - Loss: 0.5070\n",
      "Epoch 2/10 - Batch 33800 - Loss: 0.5035\n",
      "Epoch 2/10 - Batch 33900 - Loss: 0.4537\n",
      "Epoch 2/10 - Batch 34000 - Loss: 0.4957\n",
      "Epoch 2/10 - Batch 34100 - Loss: 0.5161\n",
      "Epoch 2/10 - Batch 34200 - Loss: 0.5788\n",
      "Epoch 2/10 - Batch 34300 - Loss: 0.5250\n",
      "Epoch 2/10 - Batch 34400 - Loss: 0.5216\n",
      "Epoch 2/10 - Batch 34500 - Loss: 0.4794\n",
      "Epoch 2/10 - Batch 34600 - Loss: 0.5408\n",
      "Epoch 2/10 - Batch 34700 - Loss: 0.6197\n",
      "Epoch 2/10 - Batch 34800 - Loss: 0.5750\n",
      "Epoch 2/10 - Batch 34900 - Loss: 0.5498\n",
      "Epoch 2/10 - Batch 35000 - Loss: 0.5501\n",
      "Epoch 2/10 - Batch 35100 - Loss: 0.5417\n",
      "Epoch 2/10 - Batch 35200 - Loss: 0.5869\n",
      "Epoch 2/10 - Batch 35300 - Loss: 0.5986\n",
      "Epoch 2/10 - Batch 35400 - Loss: 0.6454\n",
      "Epoch 2/10 - Batch 35500 - Loss: 0.5824\n",
      "Epoch 2/10 - Batch 35600 - Loss: 0.6596\n",
      "Epoch 2/10 - Batch 35700 - Loss: 0.9323\n",
      "Epoch 2/10 - Batch 35800 - Loss: 1.1394\n",
      "Epoch 2/10 - Batch 35900 - Loss: 1.0258\n",
      "Epoch 2/10 - Batch 36000 - Loss: 1.0858\n",
      "Epoch 2/10 - Batch 36100 - Loss: 1.0442\n",
      "Epoch 2/10 - Batch 36200 - Loss: 1.1422\n",
      "Epoch 2/10 - Batch 36300 - Loss: 1.0111\n",
      "Epoch 2/10 - Batch 36400 - Loss: 1.0734\n",
      "Epoch 2/10 - Batch 36500 - Loss: 0.9345\n",
      "Epoch 2/10 - Batch 36600 - Loss: 1.1396\n",
      "Epoch 2/10 - Batch 36700 - Loss: 1.0218\n",
      "Epoch 2/10 - Batch 36800 - Loss: 1.0126\n",
      "Epoch 2/10 - Batch 36900 - Loss: 0.9558\n",
      "Epoch 2/10 - Batch 37000 - Loss: 0.8806\n",
      "Epoch 2/10 - Batch 37100 - Loss: 1.1691\n",
      "Epoch 2/10 - Batch 37200 - Loss: 0.9259\n",
      "Epoch 2/10 - Batch 37300 - Loss: 0.8385\n",
      "Epoch 2/10 - Batch 37400 - Loss: 1.0887\n",
      "Epoch 2/10 - Batch 37500 - Loss: 0.9108\n",
      "Epoch 2/10 - Batch 37600 - Loss: 1.0117\n",
      "Epoch 2/10 - Batch 37700 - Loss: 0.9225\n",
      "Epoch 2/10 - Batch 37800 - Loss: 1.0914\n",
      "Epoch 2/10 - Batch 37900 - Loss: 1.1026\n",
      "Epoch 2/10 - Batch 38000 - Loss: 0.9561\n",
      "Epoch 2/10 - Batch 38100 - Loss: 1.0117\n",
      "Epoch 2/10 - Batch 38200 - Loss: 1.2235\n",
      "Epoch 2/10 - Batch 38300 - Loss: 0.8599\n",
      "Epoch 2/10 - Batch 38400 - Loss: 0.9718\n",
      "Epoch 2/10 - Batch 38500 - Loss: 0.9694\n",
      "Epoch 2/10 - Batch 38600 - Loss: 1.0037\n",
      "Epoch 2/10 - Batch 38700 - Loss: 0.9657\n",
      "Epoch 2/10 - Batch 38800 - Loss: 0.9848\n",
      "Epoch 2/10 - Batch 38900 - Loss: 0.9650\n",
      "Epoch 2/10 - Batch 39000 - Loss: 0.9697\n",
      "Epoch 2/10 - Batch 39100 - Loss: 0.8634\n",
      "Epoch 2/10 - Batch 39200 - Loss: 1.0429\n",
      "Epoch 2/10 - Batch 39300 - Loss: 1.0762\n",
      "Epoch 2/10 - Batch 39400 - Loss: 0.8084\n",
      "Epoch 2/10 - Batch 39500 - Loss: 1.0317\n",
      "Epoch 2/10 - Batch 39600 - Loss: 1.0303\n",
      "Epoch 2/10 - Batch 39700 - Loss: 1.0072\n",
      "Epoch 2/10 - Batch 39800 - Loss: 1.1130\n",
      "Epoch 2/10 - Batch 39900 - Loss: 0.9616\n",
      "Epoch 2/10 - Batch 40000 - Loss: 0.8384\n",
      "Epoch 2/10 - Batch 40100 - Loss: 0.9337\n",
      "Epoch 2/10 - Batch 40200 - Loss: 0.9186\n",
      "Epoch 2/10 - Batch 40300 - Loss: 1.2262\n",
      "Epoch 2/10 - Batch 40400 - Loss: 1.1087\n",
      "Epoch 2/10 - Batch 40500 - Loss: 0.7908\n",
      "Epoch 2/10 - Batch 40600 - Loss: 0.8531\n",
      "Epoch 2/10 - Batch 40700 - Loss: 0.9421\n",
      "Epoch 2/10 - Batch 40800 - Loss: 1.0577\n",
      "Epoch 2/10 - Batch 40900 - Loss: 1.1349\n",
      "Epoch 2/10 - Batch 41000 - Loss: 1.0321\n",
      "Epoch 2/10 - Batch 41100 - Loss: 1.1665\n",
      "Epoch 2/10 - Batch 41200 - Loss: 1.0284\n",
      "Epoch 2/10 - Batch 41300 - Loss: 1.0470\n",
      "Epoch 2/10 - Batch 41400 - Loss: 1.0694\n",
      "Epoch 2/10 - Batch 41500 - Loss: 1.2235\n",
      "Epoch 2/10 - Batch 41600 - Loss: 1.0128\n",
      "Epoch 2/10 - Batch 41700 - Loss: 1.2478\n",
      "Epoch 2/10 - Batch 41800 - Loss: 0.9147\n",
      "Epoch 2/10 - Batch 41900 - Loss: 0.8852\n",
      "Epoch 2/10 - Batch 42000 - Loss: 0.9223\n",
      "Epoch 2/10 - Batch 42100 - Loss: 1.0360\n",
      "Epoch 2/10 - Batch 42200 - Loss: 1.1088\n",
      "Epoch 2/10 - Batch 42300 - Loss: 0.9836\n",
      "Epoch 2/10 - Batch 42400 - Loss: 0.9542\n",
      "Epoch 2/10 - Batch 42500 - Loss: 0.9094\n",
      "Epoch 2/10 - Batch 42600 - Loss: 0.9482\n",
      "Epoch 2/10 - Batch 42700 - Loss: 0.9267\n",
      "Epoch 2/10 - Batch 42800 - Loss: 0.7816\n",
      "Epoch 2/10 - Batch 42900 - Loss: 0.9039\n",
      "Epoch 2/10 - Batch 43000 - Loss: 1.0661\n",
      "Epoch 2/10 - Batch 43100 - Loss: 0.8766\n",
      "Epoch 2/10 - Batch 43200 - Loss: 0.8218\n",
      "Epoch 2/10 - Batch 43300 - Loss: 0.7764\n",
      "Epoch 2/10 - Batch 43400 - Loss: 0.7755\n",
      "Epoch 2/10 - Batch 43500 - Loss: 0.9300\n",
      "Epoch 2/10 - Batch 43600 - Loss: 0.6031\n",
      "Epoch 2/10 - Batch 43700 - Loss: 0.7111\n",
      "Epoch 2/10 - Batch 43800 - Loss: 0.6443\n",
      "Epoch 2/10 - Batch 43900 - Loss: 0.6260\n",
      "Epoch 2/10 - Batch 44000 - Loss: 0.8374\n",
      "Epoch 2/10 - Batch 44100 - Loss: 0.6955\n",
      "Epoch 2/10 - Batch 44200 - Loss: 0.5928\n",
      "Epoch 2/10 - Batch 44300 - Loss: 0.6518\n",
      "Epoch 2/10 - Batch 44400 - Loss: 0.8607\n",
      "Epoch 2/10 - Batch 44500 - Loss: 0.7311\n",
      "Epoch 2/10 - Batch 44600 - Loss: 0.9581\n",
      "Epoch 2/10 - Batch 44700 - Loss: 0.5795\n",
      "Epoch 2/10 - Batch 44800 - Loss: 0.6648\n",
      "Epoch 2/10 - Batch 44900 - Loss: 1.0114\n",
      "Epoch 2/10 - Batch 45000 - Loss: 0.8857\n",
      "Epoch 2/10 - Batch 45100 - Loss: 0.3056\n",
      "Epoch 2/10 - Batch 45200 - Loss: 0.8184\n",
      "Epoch 2/10 - Batch 45300 - Loss: 0.7577\n",
      "Epoch 2/10 - Batch 45400 - Loss: 0.6562\n",
      "Epoch 2/10 - Batch 45500 - Loss: 1.0541\n",
      "Epoch 2/10 - Batch 45600 - Loss: 1.0673\n",
      "Epoch 2/10 - Batch 45700 - Loss: 0.9563\n",
      "Epoch 2/10 - Batch 45800 - Loss: 1.2775\n",
      "Epoch 2/10 - Batch 45900 - Loss: 1.0682\n",
      "Epoch 2/10 - Batch 46000 - Loss: 0.9663\n",
      "Epoch 2/10 - Batch 46100 - Loss: 1.1335\n",
      "Epoch 2/10 - Batch 46200 - Loss: 0.9555\n",
      "Epoch 2/10 - Batch 46300 - Loss: 1.1348\n",
      "Epoch 2/10 - Batch 46400 - Loss: 0.9739\n",
      "Epoch 2/10 - Batch 46500 - Loss: 1.1945\n",
      "Epoch 2/10 - Batch 46600 - Loss: 0.8620\n",
      "Epoch 2/10 - Batch 46700 - Loss: 0.8459\n",
      "Epoch 2/10 - Batch 46800 - Loss: 1.0370\n",
      "Epoch 2/10 - Batch 46900 - Loss: 1.0703\n",
      "Epoch 2/10 - Batch 47000 - Loss: 0.7676\n",
      "Epoch 2/10 - Batch 47100 - Loss: 1.0344\n",
      "Epoch 2/10 - Batch 47200 - Loss: 0.8881\n",
      "Epoch 2/10 - Batch 47300 - Loss: 0.8273\n",
      "Epoch 2/10 - Batch 47400 - Loss: 1.0374\n",
      "Epoch 2/10 - Batch 47500 - Loss: 0.8152\n",
      "Epoch 2/10 - Batch 47600 - Loss: 0.9560\n",
      "Epoch 2/10 - Batch 47700 - Loss: 0.9546\n",
      "Epoch 2/10 - Batch 47800 - Loss: 1.1961\n",
      "Epoch 2/10 - Batch 47900 - Loss: 0.9472\n",
      "Epoch 2/10 - Batch 48000 - Loss: 0.9605\n",
      "Epoch 2/10 - Batch 48100 - Loss: 1.1500\n",
      "Epoch 2/10 - Batch 48200 - Loss: 1.0618\n",
      "Epoch 2/10 - Batch 48300 - Loss: 1.0106\n",
      "Epoch 2/10 - Batch 48400 - Loss: 1.1105\n",
      "Epoch 2/10 - Batch 48500 - Loss: 0.9156\n",
      "Epoch 2/10 - Batch 48600 - Loss: 1.0317\n",
      "Epoch 2/10 - Batch 48700 - Loss: 0.9226\n",
      "Epoch 2/10 - Batch 48800 - Loss: 0.8638\n",
      "Epoch 2/10 - Batch 48900 - Loss: 0.9387\n",
      "Epoch 2/10 - Batch 49000 - Loss: 0.8733\n",
      "Epoch 2/10 - Batch 49100 - Loss: 0.8017\n",
      "Epoch 2/10 - Batch 49200 - Loss: 1.2917\n",
      "Epoch 3/10 - Batch 49300 - Loss: 0.7924\n",
      "Epoch 3/10 - Batch 49400 - Loss: 0.6269\n",
      "Epoch 3/10 - Batch 49500 - Loss: 0.5638\n",
      "Epoch 3/10 - Batch 49600 - Loss: 0.3360\n",
      "Epoch 3/10 - Batch 49700 - Loss: 0.4266\n",
      "Epoch 3/10 - Batch 49800 - Loss: 0.4203\n",
      "Epoch 3/10 - Batch 49900 - Loss: 0.4344\n",
      "Epoch 3/10 - Batch 50000 - Loss: 0.4250\n",
      "Epoch 3/10 - Batch 50100 - Loss: 0.4179\n",
      "Epoch 3/10 - Batch 50200 - Loss: 0.3880\n",
      "Epoch 3/10 - Batch 50300 - Loss: 0.3665\n",
      "Epoch 3/10 - Batch 50400 - Loss: 0.4086\n",
      "Epoch 3/10 - Batch 50500 - Loss: 0.3825\n",
      "Epoch 3/10 - Batch 50600 - Loss: 0.3761\n",
      "Epoch 3/10 - Batch 50700 - Loss: 0.3635\n",
      "Epoch 3/10 - Batch 50800 - Loss: 0.3654\n",
      "Epoch 3/10 - Batch 50900 - Loss: 0.3816\n",
      "Epoch 3/10 - Batch 51000 - Loss: 0.3342\n",
      "Epoch 3/10 - Batch 51100 - Loss: 0.3749\n",
      "Epoch 3/10 - Batch 51200 - Loss: 0.4063\n",
      "Epoch 3/10 - Batch 51300 - Loss: 0.3554\n",
      "Epoch 3/10 - Batch 51400 - Loss: 0.3263\n",
      "Epoch 3/10 - Batch 51500 - Loss: 0.3867\n",
      "Epoch 3/10 - Batch 51600 - Loss: 0.4224\n",
      "Epoch 3/10 - Batch 51700 - Loss: 0.3933\n",
      "Epoch 3/10 - Batch 51800 - Loss: 0.3464\n",
      "Epoch 3/10 - Batch 51900 - Loss: 0.4108\n",
      "Epoch 3/10 - Batch 52000 - Loss: 0.3935\n",
      "Epoch 3/10 - Batch 52100 - Loss: 0.4104\n",
      "Epoch 3/10 - Batch 52200 - Loss: 0.2755\n",
      "Epoch 3/10 - Batch 52300 - Loss: 0.3909\n",
      "Epoch 3/10 - Batch 52400 - Loss: 0.4034\n",
      "Epoch 3/10 - Batch 52500 - Loss: 0.4501\n",
      "Epoch 3/10 - Batch 52600 - Loss: 0.4423\n",
      "Epoch 3/10 - Batch 52700 - Loss: 0.3573\n",
      "Epoch 3/10 - Batch 52800 - Loss: 0.4443\n",
      "Epoch 3/10 - Batch 52900 - Loss: 0.3661\n",
      "Epoch 3/10 - Batch 53000 - Loss: 0.4101\n",
      "Epoch 3/10 - Batch 53100 - Loss: 0.3846\n",
      "Epoch 3/10 - Batch 53200 - Loss: 0.3536\n",
      "Epoch 3/10 - Batch 53300 - Loss: 0.3784\n",
      "Epoch 3/10 - Batch 53400 - Loss: 0.4346\n",
      "Epoch 3/10 - Batch 53500 - Loss: 0.4192\n",
      "Epoch 3/10 - Batch 53600 - Loss: 0.4190\n",
      "Epoch 3/10 - Batch 53700 - Loss: 0.3824\n",
      "Epoch 3/10 - Batch 53800 - Loss: 0.4123\n",
      "Epoch 3/10 - Batch 53900 - Loss: 0.3136\n",
      "Epoch 3/10 - Batch 54000 - Loss: 0.3840\n",
      "Epoch 3/10 - Batch 54100 - Loss: 0.3350\n",
      "Epoch 3/10 - Batch 54200 - Loss: 0.3963\n",
      "Epoch 3/10 - Batch 54300 - Loss: 0.4251\n",
      "Epoch 3/10 - Batch 54400 - Loss: 0.3483\n",
      "Epoch 3/10 - Batch 54500 - Loss: 0.4059\n",
      "Epoch 3/10 - Batch 54600 - Loss: 0.4231\n",
      "Epoch 3/10 - Batch 54700 - Loss: 0.3941\n",
      "Epoch 3/10 - Batch 54800 - Loss: 0.3615\n",
      "Epoch 3/10 - Batch 54900 - Loss: 0.3409\n",
      "Epoch 3/10 - Batch 55000 - Loss: 0.3723\n",
      "Epoch 3/10 - Batch 55100 - Loss: 0.3806\n",
      "Epoch 3/10 - Batch 55200 - Loss: 0.3935\n",
      "Epoch 3/10 - Batch 55300 - Loss: 0.4056\n",
      "Epoch 3/10 - Batch 55400 - Loss: 0.3836\n",
      "Epoch 3/10 - Batch 55500 - Loss: 0.4077\n",
      "Epoch 3/10 - Batch 55600 - Loss: 0.4065\n",
      "Epoch 3/10 - Batch 55700 - Loss: 0.3597\n",
      "Epoch 3/10 - Batch 55800 - Loss: 0.4634\n",
      "Epoch 3/10 - Batch 55900 - Loss: 0.4567\n",
      "Epoch 3/10 - Batch 56000 - Loss: 0.3906\n",
      "Epoch 3/10 - Batch 56100 - Loss: 0.4006\n",
      "Epoch 3/10 - Batch 56200 - Loss: 0.3968\n",
      "Epoch 3/10 - Batch 56300 - Loss: 0.3414\n",
      "Epoch 3/10 - Batch 56400 - Loss: 0.4181\n",
      "Epoch 3/10 - Batch 56500 - Loss: 0.4153\n",
      "Epoch 3/10 - Batch 56600 - Loss: 0.3725\n",
      "Epoch 3/10 - Batch 56700 - Loss: 0.4254\n",
      "Epoch 3/10 - Batch 56800 - Loss: 0.3778\n",
      "Epoch 3/10 - Batch 56900 - Loss: 0.3839\n",
      "Epoch 3/10 - Batch 57000 - Loss: 0.3995\n",
      "Epoch 3/10 - Batch 57100 - Loss: 0.4272\n",
      "Epoch 3/10 - Batch 57200 - Loss: 0.4004\n",
      "Epoch 3/10 - Batch 57300 - Loss: 0.3893\n",
      "Epoch 3/10 - Batch 57400 - Loss: 0.3760\n",
      "Epoch 3/10 - Batch 57500 - Loss: 0.4477\n",
      "Epoch 3/10 - Batch 57600 - Loss: 0.3838\n",
      "Epoch 3/10 - Batch 57700 - Loss: 0.3789\n",
      "Epoch 3/10 - Batch 57800 - Loss: 0.3927\n",
      "Epoch 3/10 - Batch 57900 - Loss: 0.3956\n",
      "Epoch 3/10 - Batch 58000 - Loss: 0.4327\n",
      "Epoch 3/10 - Batch 58100 - Loss: 0.3874\n",
      "Epoch 3/10 - Batch 58200 - Loss: 0.4388\n",
      "Epoch 3/10 - Batch 58300 - Loss: 0.4166\n",
      "Epoch 3/10 - Batch 58400 - Loss: 0.4129\n",
      "Epoch 3/10 - Batch 58500 - Loss: 0.3816\n",
      "Epoch 3/10 - Batch 58600 - Loss: 0.4026\n",
      "Epoch 3/10 - Batch 58700 - Loss: 0.4284\n",
      "Epoch 3/10 - Batch 58800 - Loss: 0.4855\n",
      "Epoch 3/10 - Batch 58900 - Loss: 0.4341\n",
      "Epoch 3/10 - Batch 59000 - Loss: 0.4094\n",
      "Epoch 3/10 - Batch 59100 - Loss: 0.4109\n",
      "Epoch 3/10 - Batch 59200 - Loss: 0.4327\n",
      "Epoch 3/10 - Batch 59300 - Loss: 0.4769\n",
      "Epoch 3/10 - Batch 59400 - Loss: 0.5246\n",
      "Epoch 3/10 - Batch 59500 - Loss: 0.4073\n",
      "Epoch 3/10 - Batch 59600 - Loss: 0.5028\n",
      "Epoch 3/10 - Batch 59700 - Loss: 0.4715\n",
      "Epoch 3/10 - Batch 59800 - Loss: 0.5061\n",
      "Epoch 3/10 - Batch 59900 - Loss: 0.4737\n",
      "Epoch 3/10 - Batch 60000 - Loss: 0.5314\n",
      "Epoch 3/10 - Batch 60100 - Loss: 0.5063\n",
      "Epoch 3/10 - Batch 60200 - Loss: 0.5353\n",
      "Epoch 3/10 - Batch 60300 - Loss: 0.7355\n",
      "Epoch 3/10 - Batch 60400 - Loss: 0.8558\n",
      "Epoch 3/10 - Batch 60500 - Loss: 0.8813\n",
      "Epoch 3/10 - Batch 60600 - Loss: 1.0195\n",
      "Epoch 3/10 - Batch 60700 - Loss: 0.8458\n",
      "Epoch 3/10 - Batch 60800 - Loss: 0.9400\n",
      "Epoch 3/10 - Batch 60900 - Loss: 0.9415\n",
      "Epoch 3/10 - Batch 61000 - Loss: 0.8839\n",
      "Epoch 3/10 - Batch 61100 - Loss: 0.8353\n",
      "Epoch 3/10 - Batch 61200 - Loss: 0.8790\n",
      "Epoch 3/10 - Batch 61300 - Loss: 0.9427\n",
      "Epoch 3/10 - Batch 61400 - Loss: 0.9332\n",
      "Epoch 3/10 - Batch 61500 - Loss: 0.7488\n",
      "Epoch 3/10 - Batch 61600 - Loss: 0.7530\n",
      "Epoch 3/10 - Batch 61700 - Loss: 0.8171\n",
      "Epoch 3/10 - Batch 61800 - Loss: 0.8970\n",
      "Epoch 3/10 - Batch 61900 - Loss: 0.7805\n",
      "Epoch 3/10 - Batch 62000 - Loss: 0.9164\n",
      "Epoch 3/10 - Batch 62100 - Loss: 0.8823\n",
      "Epoch 3/10 - Batch 62200 - Loss: 0.7764\n",
      "Epoch 3/10 - Batch 62300 - Loss: 0.7825\n",
      "Epoch 3/10 - Batch 62400 - Loss: 0.8733\n",
      "Epoch 3/10 - Batch 62500 - Loss: 1.1106\n",
      "Epoch 3/10 - Batch 62600 - Loss: 0.7810\n",
      "Epoch 3/10 - Batch 62700 - Loss: 0.8217\n",
      "Epoch 3/10 - Batch 62800 - Loss: 1.1262\n",
      "Epoch 3/10 - Batch 62900 - Loss: 0.7695\n",
      "Epoch 3/10 - Batch 63000 - Loss: 0.8240\n",
      "Epoch 3/10 - Batch 63100 - Loss: 0.8547\n",
      "Epoch 3/10 - Batch 63200 - Loss: 0.8447\n",
      "Epoch 3/10 - Batch 63300 - Loss: 0.8353\n",
      "Epoch 3/10 - Batch 63400 - Loss: 0.8640\n",
      "Epoch 3/10 - Batch 63500 - Loss: 0.7664\n",
      "Epoch 3/10 - Batch 63600 - Loss: 0.8285\n",
      "Epoch 3/10 - Batch 63700 - Loss: 0.7719\n",
      "Epoch 3/10 - Batch 63800 - Loss: 0.8250\n",
      "Epoch 3/10 - Batch 63900 - Loss: 1.0567\n",
      "Epoch 3/10 - Batch 64000 - Loss: 0.6442\n",
      "Epoch 3/10 - Batch 64100 - Loss: 0.9450\n",
      "Epoch 3/10 - Batch 64200 - Loss: 0.8649\n",
      "Epoch 3/10 - Batch 64300 - Loss: 0.8731\n",
      "Epoch 3/10 - Batch 64400 - Loss: 0.9650\n",
      "Epoch 3/10 - Batch 64500 - Loss: 0.8833\n",
      "Epoch 3/10 - Batch 64600 - Loss: 0.8131\n",
      "Epoch 3/10 - Batch 64700 - Loss: 0.7340\n",
      "Epoch 3/10 - Batch 64800 - Loss: 0.7796\n",
      "Epoch 3/10 - Batch 64900 - Loss: 0.9770\n",
      "Epoch 3/10 - Batch 65000 - Loss: 1.0836\n",
      "Epoch 3/10 - Batch 65100 - Loss: 0.7606\n",
      "Epoch 3/10 - Batch 65200 - Loss: 0.6865\n",
      "Epoch 3/10 - Batch 65300 - Loss: 0.8232\n",
      "Epoch 3/10 - Batch 65400 - Loss: 0.7744\n",
      "Epoch 3/10 - Batch 65500 - Loss: 1.0646\n",
      "Epoch 3/10 - Batch 65600 - Loss: 0.9676\n",
      "Epoch 3/10 - Batch 65700 - Loss: 1.0869\n",
      "Epoch 3/10 - Batch 65800 - Loss: 0.8628\n",
      "Epoch 3/10 - Batch 65900 - Loss: 0.9734\n",
      "Epoch 3/10 - Batch 66000 - Loss: 0.9142\n",
      "Epoch 3/10 - Batch 66100 - Loss: 1.1150\n",
      "Epoch 3/10 - Batch 66200 - Loss: 0.8637\n",
      "Epoch 3/10 - Batch 66300 - Loss: 0.9322\n",
      "Epoch 3/10 - Batch 66400 - Loss: 1.0793\n",
      "Epoch 3/10 - Batch 66500 - Loss: 0.7146\n",
      "Epoch 3/10 - Batch 66600 - Loss: 0.8560\n",
      "Epoch 3/10 - Batch 66700 - Loss: 0.8305\n",
      "Epoch 3/10 - Batch 66800 - Loss: 1.0055\n",
      "Epoch 3/10 - Batch 66900 - Loss: 0.8908\n",
      "Epoch 3/10 - Batch 67000 - Loss: 0.8677\n",
      "Epoch 3/10 - Batch 67100 - Loss: 0.7288\n",
      "Epoch 3/10 - Batch 67200 - Loss: 0.9683\n",
      "Epoch 3/10 - Batch 67300 - Loss: 0.7429\n",
      "Epoch 3/10 - Batch 67400 - Loss: 0.6749\n",
      "Epoch 3/10 - Batch 67500 - Loss: 0.7736\n",
      "Epoch 3/10 - Batch 67600 - Loss: 0.8494\n",
      "Epoch 3/10 - Batch 67700 - Loss: 0.8794\n",
      "Epoch 3/10 - Batch 67800 - Loss: 0.6961\n",
      "Epoch 3/10 - Batch 67900 - Loss: 0.7721\n",
      "Epoch 3/10 - Batch 68000 - Loss: 0.7028\n",
      "Epoch 3/10 - Batch 68100 - Loss: 0.6689\n",
      "Epoch 3/10 - Batch 68200 - Loss: 0.6967\n",
      "Epoch 3/10 - Batch 68300 - Loss: 0.5986\n",
      "Epoch 3/10 - Batch 68400 - Loss: 0.5419\n",
      "Epoch 3/10 - Batch 68500 - Loss: 0.4995\n",
      "Epoch 3/10 - Batch 68600 - Loss: 0.8181\n",
      "Epoch 3/10 - Batch 68700 - Loss: 0.5486\n",
      "Epoch 3/10 - Batch 68800 - Loss: 0.5287\n",
      "Epoch 3/10 - Batch 68900 - Loss: 0.5898\n",
      "Epoch 3/10 - Batch 69000 - Loss: 0.7331\n",
      "Epoch 3/10 - Batch 69100 - Loss: 0.5923\n",
      "Epoch 3/10 - Batch 69200 - Loss: 0.7053\n",
      "Epoch 3/10 - Batch 69300 - Loss: 0.6943\n",
      "Epoch 3/10 - Batch 69400 - Loss: 0.5697\n",
      "Epoch 3/10 - Batch 69500 - Loss: 0.6058\n",
      "Epoch 3/10 - Batch 69600 - Loss: 1.0809\n",
      "Epoch 3/10 - Batch 69700 - Loss: 0.3632\n",
      "Epoch 3/10 - Batch 69800 - Loss: 0.4884\n",
      "Epoch 3/10 - Batch 69900 - Loss: 0.7054\n",
      "Epoch 3/10 - Batch 70000 - Loss: 0.5753\n",
      "Epoch 3/10 - Batch 70100 - Loss: 0.7686\n",
      "Epoch 3/10 - Batch 70200 - Loss: 1.0136\n",
      "Epoch 3/10 - Batch 70300 - Loss: 0.8681\n",
      "Epoch 3/10 - Batch 70400 - Loss: 1.0071\n",
      "Epoch 3/10 - Batch 70500 - Loss: 0.9998\n",
      "Epoch 3/10 - Batch 70600 - Loss: 0.9235\n",
      "Epoch 3/10 - Batch 70700 - Loss: 1.0112\n",
      "Epoch 3/10 - Batch 70800 - Loss: 0.8904\n",
      "Epoch 3/10 - Batch 70900 - Loss: 0.9731\n",
      "Epoch 3/10 - Batch 71000 - Loss: 0.9520\n",
      "Epoch 3/10 - Batch 71100 - Loss: 0.9494\n",
      "Epoch 3/10 - Batch 71200 - Loss: 0.8354\n",
      "Epoch 3/10 - Batch 71300 - Loss: 0.7889\n",
      "Epoch 3/10 - Batch 71400 - Loss: 0.8208\n",
      "Epoch 3/10 - Batch 71500 - Loss: 1.0023\n",
      "Epoch 3/10 - Batch 71600 - Loss: 0.7894\n",
      "Epoch 3/10 - Batch 71700 - Loss: 0.8054\n",
      "Epoch 3/10 - Batch 71800 - Loss: 0.8821\n",
      "Epoch 3/10 - Batch 71900 - Loss: 0.6332\n",
      "Epoch 3/10 - Batch 72000 - Loss: 0.8809\n",
      "Epoch 3/10 - Batch 72100 - Loss: 0.8541\n",
      "Epoch 3/10 - Batch 72200 - Loss: 0.7006\n",
      "Epoch 3/10 - Batch 72300 - Loss: 0.9228\n",
      "Epoch 3/10 - Batch 72400 - Loss: 1.0415\n",
      "Epoch 3/10 - Batch 72500 - Loss: 0.8363\n",
      "Epoch 3/10 - Batch 72600 - Loss: 0.8705\n",
      "Epoch 3/10 - Batch 72700 - Loss: 0.8889\n",
      "Epoch 3/10 - Batch 72800 - Loss: 1.2700\n",
      "Epoch 3/10 - Batch 72900 - Loss: 0.8069\n",
      "Epoch 3/10 - Batch 73000 - Loss: 0.9028\n",
      "Epoch 3/10 - Batch 73100 - Loss: 0.9511\n",
      "Epoch 3/10 - Batch 73200 - Loss: 0.9051\n",
      "Epoch 3/10 - Batch 73300 - Loss: 0.8885\n",
      "Epoch 3/10 - Batch 73400 - Loss: 0.8038\n",
      "Epoch 3/10 - Batch 73500 - Loss: 0.7616\n",
      "Epoch 3/10 - Batch 73600 - Loss: 0.8512\n",
      "Epoch 3/10 - Batch 73700 - Loss: 0.6196\n",
      "Epoch 3/10 - Batch 73800 - Loss: 1.0945\n",
      "Epoch 3/10 - Batch 73900 - Loss: 0.7884\n",
      "Epoch 4/10 - Batch 74000 - Loss: 0.5661\n",
      "Epoch 4/10 - Batch 74100 - Loss: 0.4226\n",
      "Epoch 4/10 - Batch 74200 - Loss: 0.3572\n",
      "Epoch 4/10 - Batch 74300 - Loss: 0.3480\n",
      "Epoch 4/10 - Batch 74400 - Loss: 0.3349\n",
      "Epoch 4/10 - Batch 74500 - Loss: 0.3614\n",
      "Epoch 4/10 - Batch 74600 - Loss: 0.3574\n",
      "Epoch 4/10 - Batch 74700 - Loss: 0.3331\n",
      "Epoch 4/10 - Batch 74800 - Loss: 0.3668\n",
      "Epoch 4/10 - Batch 74900 - Loss: 0.2813\n",
      "Epoch 4/10 - Batch 75000 - Loss: 0.2796\n",
      "Epoch 4/10 - Batch 75100 - Loss: 0.3021\n",
      "Epoch 4/10 - Batch 75200 - Loss: 0.3718\n",
      "Epoch 4/10 - Batch 75300 - Loss: 0.2400\n",
      "Epoch 4/10 - Batch 75400 - Loss: 0.2943\n",
      "Epoch 4/10 - Batch 75500 - Loss: 0.3004\n",
      "Epoch 4/10 - Batch 75600 - Loss: 0.3004\n",
      "Epoch 4/10 - Batch 75700 - Loss: 0.2721\n",
      "Epoch 4/10 - Batch 75800 - Loss: 0.3226\n",
      "Epoch 4/10 - Batch 75900 - Loss: 0.3363\n",
      "Epoch 4/10 - Batch 76000 - Loss: 0.2443\n",
      "Epoch 4/10 - Batch 76100 - Loss: 0.3213\n",
      "Epoch 4/10 - Batch 76200 - Loss: 0.3414\n",
      "Epoch 4/10 - Batch 76300 - Loss: 0.2984\n",
      "Epoch 4/10 - Batch 76400 - Loss: 0.2983\n",
      "Epoch 4/10 - Batch 76500 - Loss: 0.3393\n",
      "Epoch 4/10 - Batch 76600 - Loss: 0.3301\n",
      "Epoch 4/10 - Batch 76700 - Loss: 0.3393\n",
      "Epoch 4/10 - Batch 76800 - Loss: 0.3111\n",
      "Epoch 4/10 - Batch 76900 - Loss: 0.2807\n",
      "Epoch 4/10 - Batch 77000 - Loss: 0.3213\n",
      "Epoch 4/10 - Batch 77100 - Loss: 0.3421\n",
      "Epoch 4/10 - Batch 77200 - Loss: 0.4152\n",
      "Epoch 4/10 - Batch 77300 - Loss: 0.2650\n",
      "Epoch 4/10 - Batch 77400 - Loss: 0.3698\n",
      "Epoch 4/10 - Batch 77500 - Loss: 0.3279\n",
      "Epoch 4/10 - Batch 77600 - Loss: 0.3218\n",
      "Epoch 4/10 - Batch 77700 - Loss: 0.3117\n",
      "Epoch 4/10 - Batch 77800 - Loss: 0.2954\n",
      "Epoch 4/10 - Batch 77900 - Loss: 0.2603\n",
      "Epoch 4/10 - Batch 78000 - Loss: 0.3330\n",
      "Epoch 4/10 - Batch 78100 - Loss: 0.3381\n",
      "Epoch 4/10 - Batch 78200 - Loss: 0.3739\n",
      "Epoch 4/10 - Batch 78300 - Loss: 0.3377\n",
      "Epoch 4/10 - Batch 78400 - Loss: 0.2939\n",
      "Epoch 4/10 - Batch 78500 - Loss: 0.3221\n",
      "Epoch 4/10 - Batch 78600 - Loss: 0.2795\n",
      "Epoch 4/10 - Batch 78700 - Loss: 0.2824\n",
      "Epoch 4/10 - Batch 78800 - Loss: 0.3033\n",
      "Epoch 4/10 - Batch 78900 - Loss: 0.3369\n",
      "Epoch 4/10 - Batch 79000 - Loss: 0.3120\n",
      "Epoch 4/10 - Batch 79100 - Loss: 0.3627\n",
      "Epoch 4/10 - Batch 79200 - Loss: 0.2794\n",
      "Epoch 4/10 - Batch 79300 - Loss: 0.3632\n",
      "Epoch 4/10 - Batch 79400 - Loss: 0.2928\n",
      "Epoch 4/10 - Batch 79500 - Loss: 0.3053\n",
      "Epoch 4/10 - Batch 79600 - Loss: 0.3208\n",
      "Epoch 4/10 - Batch 79700 - Loss: 0.3429\n",
      "Epoch 4/10 - Batch 79800 - Loss: 0.3135\n",
      "Epoch 4/10 - Batch 79900 - Loss: 0.3319\n",
      "Epoch 4/10 - Batch 80000 - Loss: 0.3394\n",
      "Epoch 4/10 - Batch 80100 - Loss: 0.3114\n",
      "Epoch 4/10 - Batch 80200 - Loss: 0.3385\n",
      "Epoch 4/10 - Batch 80300 - Loss: 0.3251\n",
      "Epoch 4/10 - Batch 80400 - Loss: 0.3325\n",
      "Epoch 4/10 - Batch 80500 - Loss: 0.3591\n",
      "Epoch 4/10 - Batch 80600 - Loss: 0.3864\n",
      "Epoch 4/10 - Batch 80700 - Loss: 0.3447\n",
      "Epoch 4/10 - Batch 80800 - Loss: 0.3450\n",
      "Epoch 4/10 - Batch 80900 - Loss: 0.3090\n",
      "Epoch 4/10 - Batch 81000 - Loss: 0.3519\n",
      "Epoch 4/10 - Batch 81100 - Loss: 0.3208\n",
      "Epoch 4/10 - Batch 81200 - Loss: 0.3142\n",
      "Epoch 4/10 - Batch 81300 - Loss: 0.3513\n",
      "Epoch 4/10 - Batch 81400 - Loss: 0.3162\n",
      "Epoch 4/10 - Batch 81500 - Loss: 0.3360\n",
      "Epoch 4/10 - Batch 81600 - Loss: 0.3286\n",
      "Epoch 4/10 - Batch 81700 - Loss: 0.3570\n",
      "Epoch 4/10 - Batch 81800 - Loss: 0.3477\n",
      "Epoch 4/10 - Batch 81900 - Loss: 0.3700\n",
      "Epoch 4/10 - Batch 82000 - Loss: 0.3046\n",
      "Epoch 4/10 - Batch 82100 - Loss: 0.3428\n",
      "Epoch 4/10 - Batch 82200 - Loss: 0.3560\n",
      "Epoch 4/10 - Batch 82300 - Loss: 0.3319\n",
      "Epoch 4/10 - Batch 82400 - Loss: 0.3060\n",
      "Epoch 4/10 - Batch 82500 - Loss: 0.3496\n",
      "Epoch 4/10 - Batch 82600 - Loss: 0.3820\n",
      "Epoch 4/10 - Batch 82700 - Loss: 0.3476\n",
      "Epoch 4/10 - Batch 82800 - Loss: 0.3527\n",
      "Epoch 4/10 - Batch 82900 - Loss: 0.3518\n",
      "Epoch 4/10 - Batch 83000 - Loss: 0.3745\n",
      "Epoch 4/10 - Batch 83100 - Loss: 0.3386\n",
      "Epoch 4/10 - Batch 83200 - Loss: 0.3314\n",
      "Epoch 4/10 - Batch 83300 - Loss: 0.3569\n",
      "Epoch 4/10 - Batch 83400 - Loss: 0.3788\n",
      "Epoch 4/10 - Batch 83500 - Loss: 0.4113\n",
      "Epoch 4/10 - Batch 83600 - Loss: 0.3687\n",
      "Epoch 4/10 - Batch 83700 - Loss: 0.3625\n",
      "Epoch 4/10 - Batch 83800 - Loss: 0.3462\n",
      "Epoch 4/10 - Batch 83900 - Loss: 0.3833\n",
      "Epoch 4/10 - Batch 84000 - Loss: 0.4398\n",
      "Epoch 4/10 - Batch 84100 - Loss: 0.3862\n",
      "Epoch 4/10 - Batch 84200 - Loss: 0.4063\n",
      "Epoch 4/10 - Batch 84300 - Loss: 0.4014\n",
      "Epoch 4/10 - Batch 84400 - Loss: 0.4280\n",
      "Epoch 4/10 - Batch 84500 - Loss: 0.4207\n",
      "Epoch 4/10 - Batch 84600 - Loss: 0.4528\n",
      "Epoch 4/10 - Batch 84700 - Loss: 0.4468\n",
      "Epoch 4/10 - Batch 84800 - Loss: 0.4534\n",
      "Epoch 4/10 - Batch 84900 - Loss: 0.5397\n",
      "Epoch 4/10 - Batch 85000 - Loss: 0.7803\n",
      "Epoch 4/10 - Batch 85100 - Loss: 0.7828\n",
      "Epoch 4/10 - Batch 85200 - Loss: 0.9305\n",
      "Epoch 4/10 - Batch 85300 - Loss: 0.7111\n",
      "Epoch 4/10 - Batch 85400 - Loss: 0.7939\n",
      "Epoch 4/10 - Batch 85500 - Loss: 0.8882\n",
      "Epoch 4/10 - Batch 85600 - Loss: 0.8282\n",
      "Epoch 4/10 - Batch 85700 - Loss: 0.8149\n",
      "Epoch 4/10 - Batch 85800 - Loss: 0.7383\n",
      "Epoch 4/10 - Batch 85900 - Loss: 0.8303\n",
      "Epoch 4/10 - Batch 86000 - Loss: 0.8289\n",
      "Epoch 4/10 - Batch 86100 - Loss: 0.6817\n",
      "Epoch 4/10 - Batch 86200 - Loss: 0.6987\n",
      "Epoch 4/10 - Batch 86300 - Loss: 0.6535\n",
      "Epoch 4/10 - Batch 86400 - Loss: 0.9617\n",
      "Epoch 4/10 - Batch 86500 - Loss: 0.6786\n",
      "Epoch 4/10 - Batch 86600 - Loss: 0.6748\n",
      "Epoch 4/10 - Batch 86700 - Loss: 0.8649\n",
      "Epoch 4/10 - Batch 86800 - Loss: 0.6977\n",
      "Epoch 4/10 - Batch 86900 - Loss: 0.7776\n",
      "Epoch 4/10 - Batch 87000 - Loss: 0.7138\n",
      "Epoch 4/10 - Batch 87100 - Loss: 0.9358\n",
      "Epoch 4/10 - Batch 87200 - Loss: 0.8356\n",
      "Epoch 4/10 - Batch 87300 - Loss: 0.6762\n",
      "Epoch 4/10 - Batch 87400 - Loss: 1.0503\n",
      "Epoch 4/10 - Batch 87500 - Loss: 0.7564\n",
      "Epoch 4/10 - Batch 87600 - Loss: 0.7139\n",
      "Epoch 4/10 - Batch 87700 - Loss: 0.7978\n",
      "Epoch 4/10 - Batch 87800 - Loss: 0.7008\n",
      "Epoch 4/10 - Batch 87900 - Loss: 0.8189\n",
      "Epoch 4/10 - Batch 88000 - Loss: 0.7477\n",
      "Epoch 4/10 - Batch 88100 - Loss: 0.7012\n",
      "Epoch 4/10 - Batch 88200 - Loss: 0.7310\n",
      "Epoch 4/10 - Batch 88300 - Loss: 0.7153\n",
      "Epoch 4/10 - Batch 88400 - Loss: 0.7624\n",
      "Epoch 4/10 - Batch 88500 - Loss: 0.8797\n",
      "Epoch 4/10 - Batch 88600 - Loss: 0.6450\n",
      "Epoch 4/10 - Batch 88700 - Loss: 0.7363\n",
      "Epoch 4/10 - Batch 88800 - Loss: 0.8222\n",
      "Epoch 4/10 - Batch 88900 - Loss: 0.7473\n",
      "Epoch 4/10 - Batch 89000 - Loss: 0.7799\n",
      "Epoch 4/10 - Batch 89100 - Loss: 0.8792\n",
      "Epoch 4/10 - Batch 89200 - Loss: 0.7830\n",
      "Epoch 4/10 - Batch 89300 - Loss: 0.6936\n",
      "Epoch 4/10 - Batch 89400 - Loss: 0.6560\n",
      "Epoch 4/10 - Batch 89500 - Loss: 0.8505\n",
      "Epoch 4/10 - Batch 89600 - Loss: 1.0062\n",
      "Epoch 4/10 - Batch 89700 - Loss: 0.7452\n",
      "Epoch 4/10 - Batch 89800 - Loss: 0.5800\n",
      "Epoch 4/10 - Batch 89900 - Loss: 0.7364\n",
      "Epoch 4/10 - Batch 90000 - Loss: 0.6943\n",
      "Epoch 4/10 - Batch 90100 - Loss: 0.8876\n",
      "Epoch 4/10 - Batch 90200 - Loss: 0.9224\n",
      "Epoch 4/10 - Batch 90300 - Loss: 0.9867\n",
      "Epoch 4/10 - Batch 90400 - Loss: 0.8194\n",
      "Epoch 4/10 - Batch 90500 - Loss: 0.8955\n",
      "Epoch 4/10 - Batch 90600 - Loss: 0.7784\n",
      "Epoch 4/10 - Batch 90700 - Loss: 0.9668\n",
      "Epoch 4/10 - Batch 90800 - Loss: 0.8719\n",
      "Epoch 4/10 - Batch 90900 - Loss: 0.8687\n",
      "Epoch 4/10 - Batch 91000 - Loss: 1.0468\n",
      "Epoch 4/10 - Batch 91100 - Loss: 0.7176\n",
      "Epoch 4/10 - Batch 91200 - Loss: 0.7397\n",
      "Epoch 4/10 - Batch 91300 - Loss: 0.7263\n",
      "Epoch 4/10 - Batch 91400 - Loss: 0.7951\n",
      "Epoch 4/10 - Batch 91500 - Loss: 0.9953\n",
      "Epoch 4/10 - Batch 91600 - Loss: 0.7086\n",
      "Epoch 4/10 - Batch 91700 - Loss: 0.7649\n",
      "Epoch 4/10 - Batch 91800 - Loss: 0.7890\n",
      "Epoch 4/10 - Batch 91900 - Loss: 0.6386\n",
      "Epoch 4/10 - Batch 92000 - Loss: 0.7165\n",
      "Epoch 4/10 - Batch 92100 - Loss: 0.6541\n",
      "Epoch 4/10 - Batch 92200 - Loss: 0.6741\n",
      "Epoch 4/10 - Batch 92300 - Loss: 0.8905\n",
      "Epoch 4/10 - Batch 92400 - Loss: 0.6596\n",
      "Epoch 4/10 - Batch 92500 - Loss: 0.6418\n",
      "Epoch 4/10 - Batch 92600 - Loss: 0.6401\n",
      "Epoch 4/10 - Batch 92700 - Loss: 0.5775\n",
      "Epoch 4/10 - Batch 92800 - Loss: 0.7018\n",
      "Epoch 4/10 - Batch 92900 - Loss: 0.4750\n",
      "Epoch 4/10 - Batch 93000 - Loss: 0.5279\n",
      "Epoch 4/10 - Batch 93100 - Loss: 0.4795\n",
      "Epoch 4/10 - Batch 93200 - Loss: 0.6179\n",
      "Epoch 4/10 - Batch 93300 - Loss: 0.5747\n",
      "Epoch 4/10 - Batch 93400 - Loss: 0.4711\n",
      "Epoch 4/10 - Batch 93500 - Loss: 0.4802\n",
      "Epoch 4/10 - Batch 93600 - Loss: 0.5361\n",
      "Epoch 4/10 - Batch 93700 - Loss: 0.6219\n",
      "Epoch 4/10 - Batch 93800 - Loss: 0.6293\n",
      "Epoch 4/10 - Batch 93900 - Loss: 0.6846\n",
      "Epoch 4/10 - Batch 94000 - Loss: 0.4293\n",
      "Epoch 4/10 - Batch 94100 - Loss: 0.5199\n",
      "Epoch 4/10 - Batch 94200 - Loss: 0.8906\n",
      "Epoch 4/10 - Batch 94300 - Loss: 0.5929\n",
      "Epoch 4/10 - Batch 94400 - Loss: 0.2339\n",
      "Epoch 4/10 - Batch 94500 - Loss: 0.6743\n",
      "Epoch 4/10 - Batch 94600 - Loss: 0.5661\n",
      "Epoch 4/10 - Batch 94700 - Loss: 0.5662\n",
      "Epoch 4/10 - Batch 94800 - Loss: 0.9022\n",
      "Epoch 4/10 - Batch 94900 - Loss: 0.7737\n",
      "Epoch 4/10 - Batch 95000 - Loss: 0.8754\n",
      "Epoch 4/10 - Batch 95100 - Loss: 0.9877\n",
      "Epoch 4/10 - Batch 95200 - Loss: 0.8056\n",
      "Epoch 4/10 - Batch 95300 - Loss: 0.9503\n",
      "Epoch 4/10 - Batch 95400 - Loss: 0.9071\n",
      "Epoch 4/10 - Batch 95500 - Loss: 0.7969\n",
      "Epoch 4/10 - Batch 95600 - Loss: 0.8472\n",
      "Epoch 4/10 - Batch 95700 - Loss: 0.8466\n",
      "Epoch 4/10 - Batch 95800 - Loss: 0.9088\n",
      "Epoch 4/10 - Batch 95900 - Loss: 0.6501\n",
      "Epoch 4/10 - Batch 96000 - Loss: 0.6079\n",
      "Epoch 4/10 - Batch 96100 - Loss: 0.9800\n",
      "Epoch 4/10 - Batch 96200 - Loss: 0.8321\n",
      "Epoch 4/10 - Batch 96300 - Loss: 0.6394\n",
      "Epoch 4/10 - Batch 96400 - Loss: 0.8037\n",
      "Epoch 4/10 - Batch 96500 - Loss: 0.6993\n",
      "Epoch 4/10 - Batch 96600 - Loss: 0.6398\n",
      "Epoch 4/10 - Batch 96700 - Loss: 0.9206\n",
      "Epoch 4/10 - Batch 96800 - Loss: 0.5282\n",
      "Epoch 4/10 - Batch 96900 - Loss: 0.8531\n",
      "Epoch 4/10 - Batch 97000 - Loss: 0.8133\n",
      "Epoch 4/10 - Batch 97100 - Loss: 0.9349\n",
      "Epoch 4/10 - Batch 97200 - Loss: 0.7554\n",
      "Epoch 4/10 - Batch 97300 - Loss: 0.7337\n",
      "Epoch 4/10 - Batch 97400 - Loss: 1.1752\n",
      "Epoch 4/10 - Batch 97500 - Loss: 0.7854\n",
      "Epoch 4/10 - Batch 97600 - Loss: 0.8317\n",
      "Epoch 4/10 - Batch 97700 - Loss: 0.9270\n",
      "Epoch 4/10 - Batch 97800 - Loss: 0.8046\n",
      "Epoch 4/10 - Batch 97900 - Loss: 0.8955\n",
      "Epoch 4/10 - Batch 98000 - Loss: 0.7141\n",
      "Epoch 4/10 - Batch 98100 - Loss: 0.6321\n",
      "Epoch 4/10 - Batch 98200 - Loss: 0.8453\n",
      "Epoch 4/10 - Batch 98300 - Loss: 0.5517\n",
      "Epoch 4/10 - Batch 98400 - Loss: 0.8440\n",
      "Epoch 4/10 - Batch 98500 - Loss: 0.9140\n",
      "Epoch 5/10 - Batch 98600 - Loss: 0.5017\n",
      "Epoch 5/10 - Batch 98700 - Loss: 0.3102\n",
      "Epoch 5/10 - Batch 98800 - Loss: 0.3646\n",
      "Epoch 5/10 - Batch 98900 - Loss: 0.2612\n",
      "Epoch 5/10 - Batch 99000 - Loss: 0.2559\n",
      "Epoch 5/10 - Batch 99100 - Loss: 0.2946\n",
      "Epoch 5/10 - Batch 99200 - Loss: 0.2774\n",
      "Epoch 5/10 - Batch 99300 - Loss: 0.2499\n",
      "Epoch 5/10 - Batch 99400 - Loss: 0.3014\n",
      "Epoch 5/10 - Batch 99500 - Loss: 0.2587\n",
      "Epoch 5/10 - Batch 99600 - Loss: 0.2303\n",
      "Epoch 5/10 - Batch 99700 - Loss: 0.2702\n",
      "Epoch 5/10 - Batch 99800 - Loss: 0.2957\n",
      "Epoch 5/10 - Batch 99900 - Loss: 0.2139\n",
      "Epoch 5/10 - Batch 100000 - Loss: 0.2396\n",
      "Epoch 5/10 - Batch 100100 - Loss: 0.2406\n",
      "Epoch 5/10 - Batch 100200 - Loss: 0.2762\n",
      "Epoch 5/10 - Batch 100300 - Loss: 0.2498\n",
      "Epoch 5/10 - Batch 100400 - Loss: 0.2650\n",
      "Epoch 5/10 - Batch 100500 - Loss: 0.3183\n",
      "Epoch 5/10 - Batch 100600 - Loss: 0.2442\n",
      "Epoch 5/10 - Batch 100700 - Loss: 0.2498\n",
      "Epoch 5/10 - Batch 100800 - Loss: 0.2709\n",
      "Epoch 5/10 - Batch 100900 - Loss: 0.2989\n",
      "Epoch 5/10 - Batch 101000 - Loss: 0.2562\n",
      "Epoch 5/10 - Batch 101100 - Loss: 0.2751\n",
      "Epoch 5/10 - Batch 101200 - Loss: 0.2748\n",
      "Epoch 5/10 - Batch 101300 - Loss: 0.2897\n",
      "Epoch 5/10 - Batch 101400 - Loss: 0.2938\n",
      "Epoch 5/10 - Batch 101500 - Loss: 0.2126\n",
      "Epoch 5/10 - Batch 101600 - Loss: 0.2757\n",
      "Epoch 5/10 - Batch 101700 - Loss: 0.2845\n",
      "Epoch 5/10 - Batch 101800 - Loss: 0.3673\n",
      "Epoch 5/10 - Batch 101900 - Loss: 0.2311\n",
      "Epoch 5/10 - Batch 102000 - Loss: 0.2932\n",
      "Epoch 5/10 - Batch 102100 - Loss: 0.3066\n",
      "Epoch 5/10 - Batch 102200 - Loss: 0.2824\n",
      "Epoch 5/10 - Batch 102300 - Loss: 0.2755\n",
      "Epoch 5/10 - Batch 102400 - Loss: 0.2454\n",
      "Epoch 5/10 - Batch 102500 - Loss: 0.2864\n",
      "Epoch 5/10 - Batch 102600 - Loss: 0.2510\n",
      "Epoch 5/10 - Batch 102700 - Loss: 0.2794\n",
      "Epoch 5/10 - Batch 102800 - Loss: 0.3305\n",
      "Epoch 5/10 - Batch 102900 - Loss: 0.3210\n",
      "Epoch 5/10 - Batch 103000 - Loss: 0.2720\n",
      "Epoch 5/10 - Batch 103100 - Loss: 0.3114\n",
      "Epoch 5/10 - Batch 103200 - Loss: 0.2266\n",
      "Epoch 5/10 - Batch 103300 - Loss: 0.2475\n",
      "Epoch 5/10 - Batch 103400 - Loss: 0.2539\n",
      "Epoch 5/10 - Batch 103500 - Loss: 0.2670\n",
      "Epoch 5/10 - Batch 103600 - Loss: 0.2667\n",
      "Epoch 5/10 - Batch 103700 - Loss: 0.3278\n",
      "Epoch 5/10 - Batch 103800 - Loss: 0.2629\n",
      "Epoch 5/10 - Batch 103900 - Loss: 0.2628\n",
      "Epoch 5/10 - Batch 104000 - Loss: 0.2504\n",
      "Epoch 5/10 - Batch 104100 - Loss: 0.2646\n",
      "Epoch 5/10 - Batch 104200 - Loss: 0.2523\n",
      "Epoch 5/10 - Batch 104300 - Loss: 0.2729\n",
      "Epoch 5/10 - Batch 104400 - Loss: 0.2883\n",
      "Epoch 5/10 - Batch 104500 - Loss: 0.2819\n",
      "Epoch 5/10 - Batch 104600 - Loss: 0.3081\n",
      "Epoch 5/10 - Batch 104700 - Loss: 0.2610\n",
      "Epoch 5/10 - Batch 104800 - Loss: 0.2763\n",
      "Epoch 5/10 - Batch 104900 - Loss: 0.3046\n",
      "Epoch 5/10 - Batch 105000 - Loss: 0.2380\n",
      "Epoch 5/10 - Batch 105100 - Loss: 0.3500\n",
      "Epoch 5/10 - Batch 105200 - Loss: 0.3132\n",
      "Epoch 5/10 - Batch 105300 - Loss: 0.2615\n",
      "Epoch 5/10 - Batch 105400 - Loss: 0.2881\n",
      "Epoch 5/10 - Batch 105500 - Loss: 0.2805\n",
      "Epoch 5/10 - Batch 105600 - Loss: 0.2834\n",
      "Epoch 5/10 - Batch 105700 - Loss: 0.2623\n",
      "Epoch 5/10 - Batch 105800 - Loss: 0.2635\n",
      "Epoch 5/10 - Batch 105900 - Loss: 0.2885\n",
      "Epoch 5/10 - Batch 106000 - Loss: 0.2831\n",
      "Epoch 5/10 - Batch 106100 - Loss: 0.3133\n",
      "Epoch 5/10 - Batch 106200 - Loss: 0.2823\n",
      "Epoch 5/10 - Batch 106300 - Loss: 0.2877\n",
      "Epoch 5/10 - Batch 106400 - Loss: 0.2886\n",
      "Epoch 5/10 - Batch 106500 - Loss: 0.3298\n",
      "Epoch 5/10 - Batch 106600 - Loss: 0.2482\n",
      "Epoch 5/10 - Batch 106700 - Loss: 0.2874\n",
      "Epoch 5/10 - Batch 106800 - Loss: 0.3345\n",
      "Epoch 5/10 - Batch 106900 - Loss: 0.2754\n",
      "Epoch 5/10 - Batch 107000 - Loss: 0.2584\n",
      "Epoch 5/10 - Batch 107100 - Loss: 0.2904\n",
      "Epoch 5/10 - Batch 107200 - Loss: 0.2777\n",
      "Epoch 5/10 - Batch 107300 - Loss: 0.3075\n",
      "Epoch 5/10 - Batch 107400 - Loss: 0.2893\n",
      "Epoch 5/10 - Batch 107500 - Loss: 0.3065\n",
      "Epoch 5/10 - Batch 107600 - Loss: 0.3136\n",
      "Epoch 5/10 - Batch 107700 - Loss: 0.3102\n",
      "Epoch 5/10 - Batch 107800 - Loss: 0.2516\n",
      "Epoch 5/10 - Batch 107900 - Loss: 0.2947\n",
      "Epoch 5/10 - Batch 108000 - Loss: 0.3456\n",
      "Epoch 5/10 - Batch 108100 - Loss: 0.3632\n",
      "Epoch 5/10 - Batch 108200 - Loss: 0.3450\n",
      "Epoch 5/10 - Batch 108300 - Loss: 0.3282\n",
      "Epoch 5/10 - Batch 108400 - Loss: 0.3248\n",
      "Epoch 5/10 - Batch 108500 - Loss: 0.3154\n",
      "Epoch 5/10 - Batch 108600 - Loss: 0.3723\n",
      "Epoch 5/10 - Batch 108700 - Loss: 0.3767\n",
      "Epoch 5/10 - Batch 108800 - Loss: 0.3051\n",
      "Epoch 5/10 - Batch 108900 - Loss: 0.3693\n",
      "Epoch 5/10 - Batch 109000 - Loss: 0.3575\n",
      "Epoch 5/10 - Batch 109100 - Loss: 0.3848\n",
      "Epoch 5/10 - Batch 109200 - Loss: 0.3823\n",
      "Epoch 5/10 - Batch 109300 - Loss: 0.3996\n",
      "Epoch 5/10 - Batch 109400 - Loss: 0.3735\n",
      "Epoch 5/10 - Batch 109500 - Loss: 0.4478\n",
      "Epoch 5/10 - Batch 109600 - Loss: 0.5975\n",
      "Epoch 5/10 - Batch 109700 - Loss: 0.7797\n",
      "Epoch 5/10 - Batch 109800 - Loss: 0.6873\n",
      "Epoch 5/10 - Batch 109900 - Loss: 0.7898\n",
      "Epoch 5/10 - Batch 110000 - Loss: 0.7039\n",
      "Epoch 5/10 - Batch 110100 - Loss: 0.8390\n",
      "Epoch 5/10 - Batch 110200 - Loss: 0.7190\n",
      "Epoch 5/10 - Batch 110300 - Loss: 0.7926\n",
      "Epoch 5/10 - Batch 110400 - Loss: 0.6373\n",
      "Epoch 5/10 - Batch 110500 - Loss: 0.7744\n",
      "Epoch 5/10 - Batch 110600 - Loss: 0.7278\n",
      "Epoch 5/10 - Batch 110700 - Loss: 0.7166\n",
      "Epoch 5/10 - Batch 110800 - Loss: 0.6150\n",
      "Epoch 5/10 - Batch 110900 - Loss: 0.5736\n",
      "Epoch 5/10 - Batch 111000 - Loss: 0.8761\n",
      "Epoch 5/10 - Batch 111100 - Loss: 0.6306\n",
      "Epoch 5/10 - Batch 111200 - Loss: 0.5795\n",
      "Epoch 5/10 - Batch 111300 - Loss: 0.8298\n",
      "Epoch 5/10 - Batch 111400 - Loss: 0.6436\n",
      "Epoch 5/10 - Batch 111500 - Loss: 0.6818\n",
      "Epoch 5/10 - Batch 111600 - Loss: 0.6378\n",
      "Epoch 5/10 - Batch 111700 - Loss: 0.7420\n",
      "Epoch 5/10 - Batch 111800 - Loss: 0.8906\n",
      "Epoch 5/10 - Batch 111900 - Loss: 0.6683\n",
      "Epoch 5/10 - Batch 112000 - Loss: 0.6699\n",
      "Epoch 5/10 - Batch 112100 - Loss: 1.0190\n",
      "Epoch 5/10 - Batch 112200 - Loss: 0.6203\n",
      "Epoch 5/10 - Batch 112300 - Loss: 0.6640\n",
      "Epoch 5/10 - Batch 112400 - Loss: 0.6390\n",
      "Epoch 5/10 - Batch 112500 - Loss: 0.7194\n",
      "Epoch 5/10 - Batch 112600 - Loss: 0.6421\n",
      "Epoch 5/10 - Batch 112700 - Loss: 0.6787\n",
      "Epoch 5/10 - Batch 112800 - Loss: 0.6594\n",
      "Epoch 5/10 - Batch 112900 - Loss: 0.7091\n",
      "Epoch 5/10 - Batch 113000 - Loss: 0.6021\n",
      "Epoch 5/10 - Batch 113100 - Loss: 0.7256\n",
      "Epoch 5/10 - Batch 113200 - Loss: 0.8206\n",
      "Epoch 5/10 - Batch 113300 - Loss: 0.4977\n",
      "Epoch 5/10 - Batch 113400 - Loss: 0.7994\n",
      "Epoch 5/10 - Batch 113500 - Loss: 0.7046\n",
      "Epoch 5/10 - Batch 113600 - Loss: 0.7445\n",
      "Epoch 5/10 - Batch 113700 - Loss: 0.7862\n",
      "Epoch 5/10 - Batch 113800 - Loss: 0.6827\n",
      "Epoch 5/10 - Batch 113900 - Loss: 0.6325\n",
      "Epoch 5/10 - Batch 114000 - Loss: 0.6479\n",
      "Epoch 5/10 - Batch 114100 - Loss: 0.6259\n",
      "Epoch 5/10 - Batch 114200 - Loss: 0.8654\n",
      "Epoch 5/10 - Batch 114300 - Loss: 0.9236\n",
      "Epoch 5/10 - Batch 114400 - Loss: 0.5591\n",
      "Epoch 5/10 - Batch 114500 - Loss: 0.5972\n",
      "Epoch 5/10 - Batch 114600 - Loss: 0.6803\n",
      "Epoch 5/10 - Batch 114700 - Loss: 0.7387\n",
      "Epoch 5/10 - Batch 114800 - Loss: 0.8506\n",
      "Epoch 5/10 - Batch 114900 - Loss: 0.8182\n",
      "Epoch 5/10 - Batch 115000 - Loss: 0.9242\n",
      "Epoch 5/10 - Batch 115100 - Loss: 0.7305\n",
      "Epoch 5/10 - Batch 115200 - Loss: 0.8122\n",
      "Epoch 5/10 - Batch 115300 - Loss: 0.7911\n",
      "Epoch 5/10 - Batch 115400 - Loss: 0.8812\n",
      "Epoch 5/10 - Batch 115500 - Loss: 0.7586\n",
      "Epoch 5/10 - Batch 115600 - Loss: 0.9133\n",
      "Epoch 5/10 - Batch 115700 - Loss: 0.8170\n",
      "Epoch 5/10 - Batch 115800 - Loss: 0.6238\n",
      "Epoch 5/10 - Batch 115900 - Loss: 0.7009\n",
      "Epoch 5/10 - Batch 116000 - Loss: 0.7683\n",
      "Epoch 5/10 - Batch 116100 - Loss: 0.8045\n",
      "Epoch 5/10 - Batch 116200 - Loss: 0.7544\n",
      "Epoch 5/10 - Batch 116300 - Loss: 0.7146\n",
      "Epoch 5/10 - Batch 116400 - Loss: 0.6280\n",
      "Epoch 5/10 - Batch 116500 - Loss: 0.6956\n",
      "Epoch 5/10 - Batch 116600 - Loss: 0.7015\n",
      "Epoch 5/10 - Batch 116700 - Loss: 0.5681\n",
      "Epoch 5/10 - Batch 116800 - Loss: 0.5899\n",
      "Epoch 5/10 - Batch 116900 - Loss: 0.8171\n",
      "Epoch 5/10 - Batch 117000 - Loss: 0.6315\n",
      "Epoch 5/10 - Batch 117100 - Loss: 0.5798\n",
      "Epoch 5/10 - Batch 117200 - Loss: 0.5913\n",
      "Epoch 5/10 - Batch 117300 - Loss: 0.5280\n",
      "Epoch 5/10 - Batch 117400 - Loss: 0.6414\n",
      "Epoch 5/10 - Batch 117500 - Loss: 0.4278\n",
      "Epoch 5/10 - Batch 117600 - Loss: 0.4823\n",
      "Epoch 5/10 - Batch 117700 - Loss: 0.4323\n",
      "Epoch 5/10 - Batch 117800 - Loss: 0.4026\n",
      "Epoch 5/10 - Batch 117900 - Loss: 0.6660\n",
      "Epoch 5/10 - Batch 118000 - Loss: 0.4414\n",
      "Epoch 5/10 - Batch 118100 - Loss: 0.4381\n",
      "Epoch 5/10 - Batch 118200 - Loss: 0.4502\n",
      "Epoch 5/10 - Batch 118300 - Loss: 0.5973\n",
      "Epoch 5/10 - Batch 118400 - Loss: 0.5025\n",
      "Epoch 5/10 - Batch 118500 - Loss: 0.6809\n",
      "Epoch 5/10 - Batch 118600 - Loss: 0.4216\n",
      "Epoch 5/10 - Batch 118700 - Loss: 0.4693\n",
      "Epoch 5/10 - Batch 118800 - Loss: 0.7112\n",
      "Epoch 5/10 - Batch 118900 - Loss: 0.8058\n",
      "Epoch 5/10 - Batch 119000 - Loss: 0.1409\n",
      "Epoch 5/10 - Batch 119100 - Loss: 0.5226\n",
      "Epoch 5/10 - Batch 119200 - Loss: 0.5907\n",
      "Epoch 5/10 - Batch 119300 - Loss: 0.4133\n",
      "Epoch 5/10 - Batch 119400 - Loss: 0.7113\n",
      "Epoch 5/10 - Batch 119500 - Loss: 0.8552\n",
      "Epoch 5/10 - Batch 119600 - Loss: 0.6661\n",
      "Epoch 5/10 - Batch 119700 - Loss: 1.0141\n",
      "Epoch 5/10 - Batch 119800 - Loss: 0.7964\n",
      "Epoch 5/10 - Batch 119900 - Loss: 0.6324\n",
      "Epoch 5/10 - Batch 120000 - Loss: 0.9761\n",
      "Epoch 5/10 - Batch 120100 - Loss: 0.7048\n",
      "Epoch 5/10 - Batch 120200 - Loss: 0.8614\n",
      "Epoch 5/10 - Batch 120300 - Loss: 0.7407\n",
      "Epoch 5/10 - Batch 120400 - Loss: 0.8644\n",
      "Epoch 5/10 - Batch 120500 - Loss: 0.6597\n",
      "Epoch 5/10 - Batch 120600 - Loss: 0.6186\n",
      "Epoch 5/10 - Batch 120700 - Loss: 0.7150\n",
      "Epoch 5/10 - Batch 120800 - Loss: 0.8456\n",
      "Epoch 5/10 - Batch 120900 - Loss: 0.5937\n",
      "Epoch 5/10 - Batch 121000 - Loss: 0.7053\n",
      "Epoch 5/10 - Batch 121100 - Loss: 0.6744\n",
      "Epoch 5/10 - Batch 121200 - Loss: 0.5650\n",
      "Epoch 5/10 - Batch 121300 - Loss: 0.8029\n",
      "Epoch 5/10 - Batch 121400 - Loss: 0.5962\n",
      "Epoch 5/10 - Batch 121500 - Loss: 0.6063\n",
      "Epoch 5/10 - Batch 121600 - Loss: 0.7813\n",
      "Epoch 5/10 - Batch 121700 - Loss: 0.9125\n",
      "Epoch 5/10 - Batch 121800 - Loss: 0.7477\n",
      "Epoch 5/10 - Batch 121900 - Loss: 0.6478\n",
      "Epoch 5/10 - Batch 122000 - Loss: 0.8400\n",
      "Epoch 5/10 - Batch 122100 - Loss: 1.0261\n",
      "Epoch 5/10 - Batch 122200 - Loss: 0.7609\n",
      "Epoch 5/10 - Batch 122300 - Loss: 0.8782\n",
      "Epoch 5/10 - Batch 122400 - Loss: 0.6807\n",
      "Epoch 5/10 - Batch 122500 - Loss: 0.7898\n",
      "Epoch 5/10 - Batch 122600 - Loss: 0.7308\n",
      "Epoch 5/10 - Batch 122700 - Loss: 0.6528\n",
      "Epoch 5/10 - Batch 122800 - Loss: 0.6906\n",
      "Epoch 5/10 - Batch 122900 - Loss: 0.6693\n",
      "Epoch 5/10 - Batch 123000 - Loss: 0.5761\n",
      "Epoch 5/10 - Batch 123100 - Loss: 0.9774\n",
      "Epoch 6/10 - Batch 123200 - Loss: 0.5988\n",
      "Epoch 6/10 - Batch 123300 - Loss: 0.3190\n",
      "Epoch 6/10 - Batch 123400 - Loss: 0.3292\n",
      "Epoch 6/10 - Batch 123500 - Loss: 0.2279\n",
      "Epoch 6/10 - Batch 123600 - Loss: 0.2305\n",
      "Epoch 6/10 - Batch 123700 - Loss: 0.2332\n",
      "Epoch 6/10 - Batch 123800 - Loss: 0.2311\n",
      "Epoch 6/10 - Batch 123900 - Loss: 0.2221\n",
      "Epoch 6/10 - Batch 124000 - Loss: 0.2170\n",
      "Epoch 6/10 - Batch 124100 - Loss: 0.2456\n",
      "Epoch 6/10 - Batch 124200 - Loss: 0.2199\n",
      "Epoch 6/10 - Batch 124300 - Loss: 0.2499\n",
      "Epoch 6/10 - Batch 124400 - Loss: 0.2133\n",
      "Epoch 6/10 - Batch 124500 - Loss: 0.2521\n",
      "Epoch 6/10 - Batch 124600 - Loss: 0.1875\n",
      "Epoch 6/10 - Batch 124700 - Loss: 0.2229\n",
      "Epoch 6/10 - Batch 124800 - Loss: 0.2255\n",
      "Epoch 6/10 - Batch 124900 - Loss: 0.1892\n",
      "Epoch 6/10 - Batch 125000 - Loss: 0.2391\n",
      "Epoch 6/10 - Batch 125100 - Loss: 0.2827\n",
      "Epoch 6/10 - Batch 125200 - Loss: 0.2146\n",
      "Epoch 6/10 - Batch 125300 - Loss: 0.1943\n",
      "Epoch 6/10 - Batch 125400 - Loss: 0.2494\n",
      "Epoch 6/10 - Batch 125500 - Loss: 0.2740\n",
      "Epoch 6/10 - Batch 125600 - Loss: 0.2231\n",
      "Epoch 6/10 - Batch 125700 - Loss: 0.1973\n",
      "Epoch 6/10 - Batch 125800 - Loss: 0.2514\n",
      "Epoch 6/10 - Batch 125900 - Loss: 0.2505\n",
      "Epoch 6/10 - Batch 126000 - Loss: 0.2769\n",
      "Epoch 6/10 - Batch 126100 - Loss: 0.1742\n",
      "Epoch 6/10 - Batch 126200 - Loss: 0.2505\n",
      "Epoch 6/10 - Batch 126300 - Loss: 0.2658\n",
      "Epoch 6/10 - Batch 126400 - Loss: 0.2829\n",
      "Epoch 6/10 - Batch 126500 - Loss: 0.2656\n",
      "Epoch 6/10 - Batch 126600 - Loss: 0.2213\n",
      "Epoch 6/10 - Batch 126700 - Loss: 0.2570\n",
      "Epoch 6/10 - Batch 126800 - Loss: 0.2376\n",
      "Epoch 6/10 - Batch 126900 - Loss: 0.2502\n",
      "Epoch 6/10 - Batch 127000 - Loss: 0.2447\n",
      "Epoch 6/10 - Batch 127100 - Loss: 0.2068\n",
      "Epoch 6/10 - Batch 127200 - Loss: 0.2202\n",
      "Epoch 6/10 - Batch 127300 - Loss: 0.2262\n",
      "Epoch 6/10 - Batch 127400 - Loss: 0.2695\n",
      "Epoch 6/10 - Batch 127500 - Loss: 0.2966\n",
      "Epoch 6/10 - Batch 127600 - Loss: 0.2486\n",
      "Epoch 6/10 - Batch 127700 - Loss: 0.2471\n",
      "Epoch 6/10 - Batch 127800 - Loss: 0.1953\n",
      "Epoch 6/10 - Batch 127900 - Loss: 0.2378\n",
      "Epoch 6/10 - Batch 128000 - Loss: 0.2100\n",
      "Epoch 6/10 - Batch 128100 - Loss: 0.2300\n",
      "Epoch 6/10 - Batch 128200 - Loss: 0.2550\n",
      "Epoch 6/10 - Batch 128300 - Loss: 0.2225\n",
      "Epoch 6/10 - Batch 128400 - Loss: 0.2788\n",
      "Epoch 6/10 - Batch 128500 - Loss: 0.2172\n",
      "Epoch 6/10 - Batch 128600 - Loss: 0.2259\n",
      "Epoch 6/10 - Batch 128700 - Loss: 0.2403\n",
      "Epoch 6/10 - Batch 128800 - Loss: 0.1952\n",
      "Epoch 6/10 - Batch 128900 - Loss: 0.2289\n",
      "Epoch 6/10 - Batch 129000 - Loss: 0.2491\n",
      "Epoch 6/10 - Batch 129100 - Loss: 0.2360\n",
      "Epoch 6/10 - Batch 129200 - Loss: 0.2337\n",
      "Epoch 6/10 - Batch 129300 - Loss: 0.2467\n",
      "Epoch 6/10 - Batch 129400 - Loss: 0.2353\n",
      "Epoch 6/10 - Batch 129500 - Loss: 0.2720\n",
      "Epoch 6/10 - Batch 129600 - Loss: 0.2071\n",
      "Epoch 6/10 - Batch 129700 - Loss: 0.2727\n",
      "Epoch 6/10 - Batch 129800 - Loss: 0.3062\n",
      "Epoch 6/10 - Batch 129900 - Loss: 0.2493\n",
      "Epoch 6/10 - Batch 130000 - Loss: 0.2566\n",
      "Epoch 6/10 - Batch 130100 - Loss: 0.2305\n",
      "Epoch 6/10 - Batch 130200 - Loss: 0.2301\n",
      "Epoch 6/10 - Batch 130300 - Loss: 0.2642\n",
      "Epoch 6/10 - Batch 130400 - Loss: 0.2572\n",
      "Epoch 6/10 - Batch 130500 - Loss: 0.2253\n",
      "Epoch 6/10 - Batch 130600 - Loss: 0.2699\n",
      "Epoch 6/10 - Batch 130700 - Loss: 0.2635\n",
      "Epoch 6/10 - Batch 130800 - Loss: 0.2434\n",
      "Epoch 6/10 - Batch 130900 - Loss: 0.2614\n",
      "Epoch 6/10 - Batch 131000 - Loss: 0.2790\n",
      "Epoch 6/10 - Batch 131100 - Loss: 0.2553\n",
      "Epoch 6/10 - Batch 131200 - Loss: 0.2602\n",
      "Epoch 6/10 - Batch 131300 - Loss: 0.2236\n",
      "Epoch 6/10 - Batch 131400 - Loss: 0.2865\n",
      "Epoch 6/10 - Batch 131500 - Loss: 0.2395\n",
      "Epoch 6/10 - Batch 131600 - Loss: 0.2441\n",
      "Epoch 6/10 - Batch 131700 - Loss: 0.2374\n",
      "Epoch 6/10 - Batch 131800 - Loss: 0.2331\n",
      "Epoch 6/10 - Batch 131900 - Loss: 0.2706\n",
      "Epoch 6/10 - Batch 132000 - Loss: 0.2569\n",
      "Epoch 6/10 - Batch 132100 - Loss: 0.2616\n",
      "Epoch 6/10 - Batch 132200 - Loss: 0.2722\n",
      "Epoch 6/10 - Batch 132300 - Loss: 0.2786\n",
      "Epoch 6/10 - Batch 132400 - Loss: 0.2319\n",
      "Epoch 6/10 - Batch 132500 - Loss: 0.2529\n",
      "Epoch 6/10 - Batch 132600 - Loss: 0.3037\n",
      "Epoch 6/10 - Batch 132700 - Loss: 0.2845\n",
      "Epoch 6/10 - Batch 132800 - Loss: 0.2919\n",
      "Epoch 6/10 - Batch 132900 - Loss: 0.2933\n",
      "Epoch 6/10 - Batch 133000 - Loss: 0.2650\n",
      "Epoch 6/10 - Batch 133100 - Loss: 0.2892\n",
      "Epoch 6/10 - Batch 133200 - Loss: 0.2951\n",
      "Epoch 6/10 - Batch 133300 - Loss: 0.3615\n",
      "Epoch 6/10 - Batch 133400 - Loss: 0.2688\n",
      "Epoch 6/10 - Batch 133500 - Loss: 0.3361\n",
      "Epoch 6/10 - Batch 133600 - Loss: 0.3021\n",
      "Epoch 6/10 - Batch 133700 - Loss: 0.3321\n",
      "Epoch 6/10 - Batch 133800 - Loss: 0.3291\n",
      "Epoch 6/10 - Batch 133900 - Loss: 0.3596\n",
      "Epoch 6/10 - Batch 134000 - Loss: 0.3288\n",
      "Epoch 6/10 - Batch 134100 - Loss: 0.3783\n",
      "Epoch 6/10 - Batch 134200 - Loss: 0.4846\n",
      "Epoch 6/10 - Batch 134300 - Loss: 0.6561\n",
      "Epoch 6/10 - Batch 134400 - Loss: 0.6543\n",
      "Epoch 6/10 - Batch 134500 - Loss: 0.7926\n",
      "Epoch 6/10 - Batch 134600 - Loss: 0.6234\n",
      "Epoch 6/10 - Batch 134700 - Loss: 0.6526\n",
      "Epoch 6/10 - Batch 134800 - Loss: 0.7989\n",
      "Epoch 6/10 - Batch 134900 - Loss: 0.6470\n",
      "Epoch 6/10 - Batch 135000 - Loss: 0.6847\n",
      "Epoch 6/10 - Batch 135100 - Loss: 0.6506\n",
      "Epoch 6/10 - Batch 135200 - Loss: 0.7435\n",
      "Epoch 6/10 - Batch 135300 - Loss: 0.6908\n",
      "Epoch 6/10 - Batch 135400 - Loss: 0.5140\n",
      "Epoch 6/10 - Batch 135500 - Loss: 0.5809\n",
      "Epoch 6/10 - Batch 135600 - Loss: 0.5790\n",
      "Epoch 6/10 - Batch 135700 - Loss: 0.7478\n",
      "Epoch 6/10 - Batch 135800 - Loss: 0.6035\n",
      "Epoch 6/10 - Batch 135900 - Loss: 0.6792\n",
      "Epoch 6/10 - Batch 136000 - Loss: 0.7351\n",
      "Epoch 6/10 - Batch 136100 - Loss: 0.5299\n",
      "Epoch 6/10 - Batch 136200 - Loss: 0.6024\n",
      "Epoch 6/10 - Batch 136300 - Loss: 0.6548\n",
      "Epoch 6/10 - Batch 136400 - Loss: 0.8585\n",
      "Epoch 6/10 - Batch 136500 - Loss: 0.6702\n",
      "Epoch 6/10 - Batch 136600 - Loss: 0.6535\n",
      "Epoch 6/10 - Batch 136700 - Loss: 0.9121\n",
      "Epoch 6/10 - Batch 136800 - Loss: 0.5692\n",
      "Epoch 6/10 - Batch 136900 - Loss: 0.6043\n",
      "Epoch 6/10 - Batch 137000 - Loss: 0.6521\n",
      "Epoch 6/10 - Batch 137100 - Loss: 0.6033\n",
      "Epoch 6/10 - Batch 137200 - Loss: 0.6743\n",
      "Epoch 6/10 - Batch 137300 - Loss: 0.6524\n",
      "Epoch 6/10 - Batch 137400 - Loss: 0.5335\n",
      "Epoch 6/10 - Batch 137500 - Loss: 0.6031\n",
      "Epoch 6/10 - Batch 137600 - Loss: 0.6074\n",
      "Epoch 6/10 - Batch 137700 - Loss: 0.6339\n",
      "Epoch 6/10 - Batch 137800 - Loss: 0.7918\n",
      "Epoch 6/10 - Batch 137900 - Loss: 0.4553\n",
      "Epoch 6/10 - Batch 138000 - Loss: 0.6983\n",
      "Epoch 6/10 - Batch 138100 - Loss: 0.6786\n",
      "Epoch 6/10 - Batch 138200 - Loss: 0.6401\n",
      "Epoch 6/10 - Batch 138300 - Loss: 0.6969\n",
      "Epoch 6/10 - Batch 138400 - Loss: 0.7548\n",
      "Epoch 6/10 - Batch 138500 - Loss: 0.5902\n",
      "Epoch 6/10 - Batch 138600 - Loss: 0.5335\n",
      "Epoch 6/10 - Batch 138700 - Loss: 0.5872\n",
      "Epoch 6/10 - Batch 138800 - Loss: 0.7705\n",
      "Epoch 6/10 - Batch 138900 - Loss: 0.8728\n",
      "Epoch 6/10 - Batch 139000 - Loss: 0.5805\n",
      "Epoch 6/10 - Batch 139100 - Loss: 0.4965\n",
      "Epoch 6/10 - Batch 139200 - Loss: 0.6341\n",
      "Epoch 6/10 - Batch 139300 - Loss: 0.6081\n",
      "Epoch 6/10 - Batch 139400 - Loss: 0.8423\n",
      "Epoch 6/10 - Batch 139500 - Loss: 0.7661\n",
      "Epoch 6/10 - Batch 139600 - Loss: 0.8487\n",
      "Epoch 6/10 - Batch 139700 - Loss: 0.7291\n",
      "Epoch 6/10 - Batch 139800 - Loss: 0.7658\n",
      "Epoch 6/10 - Batch 139900 - Loss: 0.7243\n",
      "Epoch 6/10 - Batch 140000 - Loss: 0.8750\n",
      "Epoch 6/10 - Batch 140100 - Loss: 0.6506\n",
      "Epoch 6/10 - Batch 140200 - Loss: 0.7852\n",
      "Epoch 6/10 - Batch 140300 - Loss: 0.8693\n",
      "Epoch 6/10 - Batch 140400 - Loss: 0.5708\n",
      "Epoch 6/10 - Batch 140500 - Loss: 0.6720\n",
      "Epoch 6/10 - Batch 140600 - Loss: 0.6034\n",
      "Epoch 6/10 - Batch 140700 - Loss: 0.7428\n",
      "Epoch 6/10 - Batch 140800 - Loss: 0.8159\n",
      "Epoch 6/10 - Batch 140900 - Loss: 0.6234\n",
      "Epoch 6/10 - Batch 141000 - Loss: 0.6000\n",
      "Epoch 6/10 - Batch 141100 - Loss: 0.7803\n",
      "Epoch 6/10 - Batch 141200 - Loss: 0.5277\n",
      "Epoch 6/10 - Batch 141300 - Loss: 0.5542\n",
      "Epoch 6/10 - Batch 141400 - Loss: 0.5621\n",
      "Epoch 6/10 - Batch 141500 - Loss: 0.6421\n",
      "Epoch 6/10 - Batch 141600 - Loss: 0.6741\n",
      "Epoch 6/10 - Batch 141700 - Loss: 0.5356\n",
      "Epoch 6/10 - Batch 141800 - Loss: 0.5863\n",
      "Epoch 6/10 - Batch 141900 - Loss: 0.5373\n",
      "Epoch 6/10 - Batch 142000 - Loss: 0.4671\n",
      "Epoch 6/10 - Batch 142100 - Loss: 0.5517\n",
      "Epoch 6/10 - Batch 142200 - Loss: 0.4046\n",
      "Epoch 6/10 - Batch 142300 - Loss: 0.4105\n",
      "Epoch 6/10 - Batch 142400 - Loss: 0.3794\n",
      "Epoch 6/10 - Batch 142500 - Loss: 0.6464\n",
      "Epoch 6/10 - Batch 142600 - Loss: 0.3832\n",
      "Epoch 6/10 - Batch 142700 - Loss: 0.3594\n",
      "Epoch 6/10 - Batch 142800 - Loss: 0.4459\n",
      "Epoch 6/10 - Batch 142900 - Loss: 0.5124\n",
      "Epoch 6/10 - Batch 143000 - Loss: 0.5026\n",
      "Epoch 6/10 - Batch 143100 - Loss: 0.5035\n",
      "Epoch 6/10 - Batch 143200 - Loss: 0.5369\n",
      "Epoch 6/10 - Batch 143300 - Loss: 0.4084\n",
      "Epoch 6/10 - Batch 143400 - Loss: 0.4219\n",
      "Epoch 6/10 - Batch 143500 - Loss: 0.8759\n",
      "Epoch 6/10 - Batch 143600 - Loss: 0.3292\n",
      "Epoch 6/10 - Batch 143700 - Loss: 0.2735\n",
      "Epoch 6/10 - Batch 143800 - Loss: 0.5630\n",
      "Epoch 6/10 - Batch 143900 - Loss: 0.4342\n",
      "Epoch 6/10 - Batch 144000 - Loss: 0.5193\n",
      "Epoch 6/10 - Batch 144100 - Loss: 0.7938\n",
      "Epoch 6/10 - Batch 144200 - Loss: 0.6827\n",
      "Epoch 6/10 - Batch 144300 - Loss: 0.8026\n",
      "Epoch 6/10 - Batch 144400 - Loss: 0.7856\n",
      "Epoch 6/10 - Batch 144500 - Loss: 0.7197\n",
      "Epoch 6/10 - Batch 144600 - Loss: 0.8379\n",
      "Epoch 6/10 - Batch 144700 - Loss: 0.7229\n",
      "Epoch 6/10 - Batch 144800 - Loss: 0.7748\n",
      "Epoch 6/10 - Batch 144900 - Loss: 0.7272\n",
      "Epoch 6/10 - Batch 145000 - Loss: 0.7398\n",
      "Epoch 6/10 - Batch 145100 - Loss: 0.6694\n",
      "Epoch 6/10 - Batch 145200 - Loss: 0.5700\n",
      "Epoch 6/10 - Batch 145300 - Loss: 0.6381\n",
      "Epoch 6/10 - Batch 145400 - Loss: 0.7866\n",
      "Epoch 6/10 - Batch 145500 - Loss: 0.6593\n",
      "Epoch 6/10 - Batch 145600 - Loss: 0.5567\n",
      "Epoch 6/10 - Batch 145700 - Loss: 0.7221\n",
      "Epoch 6/10 - Batch 145800 - Loss: 0.4644\n",
      "Epoch 6/10 - Batch 145900 - Loss: 0.6222\n",
      "Epoch 6/10 - Batch 146000 - Loss: 0.7559\n",
      "Epoch 6/10 - Batch 146100 - Loss: 0.4921\n",
      "Epoch 6/10 - Batch 146200 - Loss: 0.7248\n",
      "Epoch 6/10 - Batch 146300 - Loss: 0.7741\n",
      "Epoch 6/10 - Batch 146400 - Loss: 0.7223\n",
      "Epoch 6/10 - Batch 146500 - Loss: 0.6478\n",
      "Epoch 6/10 - Batch 146600 - Loss: 0.6689\n",
      "Epoch 6/10 - Batch 146700 - Loss: 1.0956\n",
      "Epoch 6/10 - Batch 146800 - Loss: 0.6814\n",
      "Epoch 6/10 - Batch 146900 - Loss: 0.6890\n",
      "Epoch 6/10 - Batch 147000 - Loss: 0.7989\n",
      "Epoch 6/10 - Batch 147100 - Loss: 0.7623\n",
      "Epoch 6/10 - Batch 147200 - Loss: 0.6864\n",
      "Epoch 6/10 - Batch 147300 - Loss: 0.6368\n",
      "Epoch 6/10 - Batch 147400 - Loss: 0.5731\n",
      "Epoch 6/10 - Batch 147500 - Loss: 0.7117\n",
      "Epoch 6/10 - Batch 147600 - Loss: 0.4457\n",
      "Epoch 6/10 - Batch 147700 - Loss: 0.8318\n",
      "Epoch 6/10 - Batch 147800 - Loss: 0.6968\n",
      "Epoch 7/10 - Batch 147900 - Loss: 0.3296\n",
      "Epoch 7/10 - Batch 148000 - Loss: 0.2266\n",
      "Epoch 7/10 - Batch 148100 - Loss: 0.2817\n",
      "Epoch 7/10 - Batch 148200 - Loss: 0.1881\n",
      "Epoch 7/10 - Batch 148300 - Loss: 0.1876\n",
      "Epoch 7/10 - Batch 148400 - Loss: 0.2517\n",
      "Epoch 7/10 - Batch 148500 - Loss: 0.2074\n",
      "Epoch 7/10 - Batch 148600 - Loss: 0.1992\n",
      "Epoch 7/10 - Batch 148700 - Loss: 0.2354\n",
      "Epoch 7/10 - Batch 148800 - Loss: 0.1857\n",
      "Epoch 7/10 - Batch 148900 - Loss: 0.1681\n",
      "Epoch 7/10 - Batch 149000 - Loss: 0.1964\n",
      "Epoch 7/10 - Batch 149100 - Loss: 0.2256\n",
      "Epoch 7/10 - Batch 149200 - Loss: 0.1430\n",
      "Epoch 7/10 - Batch 149300 - Loss: 0.1792\n",
      "Epoch 7/10 - Batch 149400 - Loss: 0.1700\n",
      "Epoch 7/10 - Batch 149500 - Loss: 0.2035\n",
      "Epoch 7/10 - Batch 149600 - Loss: 0.1768\n",
      "Epoch 7/10 - Batch 149700 - Loss: 0.2135\n",
      "Epoch 7/10 - Batch 149800 - Loss: 0.2313\n",
      "Epoch 7/10 - Batch 149900 - Loss: 0.1801\n",
      "Epoch 7/10 - Batch 150000 - Loss: 0.2227\n",
      "Epoch 7/10 - Batch 150100 - Loss: 0.2164\n",
      "Epoch 7/10 - Batch 150200 - Loss: 0.2061\n",
      "Epoch 7/10 - Batch 150300 - Loss: 0.1990\n",
      "Epoch 7/10 - Batch 150400 - Loss: 0.2131\n",
      "Epoch 7/10 - Batch 150500 - Loss: 0.2351\n",
      "Epoch 7/10 - Batch 150600 - Loss: 0.2145\n",
      "Epoch 7/10 - Batch 150700 - Loss: 0.2197\n",
      "Epoch 7/10 - Batch 150800 - Loss: 0.1900\n",
      "Epoch 7/10 - Batch 150900 - Loss: 0.2096\n",
      "Epoch 7/10 - Batch 151000 - Loss: 0.2151\n",
      "Epoch 7/10 - Batch 151100 - Loss: 0.2453\n",
      "Epoch 7/10 - Batch 151200 - Loss: 0.1800\n",
      "Epoch 7/10 - Batch 151300 - Loss: 0.2287\n",
      "Epoch 7/10 - Batch 151400 - Loss: 0.2201\n",
      "Epoch 7/10 - Batch 151500 - Loss: 0.2315\n",
      "Epoch 7/10 - Batch 151600 - Loss: 0.1867\n",
      "Epoch 7/10 - Batch 151700 - Loss: 0.1833\n",
      "Epoch 7/10 - Batch 151800 - Loss: 0.2011\n",
      "Epoch 7/10 - Batch 151900 - Loss: 0.1982\n",
      "Epoch 7/10 - Batch 152000 - Loss: 0.2191\n",
      "Epoch 7/10 - Batch 152100 - Loss: 0.2564\n",
      "Epoch 7/10 - Batch 152200 - Loss: 0.2234\n",
      "Epoch 7/10 - Batch 152300 - Loss: 0.2191\n",
      "Epoch 7/10 - Batch 152400 - Loss: 0.2089\n",
      "Epoch 7/10 - Batch 152500 - Loss: 0.1755\n",
      "Epoch 7/10 - Batch 152600 - Loss: 0.1916\n",
      "Epoch 7/10 - Batch 152700 - Loss: 0.2031\n",
      "Epoch 7/10 - Batch 152800 - Loss: 0.2178\n",
      "Epoch 7/10 - Batch 152900 - Loss: 0.1889\n",
      "Epoch 7/10 - Batch 153000 - Loss: 0.2317\n",
      "Epoch 7/10 - Batch 153100 - Loss: 0.1908\n",
      "Epoch 7/10 - Batch 153200 - Loss: 0.1960\n",
      "Epoch 7/10 - Batch 153300 - Loss: 0.1613\n",
      "Epoch 7/10 - Batch 153400 - Loss: 0.2085\n",
      "Epoch 7/10 - Batch 153500 - Loss: 0.2097\n",
      "Epoch 7/10 - Batch 153600 - Loss: 0.2086\n",
      "Epoch 7/10 - Batch 153700 - Loss: 0.2296\n",
      "Epoch 7/10 - Batch 153800 - Loss: 0.2010\n",
      "Epoch 7/10 - Batch 153900 - Loss: 0.2334\n",
      "Epoch 7/10 - Batch 154000 - Loss: 0.2035\n",
      "Epoch 7/10 - Batch 154100 - Loss: 0.2372\n",
      "Epoch 7/10 - Batch 154200 - Loss: 0.2140\n",
      "Epoch 7/10 - Batch 154300 - Loss: 0.1940\n",
      "Epoch 7/10 - Batch 154400 - Loss: 0.2672\n",
      "Epoch 7/10 - Batch 154500 - Loss: 0.2497\n",
      "Epoch 7/10 - Batch 154600 - Loss: 0.2047\n",
      "Epoch 7/10 - Batch 154700 - Loss: 0.2409\n",
      "Epoch 7/10 - Batch 154800 - Loss: 0.2039\n",
      "Epoch 7/10 - Batch 154900 - Loss: 0.2286\n",
      "Epoch 7/10 - Batch 155000 - Loss: 0.2252\n",
      "Epoch 7/10 - Batch 155100 - Loss: 0.1799\n",
      "Epoch 7/10 - Batch 155200 - Loss: 0.2336\n",
      "Epoch 7/10 - Batch 155300 - Loss: 0.2210\n",
      "Epoch 7/10 - Batch 155400 - Loss: 0.2381\n",
      "Epoch 7/10 - Batch 155500 - Loss: 0.1922\n",
      "Epoch 7/10 - Batch 155600 - Loss: 0.2478\n",
      "Epoch 7/10 - Batch 155700 - Loss: 0.2229\n",
      "Epoch 7/10 - Batch 155800 - Loss: 0.2649\n",
      "Epoch 7/10 - Batch 155900 - Loss: 0.2025\n",
      "Epoch 7/10 - Batch 156000 - Loss: 0.2065\n",
      "Epoch 7/10 - Batch 156100 - Loss: 0.2293\n",
      "Epoch 7/10 - Batch 156200 - Loss: 0.2198\n",
      "Epoch 7/10 - Batch 156300 - Loss: 0.1934\n",
      "Epoch 7/10 - Batch 156400 - Loss: 0.2162\n",
      "Epoch 7/10 - Batch 156500 - Loss: 0.2386\n",
      "Epoch 7/10 - Batch 156600 - Loss: 0.2299\n",
      "Epoch 7/10 - Batch 156700 - Loss: 0.2449\n",
      "Epoch 7/10 - Batch 156800 - Loss: 0.2438\n",
      "Epoch 7/10 - Batch 156900 - Loss: 0.2465\n",
      "Epoch 7/10 - Batch 157000 - Loss: 0.2264\n",
      "Epoch 7/10 - Batch 157100 - Loss: 0.2113\n",
      "Epoch 7/10 - Batch 157200 - Loss: 0.2290\n",
      "Epoch 7/10 - Batch 157300 - Loss: 0.2639\n",
      "Epoch 7/10 - Batch 157400 - Loss: 0.2743\n",
      "Epoch 7/10 - Batch 157500 - Loss: 0.2463\n",
      "Epoch 7/10 - Batch 157600 - Loss: 0.2696\n",
      "Epoch 7/10 - Batch 157700 - Loss: 0.2435\n",
      "Epoch 7/10 - Batch 157800 - Loss: 0.2495\n",
      "Epoch 7/10 - Batch 157900 - Loss: 0.3015\n",
      "Epoch 7/10 - Batch 158000 - Loss: 0.2598\n",
      "Epoch 7/10 - Batch 158100 - Loss: 0.2781\n",
      "Epoch 7/10 - Batch 158200 - Loss: 0.2785\n",
      "Epoch 7/10 - Batch 158300 - Loss: 0.2683\n",
      "Epoch 7/10 - Batch 158400 - Loss: 0.2888\n",
      "Epoch 7/10 - Batch 158500 - Loss: 0.3121\n",
      "Epoch 7/10 - Batch 158600 - Loss: 0.3250\n",
      "Epoch 7/10 - Batch 158700 - Loss: 0.2931\n",
      "Epoch 7/10 - Batch 158800 - Loss: 0.3894\n",
      "Epoch 7/10 - Batch 158900 - Loss: 0.5632\n",
      "Epoch 7/10 - Batch 159000 - Loss: 0.6719\n",
      "Epoch 7/10 - Batch 159100 - Loss: 0.6683\n",
      "Epoch 7/10 - Batch 159200 - Loss: 0.6334\n",
      "Epoch 7/10 - Batch 159300 - Loss: 0.6155\n",
      "Epoch 7/10 - Batch 159400 - Loss: 0.7316\n",
      "Epoch 7/10 - Batch 159500 - Loss: 0.6591\n",
      "Epoch 7/10 - Batch 159600 - Loss: 0.6538\n",
      "Epoch 7/10 - Batch 159700 - Loss: 0.5670\n",
      "Epoch 7/10 - Batch 159800 - Loss: 0.6732\n",
      "Epoch 7/10 - Batch 159900 - Loss: 0.6396\n",
      "Epoch 7/10 - Batch 160000 - Loss: 0.5835\n",
      "Epoch 7/10 - Batch 160100 - Loss: 0.5051\n",
      "Epoch 7/10 - Batch 160200 - Loss: 0.4996\n",
      "Epoch 7/10 - Batch 160300 - Loss: 0.7906\n",
      "Epoch 7/10 - Batch 160400 - Loss: 0.5413\n",
      "Epoch 7/10 - Batch 160500 - Loss: 0.4872\n",
      "Epoch 7/10 - Batch 160600 - Loss: 0.7110\n",
      "Epoch 7/10 - Batch 160700 - Loss: 0.5538\n",
      "Epoch 7/10 - Batch 160800 - Loss: 0.6183\n",
      "Epoch 7/10 - Batch 160900 - Loss: 0.5637\n",
      "Epoch 7/10 - Batch 161000 - Loss: 0.7082\n",
      "Epoch 7/10 - Batch 161100 - Loss: 0.7111\n",
      "Epoch 7/10 - Batch 161200 - Loss: 0.5403\n",
      "Epoch 7/10 - Batch 161300 - Loss: 0.7763\n",
      "Epoch 7/10 - Batch 161400 - Loss: 0.7279\n",
      "Epoch 7/10 - Batch 161500 - Loss: 0.5255\n",
      "Epoch 7/10 - Batch 161600 - Loss: 0.6255\n",
      "Epoch 7/10 - Batch 161700 - Loss: 0.5297\n",
      "Epoch 7/10 - Batch 161800 - Loss: 0.6302\n",
      "Epoch 7/10 - Batch 161900 - Loss: 0.5712\n",
      "Epoch 7/10 - Batch 162000 - Loss: 0.5835\n",
      "Epoch 7/10 - Batch 162100 - Loss: 0.5731\n",
      "Epoch 7/10 - Batch 162200 - Loss: 0.5649\n",
      "Epoch 7/10 - Batch 162300 - Loss: 0.5686\n",
      "Epoch 7/10 - Batch 162400 - Loss: 0.6808\n",
      "Epoch 7/10 - Batch 162500 - Loss: 0.5788\n",
      "Epoch 7/10 - Batch 162600 - Loss: 0.4885\n",
      "Epoch 7/10 - Batch 162700 - Loss: 0.6823\n",
      "Epoch 7/10 - Batch 162800 - Loss: 0.5864\n",
      "Epoch 7/10 - Batch 162900 - Loss: 0.6001\n",
      "Epoch 7/10 - Batch 163000 - Loss: 0.7364\n",
      "Epoch 7/10 - Batch 163100 - Loss: 0.6160\n",
      "Epoch 7/10 - Batch 163200 - Loss: 0.5019\n",
      "Epoch 7/10 - Batch 163300 - Loss: 0.5637\n",
      "Epoch 7/10 - Batch 163400 - Loss: 0.5534\n",
      "Epoch 7/10 - Batch 163500 - Loss: 0.9021\n",
      "Epoch 7/10 - Batch 163600 - Loss: 0.6651\n",
      "Epoch 7/10 - Batch 163700 - Loss: 0.4486\n",
      "Epoch 7/10 - Batch 163800 - Loss: 0.5905\n",
      "Epoch 7/10 - Batch 163900 - Loss: 0.5510\n",
      "Epoch 7/10 - Batch 164000 - Loss: 0.6990\n",
      "Epoch 7/10 - Batch 164100 - Loss: 0.7514\n",
      "Epoch 7/10 - Batch 164200 - Loss: 0.7346\n",
      "Epoch 7/10 - Batch 164300 - Loss: 0.7731\n",
      "Epoch 7/10 - Batch 164400 - Loss: 0.6595\n",
      "Epoch 7/10 - Batch 164500 - Loss: 0.6919\n",
      "Epoch 7/10 - Batch 164600 - Loss: 0.7464\n",
      "Epoch 7/10 - Batch 164700 - Loss: 0.7141\n",
      "Epoch 7/10 - Batch 164800 - Loss: 0.6794\n",
      "Epoch 7/10 - Batch 164900 - Loss: 0.9528\n",
      "Epoch 7/10 - Batch 165000 - Loss: 0.5757\n",
      "Epoch 7/10 - Batch 165100 - Loss: 0.5715\n",
      "Epoch 7/10 - Batch 165200 - Loss: 0.6197\n",
      "Epoch 7/10 - Batch 165300 - Loss: 0.6428\n",
      "Epoch 7/10 - Batch 165400 - Loss: 0.8076\n",
      "Epoch 7/10 - Batch 165500 - Loss: 0.5968\n",
      "Epoch 7/10 - Batch 165600 - Loss: 0.5815\n",
      "Epoch 7/10 - Batch 165700 - Loss: 0.6051\n",
      "Epoch 7/10 - Batch 165800 - Loss: 0.5445\n",
      "Epoch 7/10 - Batch 165900 - Loss: 0.6053\n",
      "Epoch 7/10 - Batch 166000 - Loss: 0.5146\n",
      "Epoch 7/10 - Batch 166100 - Loss: 0.5271\n",
      "Epoch 7/10 - Batch 166200 - Loss: 0.7129\n",
      "Epoch 7/10 - Batch 166300 - Loss: 0.5013\n",
      "Epoch 7/10 - Batch 166400 - Loss: 0.5169\n",
      "Epoch 7/10 - Batch 166500 - Loss: 0.4705\n",
      "Epoch 7/10 - Batch 166600 - Loss: 0.4738\n",
      "Epoch 7/10 - Batch 166700 - Loss: 0.5744\n",
      "Epoch 7/10 - Batch 166800 - Loss: 0.3121\n",
      "Epoch 7/10 - Batch 166900 - Loss: 0.4169\n",
      "Epoch 7/10 - Batch 167000 - Loss: 0.3702\n",
      "Epoch 7/10 - Batch 167100 - Loss: 0.4225\n",
      "Epoch 7/10 - Batch 167200 - Loss: 0.4710\n",
      "Epoch 7/10 - Batch 167300 - Loss: 0.4053\n",
      "Epoch 7/10 - Batch 167400 - Loss: 0.3281\n",
      "Epoch 7/10 - Batch 167500 - Loss: 0.3908\n",
      "Epoch 7/10 - Batch 167600 - Loss: 0.5325\n",
      "Epoch 7/10 - Batch 167700 - Loss: 0.4554\n",
      "Epoch 7/10 - Batch 167800 - Loss: 0.5875\n",
      "Epoch 7/10 - Batch 167900 - Loss: 0.3151\n",
      "Epoch 7/10 - Batch 168000 - Loss: 0.4072\n",
      "Epoch 7/10 - Batch 168100 - Loss: 0.7131\n",
      "Epoch 7/10 - Batch 168200 - Loss: 0.5321\n",
      "Epoch 7/10 - Batch 168300 - Loss: 0.0858\n",
      "Epoch 7/10 - Batch 168400 - Loss: 0.5752\n",
      "Epoch 7/10 - Batch 168500 - Loss: 0.4328\n",
      "Epoch 7/10 - Batch 168600 - Loss: 0.3785\n",
      "Epoch 7/10 - Batch 168700 - Loss: 0.7068\n",
      "Epoch 7/10 - Batch 168800 - Loss: 0.6445\n",
      "Epoch 7/10 - Batch 168900 - Loss: 0.6595\n",
      "Epoch 7/10 - Batch 169000 - Loss: 0.8323\n",
      "Epoch 7/10 - Batch 169100 - Loss: 0.7037\n",
      "Epoch 7/10 - Batch 169200 - Loss: 0.7171\n",
      "Epoch 7/10 - Batch 169300 - Loss: 0.7533\n",
      "Epoch 7/10 - Batch 169400 - Loss: 0.6324\n",
      "Epoch 7/10 - Batch 169500 - Loss: 0.7542\n",
      "Epoch 7/10 - Batch 169600 - Loss: 0.6499\n",
      "Epoch 7/10 - Batch 169700 - Loss: 0.7800\n",
      "Epoch 7/10 - Batch 169800 - Loss: 0.5304\n",
      "Epoch 7/10 - Batch 169900 - Loss: 0.4880\n",
      "Epoch 7/10 - Batch 170000 - Loss: 0.7446\n",
      "Epoch 7/10 - Batch 170100 - Loss: 0.6647\n",
      "Epoch 7/10 - Batch 170200 - Loss: 0.5016\n",
      "Epoch 7/10 - Batch 170300 - Loss: 0.6551\n",
      "Epoch 7/10 - Batch 170400 - Loss: 0.5795\n",
      "Epoch 7/10 - Batch 170500 - Loss: 0.4896\n",
      "Epoch 7/10 - Batch 170600 - Loss: 0.7770\n",
      "Epoch 7/10 - Batch 170700 - Loss: 0.4007\n",
      "Epoch 7/10 - Batch 170800 - Loss: 0.6878\n",
      "Epoch 7/10 - Batch 170900 - Loss: 0.5612\n",
      "Epoch 7/10 - Batch 171000 - Loss: 0.8007\n",
      "Epoch 7/10 - Batch 171100 - Loss: 0.6057\n",
      "Epoch 7/10 - Batch 171200 - Loss: 0.6452\n",
      "Epoch 7/10 - Batch 171300 - Loss: 0.9146\n",
      "Epoch 7/10 - Batch 171400 - Loss: 0.6441\n",
      "Epoch 7/10 - Batch 171500 - Loss: 0.7383\n",
      "Epoch 7/10 - Batch 171600 - Loss: 0.8117\n",
      "Epoch 7/10 - Batch 171700 - Loss: 0.6264\n",
      "Epoch 7/10 - Batch 171800 - Loss: 0.7539\n",
      "Epoch 7/10 - Batch 171900 - Loss: 0.5570\n",
      "Epoch 7/10 - Batch 172000 - Loss: 0.5124\n",
      "Epoch 7/10 - Batch 172100 - Loss: 0.6110\n",
      "Epoch 7/10 - Batch 172200 - Loss: 0.5447\n",
      "Epoch 7/10 - Batch 172300 - Loss: 0.6092\n",
      "Epoch 7/10 - Batch 172400 - Loss: 0.8512\n",
      "Epoch 8/10 - Batch 172500 - Loss: 0.3654\n",
      "Epoch 8/10 - Batch 172600 - Loss: 0.2243\n",
      "Epoch 8/10 - Batch 172700 - Loss: 0.2192\n",
      "Epoch 8/10 - Batch 172800 - Loss: 0.1744\n",
      "Epoch 8/10 - Batch 172900 - Loss: 0.1581\n",
      "Epoch 8/10 - Batch 173000 - Loss: 0.1872\n",
      "Epoch 8/10 - Batch 173100 - Loss: 0.1716\n",
      "Epoch 8/10 - Batch 173200 - Loss: 0.1790\n",
      "Epoch 8/10 - Batch 173300 - Loss: 0.1752\n",
      "Epoch 8/10 - Batch 173400 - Loss: 0.1939\n",
      "Epoch 8/10 - Batch 173500 - Loss: 0.1634\n",
      "Epoch 8/10 - Batch 173600 - Loss: 0.1818\n",
      "Epoch 8/10 - Batch 173700 - Loss: 0.1790\n",
      "Epoch 8/10 - Batch 173800 - Loss: 0.1490\n",
      "Epoch 8/10 - Batch 173900 - Loss: 0.1654\n",
      "Epoch 8/10 - Batch 174000 - Loss: 0.1596\n",
      "Epoch 8/10 - Batch 174100 - Loss: 0.1592\n",
      "Epoch 8/10 - Batch 174200 - Loss: 0.1709\n",
      "Epoch 8/10 - Batch 174300 - Loss: 0.1668\n",
      "Epoch 8/10 - Batch 174400 - Loss: 0.2016\n",
      "Epoch 8/10 - Batch 174500 - Loss: 0.2014\n",
      "Epoch 8/10 - Batch 174600 - Loss: 0.1613\n",
      "Epoch 8/10 - Batch 174700 - Loss: 0.1900\n",
      "Epoch 8/10 - Batch 174800 - Loss: 0.1979\n",
      "Epoch 8/10 - Batch 174900 - Loss: 0.1730\n",
      "Epoch 8/10 - Batch 175000 - Loss: 0.1755\n",
      "Epoch 8/10 - Batch 175100 - Loss: 0.1769\n",
      "Epoch 8/10 - Batch 175200 - Loss: 0.2051\n",
      "Epoch 8/10 - Batch 175300 - Loss: 0.2011\n",
      "Epoch 8/10 - Batch 175400 - Loss: 0.1508\n",
      "Epoch 8/10 - Batch 175500 - Loss: 0.2088\n",
      "Epoch 8/10 - Batch 175600 - Loss: 0.2091\n",
      "Epoch 8/10 - Batch 175700 - Loss: 0.2147\n",
      "Epoch 8/10 - Batch 175800 - Loss: 0.1814\n",
      "Epoch 8/10 - Batch 175900 - Loss: 0.1975\n",
      "Epoch 8/10 - Batch 176000 - Loss: 0.2178\n",
      "Epoch 8/10 - Batch 176100 - Loss: 0.1845\n",
      "Epoch 8/10 - Batch 176200 - Loss: 0.1877\n",
      "Epoch 8/10 - Batch 176300 - Loss: 0.1620\n",
      "Epoch 8/10 - Batch 176400 - Loss: 0.1993\n",
      "Epoch 8/10 - Batch 176500 - Loss: 0.2005\n",
      "Epoch 8/10 - Batch 176600 - Loss: 0.1821\n",
      "Epoch 8/10 - Batch 176700 - Loss: 0.2156\n",
      "Epoch 8/10 - Batch 176800 - Loss: 0.2121\n",
      "Epoch 8/10 - Batch 176900 - Loss: 0.1893\n",
      "Epoch 8/10 - Batch 177000 - Loss: 0.1825\n",
      "Epoch 8/10 - Batch 177100 - Loss: 0.1652\n",
      "Epoch 8/10 - Batch 177200 - Loss: 0.1848\n",
      "Epoch 8/10 - Batch 177300 - Loss: 0.1682\n",
      "Epoch 8/10 - Batch 177400 - Loss: 0.1960\n",
      "Epoch 8/10 - Batch 177500 - Loss: 0.2066\n",
      "Epoch 8/10 - Batch 177600 - Loss: 0.1918\n",
      "Epoch 8/10 - Batch 177700 - Loss: 0.1894\n",
      "Epoch 8/10 - Batch 177800 - Loss: 0.1706\n",
      "Epoch 8/10 - Batch 177900 - Loss: 0.1536\n",
      "Epoch 8/10 - Batch 178000 - Loss: 0.1701\n",
      "Epoch 8/10 - Batch 178100 - Loss: 0.1595\n",
      "Epoch 8/10 - Batch 178200 - Loss: 0.1784\n",
      "Epoch 8/10 - Batch 178300 - Loss: 0.1922\n",
      "Epoch 8/10 - Batch 178400 - Loss: 0.1874\n",
      "Epoch 8/10 - Batch 178500 - Loss: 0.2176\n",
      "Epoch 8/10 - Batch 178600 - Loss: 0.1774\n",
      "Epoch 8/10 - Batch 178700 - Loss: 0.2058\n",
      "Epoch 8/10 - Batch 178800 - Loss: 0.1921\n",
      "Epoch 8/10 - Batch 178900 - Loss: 0.1517\n",
      "Epoch 8/10 - Batch 179000 - Loss: 0.2247\n",
      "Epoch 8/10 - Batch 179100 - Loss: 0.2371\n",
      "Epoch 8/10 - Batch 179200 - Loss: 0.1898\n",
      "Epoch 8/10 - Batch 179300 - Loss: 0.1861\n",
      "Epoch 8/10 - Batch 179400 - Loss: 0.2036\n",
      "Epoch 8/10 - Batch 179500 - Loss: 0.1560\n",
      "Epoch 8/10 - Batch 179600 - Loss: 0.1976\n",
      "Epoch 8/10 - Batch 179700 - Loss: 0.1837\n",
      "Epoch 8/10 - Batch 179800 - Loss: 0.1690\n",
      "Epoch 8/10 - Batch 179900 - Loss: 0.2090\n",
      "Epoch 8/10 - Batch 180000 - Loss: 0.1932\n",
      "Epoch 8/10 - Batch 180100 - Loss: 0.1772\n",
      "Epoch 8/10 - Batch 180200 - Loss: 0.1892\n",
      "Epoch 8/10 - Batch 180300 - Loss: 0.2197\n",
      "Epoch 8/10 - Batch 180400 - Loss: 0.2094\n",
      "Epoch 8/10 - Batch 180500 - Loss: 0.1872\n",
      "Epoch 8/10 - Batch 180600 - Loss: 0.1769\n",
      "Epoch 8/10 - Batch 180700 - Loss: 0.2228\n",
      "Epoch 8/10 - Batch 180800 - Loss: 0.2012\n",
      "Epoch 8/10 - Batch 180900 - Loss: 0.1751\n",
      "Epoch 8/10 - Batch 181000 - Loss: 0.1817\n",
      "Epoch 8/10 - Batch 181100 - Loss: 0.1999\n",
      "Epoch 8/10 - Batch 181200 - Loss: 0.2039\n",
      "Epoch 8/10 - Batch 181300 - Loss: 0.1915\n",
      "Epoch 8/10 - Batch 181400 - Loss: 0.2301\n",
      "Epoch 8/10 - Batch 181500 - Loss: 0.2137\n",
      "Epoch 8/10 - Batch 181600 - Loss: 0.2019\n",
      "Epoch 8/10 - Batch 181700 - Loss: 0.1815\n",
      "Epoch 8/10 - Batch 181800 - Loss: 0.1921\n",
      "Epoch 8/10 - Batch 181900 - Loss: 0.2317\n",
      "Epoch 8/10 - Batch 182000 - Loss: 0.2416\n",
      "Epoch 8/10 - Batch 182100 - Loss: 0.2222\n",
      "Epoch 8/10 - Batch 182200 - Loss: 0.2231\n",
      "Epoch 8/10 - Batch 182300 - Loss: 0.2184\n",
      "Epoch 8/10 - Batch 182400 - Loss: 0.2093\n",
      "Epoch 8/10 - Batch 182500 - Loss: 0.2381\n",
      "Epoch 8/10 - Batch 182600 - Loss: 0.2691\n",
      "Epoch 8/10 - Batch 182700 - Loss: 0.2089\n",
      "Epoch 8/10 - Batch 182800 - Loss: 0.2477\n",
      "Epoch 8/10 - Batch 182900 - Loss: 0.2560\n",
      "Epoch 8/10 - Batch 183000 - Loss: 0.2638\n",
      "Epoch 8/10 - Batch 183100 - Loss: 0.2540\n",
      "Epoch 8/10 - Batch 183200 - Loss: 0.2887\n",
      "Epoch 8/10 - Batch 183300 - Loss: 0.2666\n",
      "Epoch 8/10 - Batch 183400 - Loss: 0.3160\n",
      "Epoch 8/10 - Batch 183500 - Loss: 0.4774\n",
      "Epoch 8/10 - Batch 183600 - Loss: 0.5430\n",
      "Epoch 8/10 - Batch 183700 - Loss: 0.5814\n",
      "Epoch 8/10 - Batch 183800 - Loss: 0.6819\n",
      "Epoch 8/10 - Batch 183900 - Loss: 0.5569\n",
      "Epoch 8/10 - Batch 184000 - Loss: 0.6986\n",
      "Epoch 8/10 - Batch 184100 - Loss: 0.5959\n",
      "Epoch 8/10 - Batch 184200 - Loss: 0.5898\n",
      "Epoch 8/10 - Batch 184300 - Loss: 0.5625\n",
      "Epoch 8/10 - Batch 184400 - Loss: 0.5786\n",
      "Epoch 8/10 - Batch 184500 - Loss: 0.6400\n",
      "Epoch 8/10 - Batch 184600 - Loss: 0.6045\n",
      "Epoch 8/10 - Batch 184700 - Loss: 0.4759\n",
      "Epoch 8/10 - Batch 184800 - Loss: 0.4514\n",
      "Epoch 8/10 - Batch 184900 - Loss: 0.6331\n",
      "Epoch 8/10 - Batch 185000 - Loss: 0.5776\n",
      "Epoch 8/10 - Batch 185100 - Loss: 0.4759\n",
      "Epoch 8/10 - Batch 185200 - Loss: 0.6802\n",
      "Epoch 8/10 - Batch 185300 - Loss: 0.5659\n",
      "Epoch 8/10 - Batch 185400 - Loss: 0.5327\n",
      "Epoch 8/10 - Batch 185500 - Loss: 0.5126\n",
      "Epoch 8/10 - Batch 185600 - Loss: 0.5706\n",
      "Epoch 8/10 - Batch 185700 - Loss: 0.8172\n",
      "Epoch 8/10 - Batch 185800 - Loss: 0.5326\n",
      "Epoch 8/10 - Batch 185900 - Loss: 0.5434\n",
      "Epoch 8/10 - Batch 186000 - Loss: 0.8591\n",
      "Epoch 8/10 - Batch 186100 - Loss: 0.5056\n",
      "Epoch 8/10 - Batch 186200 - Loss: 0.5353\n",
      "Epoch 8/10 - Batch 186300 - Loss: 0.5302\n",
      "Epoch 8/10 - Batch 186400 - Loss: 0.5671\n",
      "Epoch 8/10 - Batch 186500 - Loss: 0.5366\n",
      "Epoch 8/10 - Batch 186600 - Loss: 0.5529\n",
      "Epoch 8/10 - Batch 186700 - Loss: 0.5188\n",
      "Epoch 8/10 - Batch 186800 - Loss: 0.5713\n",
      "Epoch 8/10 - Batch 186900 - Loss: 0.4587\n",
      "Epoch 8/10 - Batch 187000 - Loss: 0.5747\n",
      "Epoch 8/10 - Batch 187100 - Loss: 0.7452\n",
      "Epoch 8/10 - Batch 187200 - Loss: 0.3665\n",
      "Epoch 8/10 - Batch 187300 - Loss: 0.6508\n",
      "Epoch 8/10 - Batch 187400 - Loss: 0.5824\n",
      "Epoch 8/10 - Batch 187500 - Loss: 0.5558\n",
      "Epoch 8/10 - Batch 187600 - Loss: 0.6563\n",
      "Epoch 8/10 - Batch 187700 - Loss: 0.5534\n",
      "Epoch 8/10 - Batch 187800 - Loss: 0.5069\n",
      "Epoch 8/10 - Batch 187900 - Loss: 0.5111\n",
      "Epoch 8/10 - Batch 188000 - Loss: 0.5026\n",
      "Epoch 8/10 - Batch 188100 - Loss: 0.6444\n",
      "Epoch 8/10 - Batch 188200 - Loss: 0.8298\n",
      "Epoch 8/10 - Batch 188300 - Loss: 0.4976\n",
      "Epoch 8/10 - Batch 188400 - Loss: 0.4863\n",
      "Epoch 8/10 - Batch 188500 - Loss: 0.5387\n",
      "Epoch 8/10 - Batch 188600 - Loss: 0.5551\n",
      "Epoch 8/10 - Batch 188700 - Loss: 0.7485\n",
      "Epoch 8/10 - Batch 188800 - Loss: 0.6706\n",
      "Epoch 8/10 - Batch 188900 - Loss: 0.8184\n",
      "Epoch 8/10 - Batch 189000 - Loss: 0.6049\n",
      "Epoch 8/10 - Batch 189100 - Loss: 0.6845\n",
      "Epoch 8/10 - Batch 189200 - Loss: 0.6396\n",
      "Epoch 8/10 - Batch 189300 - Loss: 0.7662\n",
      "Epoch 8/10 - Batch 189400 - Loss: 0.5879\n",
      "Epoch 8/10 - Batch 189500 - Loss: 0.6951\n",
      "Epoch 8/10 - Batch 189600 - Loss: 0.7966\n",
      "Epoch 8/10 - Batch 189700 - Loss: 0.4448\n",
      "Epoch 8/10 - Batch 189800 - Loss: 0.6221\n",
      "Epoch 8/10 - Batch 189900 - Loss: 0.5752\n",
      "Epoch 8/10 - Batch 190000 - Loss: 0.7489\n",
      "Epoch 8/10 - Batch 190100 - Loss: 0.6126\n",
      "Epoch 8/10 - Batch 190200 - Loss: 0.5479\n",
      "Epoch 8/10 - Batch 190300 - Loss: 0.5066\n",
      "Epoch 8/10 - Batch 190400 - Loss: 0.6409\n",
      "Epoch 8/10 - Batch 190500 - Loss: 0.5413\n",
      "Epoch 8/10 - Batch 190600 - Loss: 0.4433\n",
      "Epoch 8/10 - Batch 190700 - Loss: 0.5040\n",
      "Epoch 8/10 - Batch 190800 - Loss: 0.6403\n",
      "Epoch 8/10 - Batch 190900 - Loss: 0.5282\n",
      "Epoch 8/10 - Batch 191000 - Loss: 0.4518\n",
      "Epoch 8/10 - Batch 191100 - Loss: 0.4919\n",
      "Epoch 8/10 - Batch 191200 - Loss: 0.4516\n",
      "Epoch 8/10 - Batch 191300 - Loss: 0.4584\n",
      "Epoch 8/10 - Batch 191400 - Loss: 0.3953\n",
      "Epoch 8/10 - Batch 191500 - Loss: 0.3793\n",
      "Epoch 8/10 - Batch 191600 - Loss: 0.3372\n",
      "Epoch 8/10 - Batch 191700 - Loss: 0.3163\n",
      "Epoch 8/10 - Batch 191800 - Loss: 0.5476\n",
      "Epoch 8/10 - Batch 191900 - Loss: 0.3430\n",
      "Epoch 8/10 - Batch 192000 - Loss: 0.3180\n",
      "Epoch 8/10 - Batch 192100 - Loss: 0.3579\n",
      "Epoch 8/10 - Batch 192200 - Loss: 0.4941\n",
      "Epoch 8/10 - Batch 192300 - Loss: 0.3748\n",
      "Epoch 8/10 - Batch 192400 - Loss: 0.5399\n",
      "Epoch 8/10 - Batch 192500 - Loss: 0.3835\n",
      "Epoch 8/10 - Batch 192600 - Loss: 0.3587\n",
      "Epoch 8/10 - Batch 192700 - Loss: 0.4685\n",
      "Epoch 8/10 - Batch 192800 - Loss: 0.7914\n",
      "Epoch 8/10 - Batch 192900 - Loss: 0.1336\n",
      "Epoch 8/10 - Batch 193000 - Loss: 0.3352\n",
      "Epoch 8/10 - Batch 193100 - Loss: 0.4799\n",
      "Epoch 8/10 - Batch 193200 - Loss: 0.3649\n",
      "Epoch 8/10 - Batch 193300 - Loss: 0.5033\n",
      "Epoch 8/10 - Batch 193400 - Loss: 0.7080\n",
      "Epoch 8/10 - Batch 193500 - Loss: 0.5708\n",
      "Epoch 8/10 - Batch 193600 - Loss: 0.7764\n",
      "Epoch 8/10 - Batch 193700 - Loss: 0.6652\n",
      "Epoch 8/10 - Batch 193800 - Loss: 0.5966\n",
      "Epoch 8/10 - Batch 193900 - Loss: 0.8066\n",
      "Epoch 8/10 - Batch 194000 - Loss: 0.6156\n",
      "Epoch 8/10 - Batch 194100 - Loss: 0.6851\n",
      "Epoch 8/10 - Batch 194200 - Loss: 0.6427\n",
      "Epoch 8/10 - Batch 194300 - Loss: 0.6573\n",
      "Epoch 8/10 - Batch 194400 - Loss: 0.5882\n",
      "Epoch 8/10 - Batch 194500 - Loss: 0.5315\n",
      "Epoch 8/10 - Batch 194600 - Loss: 0.5396\n",
      "Epoch 8/10 - Batch 194700 - Loss: 0.6899\n",
      "Epoch 8/10 - Batch 194800 - Loss: 0.5356\n",
      "Epoch 8/10 - Batch 194900 - Loss: 0.5545\n",
      "Epoch 8/10 - Batch 195000 - Loss: 0.6107\n",
      "Epoch 8/10 - Batch 195100 - Loss: 0.4136\n",
      "Epoch 8/10 - Batch 195200 - Loss: 0.6764\n",
      "Epoch 8/10 - Batch 195300 - Loss: 0.5121\n",
      "Epoch 8/10 - Batch 195400 - Loss: 0.4346\n",
      "Epoch 8/10 - Batch 195500 - Loss: 0.6557\n",
      "Epoch 8/10 - Batch 195600 - Loss: 0.7302\n",
      "Epoch 8/10 - Batch 195700 - Loss: 0.5471\n",
      "Epoch 8/10 - Batch 195800 - Loss: 0.5936\n",
      "Epoch 8/10 - Batch 195900 - Loss: 0.6519\n",
      "Epoch 8/10 - Batch 196000 - Loss: 1.0025\n",
      "Epoch 8/10 - Batch 196100 - Loss: 0.5919\n",
      "Epoch 8/10 - Batch 196200 - Loss: 0.7042\n",
      "Epoch 8/10 - Batch 196300 - Loss: 0.6193\n",
      "Epoch 8/10 - Batch 196400 - Loss: 0.6495\n",
      "Epoch 8/10 - Batch 196500 - Loss: 0.6233\n",
      "Epoch 8/10 - Batch 196600 - Loss: 0.5321\n",
      "Epoch 8/10 - Batch 196700 - Loss: 0.5460\n",
      "Epoch 8/10 - Batch 196800 - Loss: 0.5562\n",
      "Epoch 8/10 - Batch 196900 - Loss: 0.4365\n",
      "Epoch 8/10 - Batch 197000 - Loss: 0.8252\n",
      "Epoch 8/10 - Batch 197100 - Loss: 0.5179\n",
      "Epoch 9/10 - Batch 197200 - Loss: 0.2382\n",
      "Epoch 9/10 - Batch 197300 - Loss: 0.2202\n",
      "Epoch 9/10 - Batch 197400 - Loss: 0.1616\n",
      "Epoch 9/10 - Batch 197500 - Loss: 0.1504\n",
      "Epoch 9/10 - Batch 197600 - Loss: 0.1798\n",
      "Epoch 9/10 - Batch 197700 - Loss: 0.1726\n",
      "Epoch 9/10 - Batch 197800 - Loss: 0.1697\n",
      "Epoch 9/10 - Batch 197900 - Loss: 0.1660\n",
      "Epoch 9/10 - Batch 198000 - Loss: 0.1724\n",
      "Epoch 9/10 - Batch 198100 - Loss: 0.1486\n",
      "Epoch 9/10 - Batch 198200 - Loss: 0.1664\n",
      "Epoch 9/10 - Batch 198300 - Loss: 0.1514\n",
      "Epoch 9/10 - Batch 198400 - Loss: 0.1678\n",
      "Epoch 9/10 - Batch 198500 - Loss: 0.1295\n",
      "Epoch 9/10 - Batch 198600 - Loss: 0.1676\n",
      "Epoch 9/10 - Batch 198700 - Loss: 0.1350\n",
      "Epoch 9/10 - Batch 198800 - Loss: 0.1454\n",
      "Epoch 9/10 - Batch 198900 - Loss: 0.1439\n",
      "Epoch 9/10 - Batch 199000 - Loss: 0.1667\n",
      "Epoch 9/10 - Batch 199100 - Loss: 0.1715\n",
      "Epoch 9/10 - Batch 199200 - Loss: 0.1318\n",
      "Epoch 9/10 - Batch 199300 - Loss: 0.1789\n",
      "Epoch 9/10 - Batch 199400 - Loss: 0.1852\n",
      "Epoch 9/10 - Batch 199500 - Loss: 0.1599\n",
      "Epoch 9/10 - Batch 199600 - Loss: 0.1261\n",
      "Epoch 9/10 - Batch 199700 - Loss: 0.1728\n",
      "Epoch 9/10 - Batch 199800 - Loss: 0.1706\n",
      "Epoch 9/10 - Batch 199900 - Loss: 0.1835\n",
      "Epoch 9/10 - Batch 200000 - Loss: 0.1453\n",
      "Epoch 9/10 - Batch 200100 - Loss: 0.1755\n",
      "Epoch 9/10 - Batch 200200 - Loss: 0.1723\n",
      "Epoch 9/10 - Batch 200300 - Loss: 0.1687\n",
      "Epoch 9/10 - Batch 200400 - Loss: 0.1786\n",
      "Epoch 9/10 - Batch 200500 - Loss: 0.1560\n",
      "Epoch 9/10 - Batch 200600 - Loss: 0.1895\n",
      "Epoch 9/10 - Batch 200700 - Loss: 0.1760\n",
      "Epoch 9/10 - Batch 200800 - Loss: 0.1698\n",
      "Epoch 9/10 - Batch 200900 - Loss: 0.1591\n",
      "Epoch 9/10 - Batch 201000 - Loss: 0.1611\n",
      "Epoch 9/10 - Batch 201100 - Loss: 0.1499\n",
      "Epoch 9/10 - Batch 201200 - Loss: 0.1623\n",
      "Epoch 9/10 - Batch 201300 - Loss: 0.1789\n",
      "Epoch 9/10 - Batch 201400 - Loss: 0.2182\n",
      "Epoch 9/10 - Batch 201500 - Loss: 0.1705\n",
      "Epoch 9/10 - Batch 201600 - Loss: 0.1654\n",
      "Epoch 9/10 - Batch 201700 - Loss: 0.1433\n",
      "Epoch 9/10 - Batch 201800 - Loss: 0.1482\n",
      "Epoch 9/10 - Batch 201900 - Loss: 0.1467\n",
      "Epoch 9/10 - Batch 202000 - Loss: 0.1697\n",
      "Epoch 9/10 - Batch 202100 - Loss: 0.1619\n",
      "Epoch 9/10 - Batch 202200 - Loss: 0.1662\n",
      "Epoch 9/10 - Batch 202300 - Loss: 0.1816\n",
      "Epoch 9/10 - Batch 202400 - Loss: 0.1669\n",
      "Epoch 9/10 - Batch 202500 - Loss: 0.1462\n",
      "Epoch 9/10 - Batch 202600 - Loss: 0.1438\n",
      "Epoch 9/10 - Batch 202700 - Loss: 0.1540\n",
      "Epoch 9/10 - Batch 202800 - Loss: 0.1583\n",
      "Epoch 9/10 - Batch 202900 - Loss: 0.1726\n",
      "Epoch 9/10 - Batch 203000 - Loss: 0.1596\n",
      "Epoch 9/10 - Batch 203100 - Loss: 0.1790\n",
      "Epoch 9/10 - Batch 203200 - Loss: 0.1561\n",
      "Epoch 9/10 - Batch 203300 - Loss: 0.1557\n",
      "Epoch 9/10 - Batch 203400 - Loss: 0.1873\n",
      "Epoch 9/10 - Batch 203500 - Loss: 0.1492\n",
      "Epoch 9/10 - Batch 203600 - Loss: 0.1757\n",
      "Epoch 9/10 - Batch 203700 - Loss: 0.1987\n",
      "Epoch 9/10 - Batch 203800 - Loss: 0.2007\n",
      "Epoch 9/10 - Batch 203900 - Loss: 0.1763\n",
      "Epoch 9/10 - Batch 204000 - Loss: 0.1735\n",
      "Epoch 9/10 - Batch 204100 - Loss: 0.1484\n",
      "Epoch 9/10 - Batch 204200 - Loss: 0.1805\n",
      "Epoch 9/10 - Batch 204300 - Loss: 0.1531\n",
      "Epoch 9/10 - Batch 204400 - Loss: 0.1509\n",
      "Epoch 9/10 - Batch 204500 - Loss: 0.1825\n",
      "Epoch 9/10 - Batch 204600 - Loss: 0.1706\n",
      "Epoch 9/10 - Batch 204700 - Loss: 0.1858\n",
      "Epoch 9/10 - Batch 204800 - Loss: 0.1663\n",
      "Epoch 9/10 - Batch 204900 - Loss: 0.1950\n",
      "Epoch 9/10 - Batch 205000 - Loss: 0.1834\n",
      "Epoch 9/10 - Batch 205100 - Loss: 0.1850\n",
      "Epoch 9/10 - Batch 205200 - Loss: 0.1526\n",
      "Epoch 9/10 - Batch 205300 - Loss: 0.1810\n",
      "Epoch 9/10 - Batch 205400 - Loss: 0.1791\n",
      "Epoch 9/10 - Batch 205500 - Loss: 0.1763\n",
      "Epoch 9/10 - Batch 205600 - Loss: 0.1459\n",
      "Epoch 9/10 - Batch 205700 - Loss: 0.1660\n",
      "Epoch 9/10 - Batch 205800 - Loss: 0.1711\n",
      "Epoch 9/10 - Batch 205900 - Loss: 0.1742\n",
      "Epoch 9/10 - Batch 206000 - Loss: 0.1650\n",
      "Epoch 9/10 - Batch 206100 - Loss: 0.1931\n",
      "Epoch 9/10 - Batch 206200 - Loss: 0.1810\n",
      "Epoch 9/10 - Batch 206300 - Loss: 0.1800\n",
      "Epoch 9/10 - Batch 206400 - Loss: 0.1797\n",
      "Epoch 9/10 - Batch 206500 - Loss: 0.2006\n",
      "Epoch 9/10 - Batch 206600 - Loss: 0.2100\n",
      "Epoch 9/10 - Batch 206700 - Loss: 0.2013\n",
      "Epoch 9/10 - Batch 206800 - Loss: 0.1928\n",
      "Epoch 9/10 - Batch 206900 - Loss: 0.2233\n",
      "Epoch 9/10 - Batch 207000 - Loss: 0.1896\n",
      "Epoch 9/10 - Batch 207100 - Loss: 0.2139\n",
      "Epoch 9/10 - Batch 207200 - Loss: 0.2321\n",
      "Epoch 9/10 - Batch 207300 - Loss: 0.1901\n",
      "Epoch 9/10 - Batch 207400 - Loss: 0.2460\n",
      "Epoch 9/10 - Batch 207500 - Loss: 0.2048\n",
      "Epoch 9/10 - Batch 207600 - Loss: 0.2398\n",
      "Epoch 9/10 - Batch 207700 - Loss: 0.2321\n",
      "Epoch 9/10 - Batch 207800 - Loss: 0.2452\n",
      "Epoch 9/10 - Batch 207900 - Loss: 0.2509\n",
      "Epoch 9/10 - Batch 208000 - Loss: 0.2653\n",
      "Epoch 9/10 - Batch 208100 - Loss: 0.3354\n",
      "Epoch 9/10 - Batch 208200 - Loss: 0.5532\n",
      "Epoch 9/10 - Batch 208300 - Loss: 0.5074\n",
      "Epoch 9/10 - Batch 208400 - Loss: 0.6622\n",
      "Epoch 9/10 - Batch 208500 - Loss: 0.5344\n",
      "Epoch 9/10 - Batch 208600 - Loss: 0.5145\n",
      "Epoch 9/10 - Batch 208700 - Loss: 0.6782\n",
      "Epoch 9/10 - Batch 208800 - Loss: 0.5881\n",
      "Epoch 9/10 - Batch 208900 - Loss: 0.5652\n",
      "Epoch 9/10 - Batch 209000 - Loss: 0.5392\n",
      "Epoch 9/10 - Batch 209100 - Loss: 0.6369\n",
      "Epoch 9/10 - Batch 209200 - Loss: 0.5287\n",
      "Epoch 9/10 - Batch 209300 - Loss: 0.4538\n",
      "Epoch 9/10 - Batch 209400 - Loss: 0.4684\n",
      "Epoch 9/10 - Batch 209500 - Loss: 0.4417\n",
      "Epoch 9/10 - Batch 209600 - Loss: 0.6899\n",
      "Epoch 9/10 - Batch 209700 - Loss: 0.4804\n",
      "Epoch 9/10 - Batch 209800 - Loss: 0.5194\n",
      "Epoch 9/10 - Batch 209900 - Loss: 0.5768\n",
      "Epoch 9/10 - Batch 210000 - Loss: 0.4763\n",
      "Epoch 9/10 - Batch 210100 - Loss: 0.5009\n",
      "Epoch 9/10 - Batch 210200 - Loss: 0.4931\n",
      "Epoch 9/10 - Batch 210300 - Loss: 0.7209\n",
      "Epoch 9/10 - Batch 210400 - Loss: 0.5877\n",
      "Epoch 9/10 - Batch 210500 - Loss: 0.4631\n",
      "Epoch 9/10 - Batch 210600 - Loss: 0.8403\n",
      "Epoch 9/10 - Batch 210700 - Loss: 0.5147\n",
      "Epoch 9/10 - Batch 210800 - Loss: 0.4981\n",
      "Epoch 9/10 - Batch 210900 - Loss: 0.5552\n",
      "Epoch 9/10 - Batch 211000 - Loss: 0.4525\n",
      "Epoch 9/10 - Batch 211100 - Loss: 0.6113\n",
      "Epoch 9/10 - Batch 211200 - Loss: 0.5125\n",
      "Epoch 9/10 - Batch 211300 - Loss: 0.4463\n",
      "Epoch 9/10 - Batch 211400 - Loss: 0.4970\n",
      "Epoch 9/10 - Batch 211500 - Loss: 0.4911\n",
      "Epoch 9/10 - Batch 211600 - Loss: 0.5411\n",
      "Epoch 9/10 - Batch 211700 - Loss: 0.6508\n",
      "Epoch 9/10 - Batch 211800 - Loss: 0.3751\n",
      "Epoch 9/10 - Batch 211900 - Loss: 0.5468\n",
      "Epoch 9/10 - Batch 212000 - Loss: 0.5430\n",
      "Epoch 9/10 - Batch 212100 - Loss: 0.4942\n",
      "Epoch 9/10 - Batch 212200 - Loss: 0.5530\n",
      "Epoch 9/10 - Batch 212300 - Loss: 0.6567\n",
      "Epoch 9/10 - Batch 212400 - Loss: 0.5324\n",
      "Epoch 9/10 - Batch 212500 - Loss: 0.4630\n",
      "Epoch 9/10 - Batch 212600 - Loss: 0.4448\n",
      "Epoch 9/10 - Batch 212700 - Loss: 0.6334\n",
      "Epoch 9/10 - Batch 212800 - Loss: 0.7350\n",
      "Epoch 9/10 - Batch 212900 - Loss: 0.4885\n",
      "Epoch 9/10 - Batch 213000 - Loss: 0.4303\n",
      "Epoch 9/10 - Batch 213100 - Loss: 0.5309\n",
      "Epoch 9/10 - Batch 213200 - Loss: 0.5035\n",
      "Epoch 9/10 - Batch 213300 - Loss: 0.6793\n",
      "Epoch 9/10 - Batch 213400 - Loss: 0.6606\n",
      "Epoch 9/10 - Batch 213500 - Loss: 0.6931\n",
      "Epoch 9/10 - Batch 213600 - Loss: 0.6036\n",
      "Epoch 9/10 - Batch 213700 - Loss: 0.6897\n",
      "Epoch 9/10 - Batch 213800 - Loss: 0.5397\n",
      "Epoch 9/10 - Batch 213900 - Loss: 0.7466\n",
      "Epoch 9/10 - Batch 214000 - Loss: 0.6034\n",
      "Epoch 9/10 - Batch 214100 - Loss: 0.6625\n",
      "Epoch 9/10 - Batch 214200 - Loss: 0.7304\n",
      "Epoch 9/10 - Batch 214300 - Loss: 0.5016\n",
      "Epoch 9/10 - Batch 214400 - Loss: 0.5719\n",
      "Epoch 9/10 - Batch 214500 - Loss: 0.4870\n",
      "Epoch 9/10 - Batch 214600 - Loss: 0.5911\n",
      "Epoch 9/10 - Batch 214700 - Loss: 0.7362\n",
      "Epoch 9/10 - Batch 214800 - Loss: 0.4545\n",
      "Epoch 9/10 - Batch 214900 - Loss: 0.5566\n",
      "Epoch 9/10 - Batch 215000 - Loss: 0.6595\n",
      "Epoch 9/10 - Batch 215100 - Loss: 0.3713\n",
      "Epoch 9/10 - Batch 215200 - Loss: 0.4959\n",
      "Epoch 9/10 - Batch 215300 - Loss: 0.4727\n",
      "Epoch 9/10 - Batch 215400 - Loss: 0.4653\n",
      "Epoch 9/10 - Batch 215500 - Loss: 0.6228\n",
      "Epoch 9/10 - Batch 215600 - Loss: 0.4496\n",
      "Epoch 9/10 - Batch 215700 - Loss: 0.4324\n",
      "Epoch 9/10 - Batch 215800 - Loss: 0.4653\n",
      "Epoch 9/10 - Batch 215900 - Loss: 0.3564\n",
      "Epoch 9/10 - Batch 216000 - Loss: 0.4656\n",
      "Epoch 9/10 - Batch 216100 - Loss: 0.3266\n",
      "Epoch 9/10 - Batch 216200 - Loss: 0.3499\n",
      "Epoch 9/10 - Batch 216300 - Loss: 0.2906\n",
      "Epoch 9/10 - Batch 216400 - Loss: 0.4671\n",
      "Epoch 9/10 - Batch 216500 - Loss: 0.3559\n",
      "Epoch 9/10 - Batch 216600 - Loss: 0.2916\n",
      "Epoch 9/10 - Batch 216700 - Loss: 0.3543\n",
      "Epoch 9/10 - Batch 216800 - Loss: 0.3600\n",
      "Epoch 9/10 - Batch 216900 - Loss: 0.4216\n",
      "Epoch 9/10 - Batch 217000 - Loss: 0.4468\n",
      "Epoch 9/10 - Batch 217100 - Loss: 0.4865\n",
      "Epoch 9/10 - Batch 217200 - Loss: 0.2936\n",
      "Epoch 9/10 - Batch 217300 - Loss: 0.3580\n",
      "Epoch 9/10 - Batch 217400 - Loss: 0.6813\n",
      "Epoch 9/10 - Batch 217500 - Loss: 0.3650\n",
      "Epoch 9/10 - Batch 217600 - Loss: 0.1490\n",
      "Epoch 9/10 - Batch 217700 - Loss: 0.4602\n",
      "Epoch 9/10 - Batch 217800 - Loss: 0.3609\n",
      "Epoch 9/10 - Batch 217900 - Loss: 0.3483\n",
      "Epoch 9/10 - Batch 218000 - Loss: 0.6841\n",
      "Epoch 9/10 - Batch 218100 - Loss: 0.5336\n",
      "Epoch 9/10 - Batch 218200 - Loss: 0.6680\n",
      "Epoch 9/10 - Batch 218300 - Loss: 0.6981\n",
      "Epoch 9/10 - Batch 218400 - Loss: 0.5959\n",
      "Epoch 9/10 - Batch 218500 - Loss: 0.7386\n",
      "Epoch 9/10 - Batch 218600 - Loss: 0.6558\n",
      "Epoch 9/10 - Batch 218700 - Loss: 0.6286\n",
      "Epoch 9/10 - Batch 218800 - Loss: 0.6182\n",
      "Epoch 9/10 - Batch 218900 - Loss: 0.5929\n",
      "Epoch 9/10 - Batch 219000 - Loss: 0.6469\n",
      "Epoch 9/10 - Batch 219100 - Loss: 0.4624\n",
      "Epoch 9/10 - Batch 219200 - Loss: 0.4324\n",
      "Epoch 9/10 - Batch 219300 - Loss: 0.7095\n",
      "Epoch 9/10 - Batch 219400 - Loss: 0.6049\n",
      "Epoch 9/10 - Batch 219500 - Loss: 0.4372\n",
      "Epoch 9/10 - Batch 219600 - Loss: 0.5906\n",
      "Epoch 9/10 - Batch 219700 - Loss: 0.4402\n",
      "Epoch 9/10 - Batch 219800 - Loss: 0.5009\n",
      "Epoch 9/10 - Batch 219900 - Loss: 0.6723\n",
      "Epoch 9/10 - Batch 220000 - Loss: 0.3280\n",
      "Epoch 9/10 - Batch 220100 - Loss: 0.6260\n",
      "Epoch 9/10 - Batch 220200 - Loss: 0.6313\n",
      "Epoch 9/10 - Batch 220300 - Loss: 0.6455\n",
      "Epoch 9/10 - Batch 220400 - Loss: 0.5385\n",
      "Epoch 9/10 - Batch 220500 - Loss: 0.5394\n",
      "Epoch 9/10 - Batch 220600 - Loss: 0.9215\n",
      "Epoch 9/10 - Batch 220700 - Loss: 0.6059\n",
      "Epoch 9/10 - Batch 220800 - Loss: 0.5713\n",
      "Epoch 9/10 - Batch 220900 - Loss: 0.6694\n",
      "Epoch 9/10 - Batch 221000 - Loss: 0.6660\n",
      "Epoch 9/10 - Batch 221100 - Loss: 0.5884\n",
      "Epoch 9/10 - Batch 221200 - Loss: 0.5335\n",
      "Epoch 9/10 - Batch 221300 - Loss: 0.4093\n",
      "Epoch 9/10 - Batch 221400 - Loss: 0.6599\n",
      "Epoch 9/10 - Batch 221500 - Loss: 0.3791\n",
      "Epoch 9/10 - Batch 221600 - Loss: 0.6572\n",
      "Epoch 9/10 - Batch 221700 - Loss: 0.6666\n",
      "Epoch 10/10 - Batch 221800 - Loss: 0.2730\n",
      "Epoch 10/10 - Batch 221900 - Loss: 0.1827\n",
      "Epoch 10/10 - Batch 222000 - Loss: 0.1440\n",
      "Epoch 10/10 - Batch 222100 - Loss: 0.1482\n",
      "Epoch 10/10 - Batch 222200 - Loss: 0.1330\n",
      "Epoch 10/10 - Batch 222300 - Loss: 0.1667\n",
      "Epoch 10/10 - Batch 222400 - Loss: 0.1441\n",
      "Epoch 10/10 - Batch 222500 - Loss: 0.1368\n",
      "Epoch 10/10 - Batch 222600 - Loss: 0.1643\n",
      "Epoch 10/10 - Batch 222700 - Loss: 0.1337\n",
      "Epoch 10/10 - Batch 222800 - Loss: 0.1245\n",
      "Epoch 10/10 - Batch 222900 - Loss: 0.1569\n",
      "Epoch 10/10 - Batch 223000 - Loss: 0.1478\n",
      "Epoch 10/10 - Batch 223100 - Loss: 0.1061\n",
      "Epoch 10/10 - Batch 223200 - Loss: 0.1290\n",
      "Epoch 10/10 - Batch 223300 - Loss: 0.1271\n",
      "Epoch 10/10 - Batch 223400 - Loss: 0.1428\n",
      "Epoch 10/10 - Batch 223500 - Loss: 0.1375\n",
      "Epoch 10/10 - Batch 223600 - Loss: 0.1467\n",
      "Epoch 10/10 - Batch 223700 - Loss: 0.1568\n",
      "Epoch 10/10 - Batch 223800 - Loss: 0.1462\n",
      "Epoch 10/10 - Batch 223900 - Loss: 0.1203\n",
      "Epoch 10/10 - Batch 224000 - Loss: 0.1537\n",
      "Epoch 10/10 - Batch 224100 - Loss: 0.1614\n",
      "Epoch 10/10 - Batch 224200 - Loss: 0.1360\n",
      "Epoch 10/10 - Batch 224300 - Loss: 0.1528\n",
      "Epoch 10/10 - Batch 224400 - Loss: 0.1429\n",
      "Epoch 10/10 - Batch 224500 - Loss: 0.1551\n",
      "Epoch 10/10 - Batch 224600 - Loss: 0.1614\n",
      "Epoch 10/10 - Batch 224700 - Loss: 0.1313\n",
      "Epoch 10/10 - Batch 224800 - Loss: 0.1793\n",
      "Epoch 10/10 - Batch 224900 - Loss: 0.1649\n",
      "Epoch 10/10 - Batch 225000 - Loss: 0.1723\n",
      "Epoch 10/10 - Batch 225100 - Loss: 0.1310\n",
      "Epoch 10/10 - Batch 225200 - Loss: 0.1677\n",
      "Epoch 10/10 - Batch 225300 - Loss: 0.1615\n",
      "Epoch 10/10 - Batch 225400 - Loss: 0.1594\n",
      "Epoch 10/10 - Batch 225500 - Loss: 0.1382\n",
      "Epoch 10/10 - Batch 225600 - Loss: 0.1405\n",
      "Epoch 10/10 - Batch 225700 - Loss: 0.1427\n",
      "Epoch 10/10 - Batch 225800 - Loss: 0.1431\n",
      "Epoch 10/10 - Batch 225900 - Loss: 0.1435\n",
      "Epoch 10/10 - Batch 226000 - Loss: 0.1717\n",
      "Epoch 10/10 - Batch 226100 - Loss: 0.1632\n",
      "Epoch 10/10 - Batch 226200 - Loss: 0.1648\n",
      "Epoch 10/10 - Batch 226300 - Loss: 0.1536\n",
      "Epoch 10/10 - Batch 226400 - Loss: 0.1264\n",
      "Epoch 10/10 - Batch 226500 - Loss: 0.1340\n",
      "Epoch 10/10 - Batch 226600 - Loss: 0.1504\n",
      "Epoch 10/10 - Batch 226700 - Loss: 0.1638\n",
      "Epoch 10/10 - Batch 226800 - Loss: 0.1336\n",
      "Epoch 10/10 - Batch 226900 - Loss: 0.1898\n",
      "Epoch 10/10 - Batch 227000 - Loss: 0.1480\n",
      "Epoch 10/10 - Batch 227100 - Loss: 0.1281\n",
      "Epoch 10/10 - Batch 227200 - Loss: 0.1176\n",
      "Epoch 10/10 - Batch 227300 - Loss: 0.1617\n",
      "Epoch 10/10 - Batch 227400 - Loss: 0.1279\n",
      "Epoch 10/10 - Batch 227500 - Loss: 0.1588\n",
      "Epoch 10/10 - Batch 227600 - Loss: 0.1662\n",
      "Epoch 10/10 - Batch 227700 - Loss: 0.1410\n",
      "Epoch 10/10 - Batch 227800 - Loss: 0.1618\n",
      "Epoch 10/10 - Batch 227900 - Loss: 0.1342\n",
      "Epoch 10/10 - Batch 228000 - Loss: 0.1444\n",
      "Epoch 10/10 - Batch 228100 - Loss: 0.1415\n",
      "Epoch 10/10 - Batch 228200 - Loss: 0.1224\n",
      "Epoch 10/10 - Batch 228300 - Loss: 0.1929\n",
      "Epoch 10/10 - Batch 228400 - Loss: 0.1728\n",
      "Epoch 10/10 - Batch 228500 - Loss: 0.1344\n",
      "Epoch 10/10 - Batch 228600 - Loss: 0.1403\n",
      "Epoch 10/10 - Batch 228700 - Loss: 0.1665\n",
      "Epoch 10/10 - Batch 228800 - Loss: 0.1545\n",
      "Epoch 10/10 - Batch 228900 - Loss: 0.1475\n",
      "Epoch 10/10 - Batch 229000 - Loss: 0.1392\n",
      "Epoch 10/10 - Batch 229100 - Loss: 0.1577\n",
      "Epoch 10/10 - Batch 229200 - Loss: 0.1548\n",
      "Epoch 10/10 - Batch 229300 - Loss: 0.1714\n",
      "Epoch 10/10 - Batch 229400 - Loss: 0.1523\n",
      "Epoch 10/10 - Batch 229500 - Loss: 0.1545\n",
      "Epoch 10/10 - Batch 229600 - Loss: 0.1634\n",
      "Epoch 10/10 - Batch 229700 - Loss: 0.1595\n",
      "Epoch 10/10 - Batch 229800 - Loss: 0.1429\n",
      "Epoch 10/10 - Batch 229900 - Loss: 0.1326\n",
      "Epoch 10/10 - Batch 230000 - Loss: 0.1652\n",
      "Epoch 10/10 - Batch 230100 - Loss: 0.1607\n",
      "Epoch 10/10 - Batch 230200 - Loss: 0.1307\n",
      "Epoch 10/10 - Batch 230300 - Loss: 0.1473\n",
      "Epoch 10/10 - Batch 230400 - Loss: 0.1682\n",
      "Epoch 10/10 - Batch 230500 - Loss: 0.1711\n",
      "Epoch 10/10 - Batch 230600 - Loss: 0.1284\n",
      "Epoch 10/10 - Batch 230700 - Loss: 0.1769\n",
      "Epoch 10/10 - Batch 230800 - Loss: 0.1510\n",
      "Epoch 10/10 - Batch 230900 - Loss: 0.1661\n",
      "Epoch 10/10 - Batch 231000 - Loss: 0.1350\n",
      "Epoch 10/10 - Batch 231100 - Loss: 0.1708\n",
      "Epoch 10/10 - Batch 231200 - Loss: 0.2023\n",
      "Epoch 10/10 - Batch 231300 - Loss: 0.2046\n",
      "Epoch 10/10 - Batch 231400 - Loss: 0.1749\n",
      "Epoch 10/10 - Batch 231500 - Loss: 0.1920\n",
      "Epoch 10/10 - Batch 231600 - Loss: 0.1774\n",
      "Epoch 10/10 - Batch 231700 - Loss: 0.1664\n",
      "Epoch 10/10 - Batch 231800 - Loss: 0.2091\n",
      "Epoch 10/10 - Batch 231900 - Loss: 0.2135\n",
      "Epoch 10/10 - Batch 232000 - Loss: 0.1871\n",
      "Epoch 10/10 - Batch 232100 - Loss: 0.1988\n",
      "Epoch 10/10 - Batch 232200 - Loss: 0.1900\n",
      "Epoch 10/10 - Batch 232300 - Loss: 0.1982\n",
      "Epoch 10/10 - Batch 232400 - Loss: 0.2349\n",
      "Epoch 10/10 - Batch 232500 - Loss: 0.2313\n",
      "Epoch 10/10 - Batch 232600 - Loss: 0.2073\n",
      "Epoch 10/10 - Batch 232700 - Loss: 0.2795\n",
      "Epoch 10/10 - Batch 232800 - Loss: 0.4287\n",
      "Epoch 10/10 - Batch 232900 - Loss: 0.5529\n",
      "Epoch 10/10 - Batch 233000 - Loss: 0.5129\n",
      "Epoch 10/10 - Batch 233100 - Loss: 0.6000\n",
      "Epoch 10/10 - Batch 233200 - Loss: 0.5127\n",
      "Epoch 10/10 - Batch 233300 - Loss: 0.6415\n",
      "Epoch 10/10 - Batch 233400 - Loss: 0.5425\n",
      "Epoch 10/10 - Batch 233500 - Loss: 0.5701\n",
      "Epoch 10/10 - Batch 233600 - Loss: 0.4508\n",
      "Epoch 10/10 - Batch 233700 - Loss: 0.5852\n",
      "Epoch 10/10 - Batch 233800 - Loss: 0.5399\n",
      "Epoch 10/10 - Batch 233900 - Loss: 0.4951\n",
      "Epoch 10/10 - Batch 234000 - Loss: 0.4452\n",
      "Epoch 10/10 - Batch 234100 - Loss: 0.3970\n",
      "Epoch 10/10 - Batch 234200 - Loss: 0.6876\n",
      "Epoch 10/10 - Batch 234300 - Loss: 0.4595\n",
      "Epoch 10/10 - Batch 234400 - Loss: 0.3978\n",
      "Epoch 10/10 - Batch 234500 - Loss: 0.6283\n",
      "Epoch 10/10 - Batch 234600 - Loss: 0.4573\n",
      "Epoch 10/10 - Batch 234700 - Loss: 0.4998\n",
      "Epoch 10/10 - Batch 234800 - Loss: 0.4344\n",
      "Epoch 10/10 - Batch 234900 - Loss: 0.5982\n",
      "Epoch 10/10 - Batch 235000 - Loss: 0.6339\n",
      "Epoch 10/10 - Batch 235100 - Loss: 0.4868\n",
      "Epoch 10/10 - Batch 235200 - Loss: 0.5289\n",
      "Epoch 10/10 - Batch 235300 - Loss: 0.7819\n",
      "Epoch 10/10 - Batch 235400 - Loss: 0.4400\n",
      "Epoch 10/10 - Batch 235500 - Loss: 0.4949\n",
      "Epoch 10/10 - Batch 235600 - Loss: 0.4530\n",
      "Epoch 10/10 - Batch 235700 - Loss: 0.5086\n",
      "Epoch 10/10 - Batch 235800 - Loss: 0.4916\n",
      "Epoch 10/10 - Batch 235900 - Loss: 0.4828\n",
      "Epoch 10/10 - Batch 236000 - Loss: 0.4653\n",
      "Epoch 10/10 - Batch 236100 - Loss: 0.4808\n",
      "Epoch 10/10 - Batch 236200 - Loss: 0.4563\n",
      "Epoch 10/10 - Batch 236300 - Loss: 0.5485\n",
      "Epoch 10/10 - Batch 236400 - Loss: 0.5553\n",
      "Epoch 10/10 - Batch 236500 - Loss: 0.3820\n",
      "Epoch 10/10 - Batch 236600 - Loss: 0.5741\n",
      "Epoch 10/10 - Batch 236700 - Loss: 0.5112\n",
      "Epoch 10/10 - Batch 236800 - Loss: 0.4987\n",
      "Epoch 10/10 - Batch 236900 - Loss: 0.6050\n",
      "Epoch 10/10 - Batch 237000 - Loss: 0.4868\n",
      "Epoch 10/10 - Batch 237100 - Loss: 0.4246\n",
      "Epoch 10/10 - Batch 237200 - Loss: 0.4848\n",
      "Epoch 10/10 - Batch 237300 - Loss: 0.4110\n",
      "Epoch 10/10 - Batch 237400 - Loss: 0.6946\n",
      "Epoch 10/10 - Batch 237500 - Loss: 0.6861\n",
      "Epoch 10/10 - Batch 237600 - Loss: 0.3858\n",
      "Epoch 10/10 - Batch 237700 - Loss: 0.4451\n",
      "Epoch 10/10 - Batch 237800 - Loss: 0.5135\n",
      "Epoch 10/10 - Batch 237900 - Loss: 0.5406\n",
      "Epoch 10/10 - Batch 238000 - Loss: 0.6606\n",
      "Epoch 10/10 - Batch 238100 - Loss: 0.5753\n",
      "Epoch 10/10 - Batch 238200 - Loss: 0.6939\n",
      "Epoch 10/10 - Batch 238300 - Loss: 0.5892\n",
      "Epoch 10/10 - Batch 238400 - Loss: 0.5968\n",
      "Epoch 10/10 - Batch 238500 - Loss: 0.5805\n",
      "Epoch 10/10 - Batch 238600 - Loss: 0.6446\n",
      "Epoch 10/10 - Batch 238700 - Loss: 0.5306\n",
      "Epoch 10/10 - Batch 238800 - Loss: 0.7992\n",
      "Epoch 10/10 - Batch 238900 - Loss: 0.5322\n",
      "Epoch 10/10 - Batch 239000 - Loss: 0.4786\n",
      "Epoch 10/10 - Batch 239100 - Loss: 0.5100\n",
      "Epoch 10/10 - Batch 239200 - Loss: 0.5885\n",
      "Epoch 10/10 - Batch 239300 - Loss: 0.6485\n",
      "Epoch 10/10 - Batch 239400 - Loss: 0.5288\n",
      "Epoch 10/10 - Batch 239500 - Loss: 0.4942\n",
      "Epoch 10/10 - Batch 239600 - Loss: 0.4640\n",
      "Epoch 10/10 - Batch 239700 - Loss: 0.5162\n",
      "Epoch 10/10 - Batch 239800 - Loss: 0.5104\n",
      "Epoch 10/10 - Batch 239900 - Loss: 0.4023\n",
      "Epoch 10/10 - Batch 240000 - Loss: 0.4492\n",
      "Epoch 10/10 - Batch 240100 - Loss: 0.6149\n",
      "Epoch 10/10 - Batch 240200 - Loss: 0.4185\n",
      "Epoch 10/10 - Batch 240300 - Loss: 0.4033\n",
      "Epoch 10/10 - Batch 240400 - Loss: 0.3688\n",
      "Epoch 10/10 - Batch 240500 - Loss: 0.3885\n",
      "Epoch 10/10 - Batch 240600 - Loss: 0.4745\n",
      "Epoch 10/10 - Batch 240700 - Loss: 0.2691\n",
      "Epoch 10/10 - Batch 240800 - Loss: 0.3277\n",
      "Epoch 10/10 - Batch 240900 - Loss: 0.2863\n",
      "Epoch 10/10 - Batch 241000 - Loss: 0.2738\n",
      "Epoch 10/10 - Batch 241100 - Loss: 0.4806\n",
      "Epoch 10/10 - Batch 241200 - Loss: 0.3000\n",
      "Epoch 10/10 - Batch 241300 - Loss: 0.2838\n",
      "Epoch 10/10 - Batch 241400 - Loss: 0.3170\n",
      "Epoch 10/10 - Batch 241500 - Loss: 0.4301\n",
      "Epoch 10/10 - Batch 241600 - Loss: 0.3856\n",
      "Epoch 10/10 - Batch 241700 - Loss: 0.5142\n",
      "Epoch 10/10 - Batch 241800 - Loss: 0.2512\n",
      "Epoch 10/10 - Batch 241900 - Loss: 0.3258\n",
      "Epoch 10/10 - Batch 242000 - Loss: 0.5693\n",
      "Epoch 10/10 - Batch 242100 - Loss: 0.5395\n",
      "Epoch 10/10 - Batch 242200 - Loss: 0.0682\n",
      "Epoch 10/10 - Batch 242300 - Loss: 0.4163\n",
      "Epoch 10/10 - Batch 242400 - Loss: 0.3893\n",
      "Epoch 10/10 - Batch 242500 - Loss: 0.2705\n",
      "Epoch 10/10 - Batch 242600 - Loss: 0.5293\n",
      "Epoch 10/10 - Batch 242700 - Loss: 0.6262\n",
      "Epoch 10/10 - Batch 242800 - Loss: 0.4855\n",
      "Epoch 10/10 - Batch 242900 - Loss: 0.7485\n",
      "Epoch 10/10 - Batch 243000 - Loss: 0.6467\n",
      "Epoch 10/10 - Batch 243100 - Loss: 0.5075\n",
      "Epoch 10/10 - Batch 243200 - Loss: 0.7058\n",
      "Epoch 10/10 - Batch 243300 - Loss: 0.5284\n",
      "Epoch 10/10 - Batch 243400 - Loss: 0.6678\n",
      "Epoch 10/10 - Batch 243500 - Loss: 0.5140\n",
      "Epoch 10/10 - Batch 243600 - Loss: 0.7120\n",
      "Epoch 10/10 - Batch 243700 - Loss: 0.4308\n",
      "Epoch 10/10 - Batch 243800 - Loss: 0.4267\n",
      "Epoch 10/10 - Batch 243900 - Loss: 0.5422\n",
      "Epoch 10/10 - Batch 244000 - Loss: 0.6551\n",
      "Epoch 10/10 - Batch 244100 - Loss: 0.4117\n",
      "Epoch 10/10 - Batch 244200 - Loss: 0.5391\n",
      "Epoch 10/10 - Batch 244300 - Loss: 0.5013\n",
      "Epoch 10/10 - Batch 244400 - Loss: 0.3754\n",
      "Epoch 10/10 - Batch 244500 - Loss: 0.6811\n",
      "Epoch 10/10 - Batch 244600 - Loss: 0.3741\n",
      "Epoch 10/10 - Batch 244700 - Loss: 0.4959\n",
      "Epoch 10/10 - Batch 244800 - Loss: 0.5135\n",
      "Epoch 10/10 - Batch 244900 - Loss: 0.6553\n",
      "Epoch 10/10 - Batch 245000 - Loss: 0.5590\n",
      "Epoch 10/10 - Batch 245100 - Loss: 0.4965\n",
      "Epoch 10/10 - Batch 245200 - Loss: 0.7182\n",
      "Epoch 10/10 - Batch 245300 - Loss: 0.6766\n",
      "Epoch 10/10 - Batch 245400 - Loss: 0.6382\n",
      "Epoch 10/10 - Batch 245500 - Loss: 0.6793\n",
      "Epoch 10/10 - Batch 245600 - Loss: 0.5210\n",
      "Epoch 10/10 - Batch 245700 - Loss: 0.5860\n",
      "Epoch 10/10 - Batch 245800 - Loss: 0.5216\n",
      "Epoch 10/10 - Batch 245900 - Loss: 0.4192\n",
      "Epoch 10/10 - Batch 246000 - Loss: 0.5346\n",
      "Epoch 10/10 - Batch 246100 - Loss: 0.4920\n",
      "Epoch 10/10 - Batch 246200 - Loss: 0.4122\n",
      "Epoch 10/10 - Batch 246300 - Loss: 0.8198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مصر المجلة العالمية لحقوق الانسان الاصوات العالمية\n"
     ]
    }
   ],
   "source": [
    "# Set up your training hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 10\n",
    "# Set the model to train mode\n",
    "model.train()\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "total_loss = 0.0\n",
    "batch_count = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for idx in range(len(train_dataset)):\n",
    "        source_inputs, target_inputs = train_dataset[idx]\n",
    "        source_inputs = source_inputs.unsqueeze(0).to(model.device)\n",
    "        target_inputs = target_inputs.unsqueeze(0).to(model.device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=source_inputs, labels=target_inputs)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "\n",
    "        # Print average loss every 100 batches\n",
    "        if batch_count % 100 == 0:\n",
    "            average_loss = total_loss / 100\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Batch {batch_count} - Loss: {average_loss:.4f}\")\n",
    "            total_loss = 0.0\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"translator_model\")\n",
    "tokenizer.save_pretrained(\"translator_model\")\n",
    "\n",
    "# Set up the model and tokenizer\n",
    "model_name = \"translator_model\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستجدون ايضا روابط لبعض المنصات المجانية على الانترنت لانشاء وانقاذ ابتكاراتكم في قبل اسبوع من اليوم العالمي للغة الام سنشارك مساهماتنا من جميع انحاء العالم ونعرض بعض المفضلين هنا على الاصوات الصاعدة\n"
     ]
    }
   ],
   "source": [
    "# Translate a sentence\n",
    "english_text = \"you will also find links to some free web based platforms to \\\n",
    "create and save your creations in the week prior to international mother language day we ll be sharing retweeting and liking contributions \\\n",
    "from around the world and featuring some of our favorites here on rising voices.\"\n",
    "encoded_input = tokenizer.encode(english_text, return_tensors=\"pt\")\n",
    "translated = model.generate(encoded_input)\n",
    "arabic_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "print(arabic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer, AdamW\n",
    "from torch.utils.data import Dataset\n",
    "# translate a file\n",
    "english_file = \"data/english_test.txt\"\n",
    "arabic_file = \"data/arabic_output.txt\"\n",
    "#use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Set up the model and tokenizer\n",
    "model_name = \"translator_model\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "\n",
    "# Read the file\n",
    "with open(english_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "    english_texts = [line.strip() for line in lines]\n",
    "\n",
    "# Translate the texts\n",
    "arabic_texts = []\n",
    "for english_text in english_texts:\n",
    "    encoded_input = tokenizer.encode(english_text, return_tensors=\"pt\").to(device)\n",
    "    translated = model.generate(encoded_input)\n",
    "    arabic_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    arabic_texts.append(arabic_text)\n",
    "    # write each translated text to the file\n",
    "    with open(arabic_file, \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(arabic_text + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Paths for the output files\n",
    "validation_dataset_file = \"./data/english_validation_dataset.txt\"\n",
    "translations_file = \"./data/translations.txt\"\n",
    "model_outputs_file = \"./data/model_outputs.txt\"\n",
    "\n",
    "# Open the files in write mode\n",
    "with open(validation_dataset_file, \"w\", encoding=\"utf-8\") as dataset_file, \\\n",
    "        open(translations_file, \"w\", encoding=\"utf-8\") as translations_output_file:\n",
    "\n",
    "    for idx in range(len(valid_dataset)):\n",
    "        source_inputs, target_inputs = valid_dataset[idx]\n",
    "        source_sentence = tokenizer.decode(source_inputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        target_sentence = tokenizer.decode(target_inputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "        # Remove underscores from the sentences\n",
    "        source_sentence = source_sentence.replace(\"▁\", \" \").strip()\n",
    "        target_sentence = target_sentence.replace(\"▁\", \" \").strip()\n",
    "\n",
    "        # Write English validation dataset to file\n",
    "        dataset_file.write(source_sentence + \"\\n\")\n",
    "\n",
    "        # Write translations to file\n",
    "        translations_output_file.write(target_sentence + \"\\n\")\n",
    "\n",
    "print(\"Validation dataset and translations written successfully.\")\n",
    "\n",
    "# Read the validation dataset\n",
    "with open(validation_dataset_file, \"r\", encoding=\"utf-8\") as dataset_file:\n",
    "    validation_sentences = dataset_file.readlines()\n",
    "\n",
    "# Initialize an empty list to store the generated translations\n",
    "generated_translations = []\n",
    "\n",
    "# Generate translations using the model\n",
    "for sentence in validation_sentences:\n",
    "    input_ids = tokenizer.encode(sentence, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.to(device)\n",
    "    translation = model.generate(input_ids)\n",
    "    translated_sentence = tokenizer.decode(translation[0], skip_special_tokens=True)\n",
    "    generated_translations.append(translated_sentence)\n",
    "\n",
    "# Write the model output to a file\n",
    "with open(model_outputs_file, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    output_file.write(\"\\n\".join(generated_translations))\n",
    "\n",
    "# Compare the generated translations with the ground truth translations for accuracy\n",
    "with open(translations_file, \"r\", encoding=\"utf-8\") as translations_file:\n",
    "    ground_truth_translations = translations_file.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 37.03%\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "total = len(ground_truth_translations)\n",
    "for generated_translation, ground_truth_translation in zip(generated_translations, ground_truth_translations):\n",
    "    generated_words = generated_translation.strip().split()\n",
    "    ground_truth_words = ground_truth_translation.strip().split()\n",
    "    common_words = set(generated_words).intersection(ground_truth_words)\n",
    "    if len(common_words) >= len(ground_truth_words) * 0.35:\n",
    "        num_correct += 1\n",
    "\n",
    "accuracy = num_correct / total\n",
    "print(\"Accuracy: {:.2%}\".format(accuracy))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 52.54%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the ground truth translations\n",
    "ground_truth_vectors = vectorizer.fit_transform(ground_truth_translations)\n",
    "\n",
    "num_correct = 0\n",
    "total = len(ground_truth_translations)\n",
    "\n",
    "for generated_translation, ground_truth_vector in zip(generated_translations, ground_truth_vectors):\n",
    "    generated_vector = vectorizer.transform([generated_translation])\n",
    "    similarity = cosine_similarity(generated_vector, ground_truth_vector)\n",
    "    if similarity > 0.35:  # Adjust the threshold as per your requirement\n",
    "        num_correct += 1\n",
    "\n",
    "accuracy = num_correct / total\n",
    "print(\"Accuracy: {:.2%}\".format(accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bb0fe8ced3cf0716ac3718fe834e829af40e8ba0fef1c4cadecb390da29a017"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('torch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
